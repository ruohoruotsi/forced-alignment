{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP8: Creation of Labelled Data\n",
    "As usual, let's define the imports and some helper functions before we start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from corpus_util import *\n",
    "from audio_util import *\n",
    "\n",
    "def show_audio(corpus_entry):\n",
    "    entry_title = HTML(f\"\"\"\n",
    "    <h3>Sample corpus entry: {corpus_entry.name}</h3>\n",
    "    <p><strong>Path to raw data</strong>: {corpus_entry.original_path}</p>\n",
    "    \"\"\")\n",
    "    entry_audio = Audio(corpus_entry.audio_file)\n",
    "    entry_text = widgets.Accordion(children=[widgets.HTML(f'<pre>{corpus_entry.transcript}</pre>')], selected_index=None)\n",
    "    entry_text.set_title(0, 'Transcript')\n",
    "    \n",
    "    display(entry_title)\n",
    "    display(entry_audio)\n",
    "    display(entry_text)\n",
    "\n",
    "def show_spectrogram(audio_file):\n",
    "    NFFT = 200  # Length of each window segment\n",
    "    Fs = 8000  # Sampling frequencies\n",
    "    noverlap = 120  # Overlap between windows\n",
    "\n",
    "    freqs, times, spec = calculate_spectrogram(audio_file, nfft=NFFT, fs=Fs, noverlap=noverlap)\n",
    "\n",
    "    pad_xextent = (NFFT - noverlap) / Fs / 2\n",
    "    xextent = np.min(times) - pad_xextent, np.max(times) + pad_xextent\n",
    "    xmin, xmax = xextent\n",
    "    extent = xmin, xmax, freqs[0], freqs[-1]\n",
    "\n",
    "    im = plt.imshow(spec, extent=extent, aspect='auto')\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [steps]')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the corpora we can start creating labelled data to train an RNN. In the following sections the following variable namesare used to denote the parts of this data:\n",
    "\n",
    "* `X`: The training data, i.e. the spectrograms (one spectrogram per corpus entry)\n",
    "* `Y`: The training labels, i.e. sequences of zeroes when text is being spoken and sequences of ones when nothing is being spoken (i.e. silence or only background noise)\n",
    "\n",
    "Let's load the created corpora to make them available to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_corpus = load_corpus(r'E:\\librispeech-corpus\\librispeech.corpus')\n",
    "rl_corpus = load_corpus(r'E:\\readylingua-corpus\\readylingua.corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev/Test split\n",
    "The labelled data is split into subsets for training (_train-set_), validation (_dev-set_) and model evaluation (_test-set_). Since the corpora were constructed from different amounts of raw data, they vary in size and probability distribution (number of languages, homogeneity of the recording quality, ratio of male vs. female speakers, presence of distortions like reverb, echo or overdrive, and many more). Since the starting point for the creation of the corpus was so different, different approaches were taken to split the corpus up into train-, dev- and test-set.\n",
    "\n",
    "#### ReadyLingua corpus\n",
    "tbd.\n",
    "\n",
    "#### LibriSpeech corpus\n",
    "The LibriSpeech raw data is already split into train-, dev- and test-set. Each chapter is read by a different speaker. Each speaker is only contained in one of the subsets. Efforts have been made to keep the different sets within the same probability distributions (regarding to accents, ratio of male/female speakers, ...). To leverage these efforts, the corresponding corpus entries created from the raw data are kept in the same sets.\n",
    "\n",
    "---\n",
    "\n",
    "You can explore the subsets by executing the cell below to see the number of samples (corpus entries) in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_train, ls_dev, ls_test = ls_corpus.train_dev_test_split()\n",
    "print(f'#train-samples: {len(ls_train)}, #dev-samples: {len(ls_dev)}, #test-samples: {len(ls_test)}')\n",
    "\n",
    "rl_train, rl_dev, rl_test = rl_corpus.train_dev_test_split()\n",
    "print(f'#train-samples: {len(rl_train)}, #dev-samples: {len(rl_dev)}, #test-samples: {len(rl_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  From corpus entries to spectrograms\n",
    "In order to train an RNN, each sample needs to be converted into some sort of sequence. In this case the samples are the audio files from the corpus entries and the sequences are their spectrograms. You can explore a random sample together with its spectrogram by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_entry = random.choice(ls_corpus)\n",
    "show_spectrogram(random_entry.audio_file)\n",
    "show_audio(random_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
