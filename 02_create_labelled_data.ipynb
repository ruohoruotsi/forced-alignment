{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP8: Creation of Labelled Data\n",
    "The RNN will be trained on spectrograms of the audio data from the created corpora. Since this process is computationally expensive and requires a lot of time. To speed up the iterations when training the RNN and get feedback faster, the input data (the spectrograms) are pre-computed and stored on disk. Also, the labels (the information about speech pauses) need to be encoded in a suitable format. This notebook describes how this is done.\n",
    "\n",
    "Before we start, define a path to an empty directory with enough free storage where the labelled data can be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_root = r'E:/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's do the imports and some helper functions before we start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, Audio\n",
    "\n",
    "from create_labelled_data import create_X_Y\n",
    "from corpus_util import *\n",
    "from audio_util import *\n",
    "from webrtc_util import *\n",
    "\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "from IPython.display import HTML, Audio\n",
    "import librosa.display\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "\n",
    "# import mpld3\n",
    "# mpld3.enable_notebook()\n",
    "\n",
    "from corpus_util import *\n",
    "from audio_util import *\n",
    "\n",
    "rl_corpus_root = os.path.join(target_root, 'readylingua-corpus')\n",
    "ls_corpus_root = os.path.join(target_root, 'librispeech-corpus')\n",
    "\n",
    "rl_data_root = os.path.join(target_root, 'readylingua-data')\n",
    "ls_data_root = os.path.join(target_root, 'librispeech-data')\n",
    "\n",
    "rl_corpus_path = os.path.join(rl_corpus_root, 'readylingua.corpus')\n",
    "ls_corpus_path = os.path.join(ls_corpus_root, 'librispeech.corpus')\n",
    "\n",
    "default_figsize = (50,20)\n",
    "default_facecolor = 'white'\n",
    "default_font = {'family': 'serif', 'weight': 'normal', 'size': 32}\n",
    "\n",
    "plt.rc('font', **default_font)\n",
    "\n",
    "def show_labelled_data(corpus_entry, data_root):\n",
    "    display(HTML(f'<h3>{corpus_entry.name} (id={corpus_entry.id})</h3>'))\n",
    "    display(HTML(f'{len(corpus_entry.speech_segments)} speech segments, {len(corpus_entry.pause_segments)} pause segments'))\n",
    "    \n",
    "    # audio data\n",
    "    sample_rate, audio = corpus_entry.audio\n",
    "    display(Audio(data=audio, rate=sample_rate))\n",
    "    \n",
    "    fig = plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "    \n",
    "    # plot spectrogram\n",
    "    freqs, times, spec = corpus_entry.spectrogram\n",
    "    ax_spec, extent = show_spectrogram(freqs, times, spec, fig)\n",
    "    \n",
    "    # plot raw wave\n",
    "    ax_wave = show_wave(audio, sample_rate, fig)\n",
    "    \n",
    "    # overlay pauses\n",
    "    y = corpus_entry.labels\n",
    "    if y is None:\n",
    "        print(f'no labels for corpus entry with id={corpus_entry.id}')\n",
    "    else:\n",
    "        left, right, bottom, top = extent\n",
    "        boundaries_frames = calculate_pause_boundaries_from_ground_truth(audio, y)\n",
    "        show_pause_segments(ax_wave, boundaries_frames)\n",
    "        show_pause_segments(ax_spec, (right-left) * boundaries_frames / len(audio))\n",
    "        \n",
    "    return ax_spec, ax_wave\n",
    "\n",
    "def show_spectrogram(freqs, times, spec, fig=None):\n",
    "    if not fig:\n",
    "        fig = plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "    ax2 = fig.add_subplot(211)\n",
    "    ax2.set_xlim(0, times.max())\n",
    "    extent = [times.min(), times.max(), freqs.min(), freqs.max()]\n",
    "    ax2.imshow(spec.T, aspect='auto', origin='lower', extent=extent)\n",
    "    ax2.set_yticks(freqs[::16])\n",
    "    ax2.set_xticks(times[::int(len(times)/10)])\n",
    "    ax2.set_title('Spectrogram of ' + corpus_entry.audio_file)\n",
    "    ax2.set_ylabel('Freqs in Hz')\n",
    "    ax2.set_xlabel('Seconds')\n",
    "    return ax2, extent\n",
    "\n",
    "def show_wave(audio, sample_rate, fig=None, title=None):\n",
    "    if not fig:\n",
    "        fig = plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "    if not title:\n",
    "        title = f'Raw wave of {corpus_entry.audio_file} with speech pauses'\n",
    "    \n",
    "    ax1 = fig.add_subplot(212)\n",
    "    ax1.set_xlim(0, len(audio))\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_ylabel('Amplitude')\n",
    "    ax1.set_xlabel('Audio frames')\n",
    "    ax1.plot(np.linspace(0, len(audio), len(audio)), audio)\n",
    "    return ax1\n",
    "\n",
    "def show_pause_segments(ax, boundaries, ymin=0, ymax=1, color='red'):\n",
    "    for pause_start, pause_end in boundaries:\n",
    "        ax.axvspan(pause_start, pause_end, ymin=ymin, ymax=ymax, color=color, alpha=0.8)\n",
    "    \n",
    "def calculate_pause_boundaries_from_ground_truth(x, y):\n",
    "    \"\"\"calculates the boundaries of pause segments in x given a label vector y.\n",
    "    \n",
    "    :y: numpy array of shape (1, T_y) containing the labels (\"speech\"/\"no speech\") for a RNN\n",
    "    :x: numpy array of shape (T_x, ) containing the audio signal for a RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    num_x = len(x)\n",
    "    \n",
    "    # pause boundaries as binary vector: [0,0,1,1,1,0,...]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    # pause boundaries as indices of 1-groups in y (start and end indices of group): [[2,4], ...]\n",
    "    boundaries = np.flatnonzero(np.diff(np.r_[0,y,0]) != 0).reshape(-1,2) - [0,1]\n",
    "    \n",
    "    # pause boundaries as indices of 1-groups in x (calculated from relative position of frames in y)\n",
    "    boundaries = len(x) * boundaries / len(y)\n",
    "    \n",
    "    # no fractional indices\n",
    "    return boundaries.astype(int)\n",
    "\n",
    "def calculate_boundaries_from_webrtc(segments, audio, sample_rate):\n",
    "    boundaries = []\n",
    "    for frames in segments:\n",
    "        start_frame = frames[0].timestamp * sample_rate\n",
    "        end_frame = (frames[-1].timestamp + frames[-1].duration) * sample_rate\n",
    "        boundaries.append((start_frame, end_frame))\n",
    "    return 2 * np.array(boundaries).astype(int)\n",
    "    \n",
    "def on_create_data_rl_button_click(sender):\n",
    "    rl_target_root = os.path.join(target_root, 'readylingua-data')\n",
    "    create_X_Y(ls_corpus, rl_target_root)\n",
    "    \n",
    "def on_create_data_ls_button_click(sender):\n",
    "    ls_target_root = os.path.join(target_root, 'librispeech-data')\n",
    "    create_X_Y(ls_corpus, ls_target_root)      \n",
    "    \n",
    "# UI elements\n",
    "layout = widgets.Layout(width='250px', height='50px')\n",
    "create_data_rl_btn = widgets.Button(description=\"Create labelled data for ReadyLingua\", button_style='info', layout=layout, icon='download')\n",
    "create_data_rl_btn.on_click(on_create_data_rl_button_click)\n",
    "create_data_ls_btn = widgets.Button(description=\"Create labelled data for LibriSpeech\", button_style='info', layout=layout, icon='download')\n",
    "create_data_ls_btn.on_click(on_create_data_ls_button_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having created the corpora from raw data we can now start creating labelled data (spectrograms and labels) for the RNN. This data is stored as numpy arrays whose dimensions partially depend on the proposed network architecture. \n",
    "\n",
    "The RNN is trained on audio data (sequence of frames) and will output whether a specific section in the audio signal is speech or pause (sequence of labels). Because both the input and the output is a sequence, it is a sequence-to-sequence model with a **many-to-many** architecture. This means we have the following values to consider:\n",
    "\n",
    "* $T_x$: Number of sequence tokens in an individual sample. This value may be different for each sample!\n",
    "* $T_y$: Number of sequence tokens in the output. This value is always the same for each sample but may be different from $T_x$\n",
    "\n",
    "In the following sections the following variable names are used to denote the two components of the labelled data:\n",
    "\n",
    "* `X`: The actual data, i.e. the spectrograms. One spectrogram is created per corpus entry and saved to disk. The saved data consists of three components:\n",
    "  * `freqs`: The frequencies used in the spectrogram (array of shape $(161, 1)$)\n",
    "  * `times`: The time steps used in the spectrogram (array of shape $(T_x, 1)$)\n",
    "  * `spec`: The spectrogram data (array of shape $(T_x, 161)$)\n",
    "  'freqs' and 'times' are only needed to plot the spectrogram along in a Cartesian coordinate system, where the time steps will be plotted along the x-axis and  frequencies along the y-axis. For training, only the `spec` part is needed.\n",
    "* `Y`: The labels, i.e. the information about speech- or pause segments. The labels are encoded as 1-dimensional binary vectors of shape $(1, T_y)$. A speech segment will be encoded as a sequence of zeroes and a pause segment as a sequence of ones. Pause sections may contain some signal (e.g. background noise) but no spoken text from the transcript.\n",
    "\n",
    "Let's load the created corpora to make them available to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_corpus = load_corpus(rl_corpus_path)\n",
    "ls_corpus = load_corpus(ls_corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev/Test split\n",
    "The labelled data is split into subsets for training (_train-set_), parameter tuning (_dev-set_) and model evaluation (_test-set_). Since the corpora were create from different sources of raw data, they vary in size and probability distribution (number of languages, homogeneity of the recording quality, ratio of male vs. female speakers, presence of distortions like reverb or overdrive, and many more). Since the starting point for the creation of the corpus was so variable, different approaches were taken to split the corpus up into train-, dev- and test-set.\n",
    "\n",
    "#### ReadyLingua corpus\n",
    "The raw data exhibits a high variance with respect to relevant features (recording quality, length of samples, presence of distortion, ...). Since the corpus is rather small there may be only one sample for a specific feature value (e.g. only one recording with reverb). Therefore to keep things simple the split into train-, dev- and test-set was done with a 80/10/10-rule without closer examination of the underlying data. This might not result in an optimal split since it would be possible for example that all the female speakers will be put in one subset.\n",
    "\n",
    "Improvements could be made by manually assigning each sample to a specific set by carefully inspecting the relevant features. The corpus could also be extended by creating synthetisized data, e.g. creating samples with reverb from the original samples. Because the LibriSpeech corpus looks much more promising at the moment, this time was not invested.\n",
    "\n",
    "#### LibriSpeech corpus\n",
    "The LibriSpeech raw data is already split into train-, dev- and test-set. Each chapter is read by a different speaker. Each speaker is only contained in one of the subsets. Efforts have been made to keep the different sets within the same probability distributions (regarding to accents, ratio of male/female speakers, ...). The information about the subset has been preserved when creating the corpora from raw data. To leverage the efforts made by the LibriSpeech project, the corresponding labelled data will be kept in the same subset.\n",
    "\n",
    "---\n",
    "\n",
    "You can explore the size of the subsets for each corpus by executing the cell below to see the number of samples (corpus entries) in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_train, ls_dev, ls_test = ls_corpus.train_dev_test_split()\n",
    "print(f'LibriSpeech corpus ({len(ls_corpus)} samples): #train-samples: {len(ls_train)}, #dev-samples: {len(ls_dev)}, #test-samples: {len(ls_test)}')\n",
    "\n",
    "rl_train, rl_dev, rl_test = rl_corpus.train_dev_test_split()\n",
    "print(f'ReadyLingua corpus ({len(rl_corpus)} samples): #train-samples: {len(rl_train)}, #dev-samples: {len(rl_dev)}, #test-samples: {len(rl_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  From corpus entries to spectrograms\n",
    "In order to train an RNN, each sample needs to be converted into some sort of sequence. In this case the samples are the audio files from the corpus entries and the sequences are their spectrograms. You can explore a random sample from the ReadyLingua corpus and visualize its spectrogram by executing the cell below. This will calculate a spectrogram for a corpus entry on-the-fly and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry = random.choice(ls_corpus)\n",
    "rate, audio = corpus_entry.audio\n",
    "freqs, times, spec = log_specgram(audio, rate)\n",
    "show_spectrogram(freqs, times, spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a spectrogram is now created as a matrix `x` for every single corpus entry. A label vector `y` is also created for each corpus entry. This leaves us with two files for each entry. Since the spectrograms can become quite big, separate files are created for each entry. The files share a common naming pattern to identify their type (spectrogram or label), subset membership (train-, dev- or test-set) and corresponding corpus entry (ID of the corpus entry):\n",
    "\n",
    "`{id}.{X|Y}.{train|dev|test}.npy`\n",
    "\n",
    "For example the following files will be created for a corpus entry in the dev-set with ID `1234`:\n",
    "\n",
    "```\n",
    "1234.X.dev.npy\n",
    "1234.Y.dev.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the spectrograms and labels\n",
    "\n",
    "Creating the labelled data might take some time (around 15 minutes for the ReadyLingua corpus up to several hours for the LibriSpeech corpus). Click the button below to start computing the spectrograms and label vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(widgets.HBox([create_data_rl_btn, create_data_ls_btn]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the labelled data\n",
    "After the labelled data has been created, we can explore a sample by visualizing its spectrogram together with the information from the label vector. Execute the following cell to explore a random sample from the ReadyLingua corpus. In contrast to the spectrogram above this will not calculate the spectrogram on-the-fly but merely load the pre-computed files from disk. Together with the spectrogram the raw wave form will also be displayed. The labels (the information about pause segments) will be overlaid to both and will be visibile as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_entry = random.choice(rl_corpus)\n",
    "# corpus_entry = rl_corpus[0]\n",
    "corpus_entry = rl_corpus['news170524']\n",
    "show_labelled_data(corpus_entry, rl_data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can detect speech pauses using a VAD (Voice Activity Detection) algorithm like the one from [WebRTC](https://webrtc.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry = random.choice(rl_corpus)\n",
    "\n",
    "sample_rate, audio = corpus_entry.audio\n",
    "display(Audio(data=audio, rate=sample_rate))\n",
    "\n",
    "# pause boundaries from raw data\n",
    "y = corpus_entry.labels\n",
    "original_boundaries = calculate_pause_boundaries_from_ground_truth(audio, y)\n",
    "\n",
    "# pause boundaries from WebRTC\n",
    "voiced_segments, unvoiced_segments = split_segments(corpus_entry)\n",
    "webrtc_boundaries = calculate_boundaries_from_webrtc(unvoiced_segments, audio, sample_rate)\n",
    "\n",
    "title = f'Raw wave of {corpus_entry.audio_file} with {len(original_boundaries)} original speech pauses (red) and {len(webrtc_boundaries)} speech pauses detected by WebRTC (green)'\n",
    "ax_wave = show_wave(audio, sample_rate, title=title)\n",
    "show_pause_segments(ax_wave, original_boundaries, ymax=0.5)\n",
    "show_pause_segments(ax_wave, webrtc_boundaries, ymin=0.5, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate how much the speech pauses detected by WebRTC coincide with the speech pauses from raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_overlaps(original_boundaries, webrtc_boundaries):\n",
    "    boundaries = np.concatenate((original_boundaries, webrtc_boundaries))\n",
    "    \n",
    "    prev_start = -1\n",
    "    prev_end = -1\n",
    "    num_overlaps = 0\n",
    "    overlaps = []\n",
    "    for start, end in np.sort(boundaries, axis=0):\n",
    "        if start <= prev_end: # we have an overlap\n",
    "            overlap_start, overlap_end = (start, min(end, prev_end))\n",
    "            \n",
    "            # calculate number of frames that overlap\n",
    "            overlap_frames = overlap_end - overlap_start\n",
    "            \n",
    "            # calculate degree of overlap\n",
    "            segment_frames = prev_end - prev_start\n",
    "            overlap_ratio = overlap_frames/segment_frames\n",
    "            \n",
    "            overlaps.append((overlap_frames, overlap_ratio))\n",
    "        prev_start = start\n",
    "        prev_end = end\n",
    "        \n",
    "    return overlaps\n",
    "\n",
    "def compare_corpus_entry(corpus_entry, data_root):\n",
    "    entry_stats = {}\n",
    "    y = corpus_entry.labels\n",
    "    original_boundaries = calculate_pause_boundaries_from_ground_truth(audio, y)\n",
    "    \n",
    "    entry_stats['raw'] = {}\n",
    "    entry_stats['raw']['#pauses'] = len(corpus_entry.pause_segments)\n",
    "    entry_stats['raw']['#pause_frames'] = sum([end_frame - start_frame for start_frame, end_frame in original_boundaries])\n",
    "    \n",
    "    voiced_segments, unvoiced_segments = split_segments(corpus_entry)\n",
    "    webrtc_boundaries = calculate_boundaries_from_webrtc(unvoiced_segments, audio, sample_rate)    \n",
    "    \n",
    "    entry_stats['webrtc'] = {}\n",
    "    entry_stats['webrtc']['#pauses'] = len(unvoiced_segments)\n",
    "    entry_stats['webrtc']['#pause_frames'] = sum([end_frame - start_frame for start_frame, end_frame in webrtc_boundaries])\n",
    "\n",
    "    overlaps = calculate_overlaps(original_boundaries, webrtc_boundaries)\n",
    "    \n",
    "    entry_stats['#overlaps'] = len(overlaps)\n",
    "    entry_stats['#overlap_frames'] = np.sum([overlap_frames for overlap_frames, _ in overlaps])\n",
    "    entry_stats['avg_overlap'] = np.mean([overlap_ratio for _, overlap_ratio in overlaps])\n",
    "    \n",
    "    return entry_stats\n",
    "    \n",
    "entry_stats = compare_corpus_entry(corpus_entry, rl_data_root)\n",
    "\n",
    "num_pauses_raw = entry_stats['raw']['#pauses']\n",
    "sum_pauses_raw = entry_stats['raw']['#pause_frames']\n",
    "print(f'Length of all {num_pauses_raw} speech pauses in corpus entry (ground truth): {sum_pauses_raw}')\n",
    "\n",
    "num_pauses_webrtc = entry_stats['webrtc']['#pauses']\n",
    "sum_pauses_webrtc = entry_stats['webrtc']['#pause_frames']\n",
    "print(f'Length of all {num_pauses_webrtc} speech pauses in corpus entry (WebRTC): {sum_pauses_webrtc}')\n",
    "\n",
    "num_overlaps = entry_stats['#overlaps']\n",
    "sum_overlaps = entry_stats['#overlap_frames']\n",
    "print(f'WebRTC overlaps with original pauses {num_overlaps}/{num_pauses_raw} times ({100*num_overlaps/num_pauses_raw:.2f}%)')\n",
    "print(f'WebRTC overlaps with original pauses in {sum_overlaps}/{sum_pauses_raw} frames ({100*sum_overlaps/sum_pauses_raw:.2f}%)')\n",
    "\n",
    "avg_overlap = entry_stats['avg_overlap']\n",
    "print(f'WebRTC pauses coincide with original pauses with {avg_overlap}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further examine to what degree the speech pauses detected by WebRTC overlap with the speech pauses from the raw data for a whole corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compare_corpus(corpus, data_root):\n",
    "    total_orig_pauses = 0\n",
    "    total_orig_pause_frames = 0\n",
    "    total_webrtc_pauses = 0\n",
    "    total_webrtc_pause_frames = 0\n",
    "\n",
    "    entry_stats = []\n",
    "    for corpus_entry in tqdm(corpus, unit=' entries'):\n",
    "        entry_stat = compare_corpus_entry(corpus_entry, data_root)\n",
    "        entry_stats.append(entry_stat)\n",
    "\n",
    "    total_overlaps = len(entry_stats)\n",
    "    \n",
    "    corpus_stats = {}\n",
    "    \n",
    "    corpus_stats['raw'] = {}\n",
    "    corpus_stats['raw']['#pauses'] = sum(entry_stat['raw']['#pauses'] for entry_stat in entry_stats)\n",
    "    corpus_stats['raw']['#pause_frames'] = sum(entry_stat['raw']['#pause_frames'] for entry_stat in entry_stats)\n",
    "    \n",
    "    corpus_stats['webrtc'] = {}\n",
    "    corpus_stats['webrtc']['#pauses'] = sum(entry_stat['webrtc']['#pauses'] for entry_stat in entry_stats)\n",
    "    corpus_stats['webrtc']['#pause_frames'] = sum(entry_stat['webrtc']['#pause_frames'] for entry_stat in entry_stats)\n",
    "    \n",
    "    corpus_stats['#overlaps'] = sum(entry_stat['#overlaps'] for entry_stat in entry_stats)\n",
    "    corpus_stats['#overlap_frames'] = sum(entry_stat['#overlap_frames'] for entry_stat in entry_stats)\n",
    "    corpus_stats['avg_overlap'] = np.mean([entry_stat['avg_overlap'] for entry_stat in entry_stats])\n",
    "    \n",
    "    num_pauses_raw = corpus_stats['raw']['#pauses']\n",
    "    sum_pauses_raw = corpus_stats['raw']['#pause_frames']\n",
    "    print(f'Length of all {num_pauses_raw} speech pauses in corpus (ground truth): {sum_pauses_raw}')\n",
    "\n",
    "    num_pauses_webrtc = corpus_stats['webrtc']['#pauses']\n",
    "    sum_pauses_webrtc = corpus_stats['webrtc']['#pause_frames']\n",
    "    print(f'Length of all {num_pauses_webrtc} speech pauses in corpus (WebRTC): {sum_pauses_webrtc}')\n",
    "\n",
    "    num_overlaps = corpus_stats['#overlaps']\n",
    "    sum_overlaps = corpus_stats['#overlap_frames']\n",
    "\n",
    "    print(f'WebRTC overlaps with original pauses {num_overlaps}/{num_pauses_raw} times ({100*num_overlaps/num_pauses_raw:.2f}%)')\n",
    "    print(f'WebRTC overlaps with original pauses in {sum_overlaps}/{sum_pauses_raw} frames ({100*sum_overlaps/sum_pauses_raw:.2f}%)')\n",
    "\n",
    "    return corpus_stats\n",
    "\n",
    "corpus_stats = compare_corpus(rl_corpus, rl_data_root)\n",
    "# corpus_stats = compare_corpus(ls_corpus, ls_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
