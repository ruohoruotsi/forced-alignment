{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# VAD stage\n",
    "\n",
    "Segmentation information is the information about when in an audio signal someone is speaking. Because the training data consists of aligned text, this information will be given at training time: For a given corpus entry the speech segments and their transcription can easily be derived from metadata and used for training in the ASR-stage of the pipeline. However, there will be no segmentation information available at test time nor in production. The only thing known then will be the entire audio signal and its transcription. \n",
    "\n",
    "What is needed is the audio signal split into chunks, i.e. speech segments. Such speech segments can then be fed to the trained RNN, which will output a potentially faulty transcript which can then be aligned with the transcript of the whole recording in the LSA-stage.\n",
    "\n",
    "As stated in [the first notebook](00_introduction.ipynb) the original idea was to perform this chunking by using another RNN, which would learn how to detect speech pauses. However, instead of having to rely on such a RNN, we could try out detecting speech pauses using a VAD (_Voice Activity Detection_) algorithm. A VAD algorithm that is able to detect speech pauses with reasonable accuracy would free us from the task of detecting them ourselves (by training an RNN e.g.).\n",
    "\n",
    "This chapter will compare one state-of-the-art implementation for VAD against the segmentation information from the corpus data that was acquired through manual labelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = r'E:/' # define the path to where the corpus files are located!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.corpus_util import *\n",
    "from util.webrtc_util import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import librosa.display\n",
    "\n",
    "rl_corpus_root = os.path.join(corpus_root, 'readylingua-corpus')\n",
    "ls_corpus_root = os.path.join(corpus_root, 'librispeech-corpus')\n",
    "\n",
    "default_figsize = (12,5)\n",
    "default_facecolor = 'white'\n",
    "\n",
    "def show_wave(audio, sample_rate, ax=None, title=None):\n",
    "    if not ax:\n",
    "        plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "        \n",
    "    p = librosa.display.waveplot(audio.astype(float), sample_rate)\n",
    "    ax = p.axes\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def show_segments(ax, boundaries, ymin=0, ymax=1, color='red'):\n",
    "    for i, (start_frame, end_frame) in enumerate(boundaries):\n",
    "        rect = ax.axvspan(start_frame, end_frame, ymin=ymin, ymax=ymax, color=color, alpha=0.5)\n",
    "        y_0, y_1 = ax.get_ylim()\n",
    "        x = start_frame + (end_frame - start_frame)/2\n",
    "        y = y_0 + 0.01*(y_1-y_0) if ymin==0 else y_1 - 0.05*(y_1-y_0)\n",
    "        ax.text(x, y, str(i+1), horizontalalignment='center', fontdict={'family': 'sans-serif', 'size': 15, 'color': 'white'})          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_corpus = load_corpus(rl_corpus_root)\n",
    "ls_corpus = load_corpus(ls_corpus_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebRTC\n",
    "\n",
    "[WebRTC](https://webrtc.org/) is a free, open project that provides browsers and mobile applications with Real-Time Communications (RTC) capabilities via simple APIs. The WebRTC components have been optimized to best serve this purpose. There is also a VAD component, whose functionality has been [ported to Python by John Wiseman](https://github.com/wiseman/py-webrtcvad). It uses C code under the hood and is therefore very performant. Unfortunately, there is no information about the inner workings since there is no documentation available. Judging from the [source files](https://webrtc.googlesource.com/) however I suspect a Gaussian Mixture Model (GMM) is used to model the probability of a frame being speech or not.\n",
    "\n",
    "Execute the cell below to compare the pause segments detected by WebRTC together with the pause segments from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry = rl_corpus['news170524']\n",
    "# corpus_entry = random.choice(rl_corpus)\n",
    "# corpus_entry = rl_corpus[0]\n",
    "\n",
    "audio, rate = corpus_entry.audio, corpus_entry.rate\n",
    "display(Audio(data=audio, rate=rate))\n",
    "\n",
    "# pause boundaries from raw data\n",
    "original_boundaries = calculate_boundaries(corpus_entry.speech_segments)\n",
    "original_boundaries = original_boundaries / rate\n",
    "\n",
    "# pause boundaries from WebRTC\n",
    "webrtc_boundaries, voiced_segments = calculate_boundaries_webrtc(corpus_entry)\n",
    "\n",
    "title = f'Raw wave of {corpus_entry.audio_file}'\n",
    "ax_wave = show_wave(audio, rate, title=title)\n",
    "show_segments(ax_wave, original_boundaries, ymax=0.5, color='green')\n",
    "show_segments(ax_wave, webrtc_boundaries, ymin=0.5, color='blue')\n",
    "\n",
    "pause_segments_original = mpatches.Patch(color='green', alpha=0.6, label=f'original speech segments ({len(original_boundaries)})')\n",
    "pause_segments_webrtc = mpatches.Patch(color='blue', alpha=0.6, label=f'speech segments detected by WebRTC ({len(webrtc_boundaries)})')\n",
    "ax_wave.legend(handles=[pause_segments_original, pause_segments_webrtc], bbox_to_anchor=(0, -0.2, 1., -0.1), loc=3, mode='expand', borderaxespad=0, ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also listen to speech segments detected by WebRTC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def play_webrtc_sample(webrtc_sample):\n",
    "    audio = np.concatenate([frame.audio for frame in webrtc_sample])\n",
    "    display(Audio(data=audio, rate=rate))\n",
    "    \n",
    "[play_webrtc_sample(sample) for sample in (voiced_segments[i] for i in range(10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebRTC vs. manual segmentation\n",
    "\n",
    "By comparing the speech segments produced by WebRTC with the manually defined speech segments we can now calculate how much the speech pauses detected by WebRTC coincide with the speech pauses from raw data. To do this we can compare different metrics of the two results:\n",
    "\n",
    "* **Precision**: Percentage of audio frames in classified as \"speech\" by WebRTC that are were also  classified as \"speech\" by a human\n",
    "* **Recall**: Percentage of manually classified \"speech\" frames that were also detected by WebRTC\n",
    "* **Difference**: Difference between the number of speech segments detected by WebRTC and manual segmentation. A negative value means WebRTC detected fewer speech segments. A positive value means WebRTC detected more speech segments. A value of zero means both methods produced the same number of (but not neccessarily the same) speech segments.\n",
    "\n",
    "These metrics can be calculated for a corpus entry or the whole corpus. Precision and Recall can be further combined to a single value by calculating its **F-Score**:\n",
    "\n",
    "$$ F = 2 \\cdot \\frac{P \\cdot R}{P+R} $$\n",
    "\n",
    "The first two metrics have to be taken with a grain of salt though, because they depend on the definition of a speech pause, which is highly subjective. WebRTC provides a parameter which controls the \"aggressiveness\" of speech detection (values between 0 and 3). A higher value means higher aggressiveness, which results in a higher probability for a frame being classified as \"speech\" and therefore in more speech segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aggressiveness in 0, 1, 2, 3:\n",
    "    print(f'measuring precision/recall for WebRTC-VAD with aggressiveness={aggressiveness}')\n",
    "    p, r, f, d = precision_recall(corpus_entry, aggressiveness)\n",
    "    print(f'precision: {p:.3f}, recall: {r:3f}, F-score: {f:.3f}, difference: {d:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further examine to what degree the speech pauses detected by WebRTC overlap with the speech pauses from the raw data for a whole corpus. We do this by iterating over the whole corpus and perform above calculations for each corpus entry. The results for precision and recall can be averaged to get an idea of how well WebRTC generally performs. The results for the difference must be inspected more closely because the negative and positive values might cancel each other out, yielding an overall difference of zero, which is not correct since we are interested in the average difference of produced speech segments. We therefore differenciate three values for the difference:\n",
    "\n",
    "* **Negative Difference**: Average difference between the number of of speech segments produced by WebRTC and the number of manually defined speech segments. Only those corpus entries were considered, where WebRTC produced **less** speech segments than a human.\n",
    "* **Positive Difference**: Average difference between the number of of speech segments produced by WebRTC and the number of manually defined speech segments. Only those corpus entries were considered, where WebRTC produced **more** speech segments than a human.\n",
    "* **Average Difference**: Average difference between the number of speech segments produced by WebRTC and the number of manually defined speech segments. **All** corpus entries were considered. A negative value means WebRTC generally produced less speech segments than a human would. A positive value means WebRTC produced more speech segments than a human. A value of zero means WebRTC produced exactly the same number of speech segments **or the positive and negative difference would cancel each other out**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'Comparison of automatic/manual VAD for {rl_corpus.name} corpus'\n",
    "plot_stats(create_corpus_stats(rl_corpus), title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'Comparison of automatic/manual VAD for {ls_corpus.name} corpus'\n",
    "plot_stats(create_corpus_stats(ls_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and interpretation\n",
    "\n",
    "Aboce cell compares the manual and automatic segmentation by calculating the average precision, average recall and average difference in number of speech segments created. The comparison has been made for each corpus and for all levels of aggressiveness. Since this process takes some time, the following figures and table show the result of a previous run. The best results are marked green.\n",
    "\n",
    "#### Avg. Precision\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>.849</td>\n",
    "    <td>.850</td>\n",
    "    <td>.873</td>\n",
    "    <td style=\"background-color: lightgreen;\">.901</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Avg. Recall\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td style=\"background-color: lightgreen;\">.988</td>\n",
    "    <td>.987</td>\n",
    "    <td>.982</td>\n",
    "    <td>.970</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>    \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### F-Score\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>.910</td>\n",
    "    <td>.911</td>\n",
    "    <td>.422</td>\n",
    "    <td style=\"background-color: lightgreen;\">.931</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Differences in number of speech segments\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Avg. Difference</th>\n",
    "    <th colspan=\"4\">Avg. Difference (neg)</th>\n",
    "    <th colspan=\"4\">Avg. Difference (pos)</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>30.204</td>\n",
    "    <td>29.359</td>\n",
    "    <td>20.081</td>\n",
    "    <td style=\"background-color: lightgreen;\">16.645</td>\n",
    "    <td>-34.756</td>\n",
    "    <td>-34.312</td>\n",
    "    <td>-27.068</td>\n",
    "    <td style=\"background-color: lightgreen;\">-15.677</td>\n",
    "    <td>6.270</td>\n",
    "    <td  style=\"background-color: lightgreen;\">6.211</td>\n",
    "    <td>10.052</td>\n",
    "    <td>17.330</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReadyLingua corpus\n",
    "\n",
    "The following plot visualizes the results for the ReadyLingua corpus. We can clearly observe that the precision increases quite significantly with increasing aggressiveness. At the same time, recall decreases, but not to the same rate. In its highest setting for aggressiveness WebRTC is able to detect speech segments with an F-Score of 0.931, which corresponds to values for Precision and Recall of over 90%.\n",
    "\n",
    "The average difference in number of speech segments approaches zero with increasing aggressiveness. From the positive value we can conclude that WebRTC will generally produce more speech segments with increasing aggressiveness. For corpus entries, where WebRTC would produce more speech segments than a human, the difference is at only +6, meaning that when WebRTC produces more speech segments than a human the difference is only marginal. On the other hand the average difference when WebRTC produces fewer segments than a human, the difference is a higher.\n",
    "\n",
    "Generally speaking the performance of WebRTC-VAD can be considered very good, yielding results near-par to human performance when set to highest aggressiveness. The conclusion is to leave the aggressiveness of WebRTC-VAD at its highest setting (`3`).\n",
    "\n",
    "![WebRTC VAD vs. manual speech segmentation](../assets/webrtc_vs_manual_rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LibriSpeech corpus\n",
    "\n",
    "tbd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this chapter the state-of-the-art automatic VAD system from WebRTC was compared against manually defined segmentations from different sources. Even though the inner workings remain unclear, the automatically detected speech segments results showed very high similarity to manual segmentation and a very good perceived quality for randomly selected samples. \n",
    "\n",
    "These findings could be verified by measuring precision and recall for the whole corpora used in this project. Both were in ranges above 90% when set to suitable values for its aggressiveness. Given the highly subjective nature of speech segmentation, this is a very good result which makes WebRTC a valid candidate for the VAD-stage of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
