{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# VAD stage\n",
    "\n",
    "Segmentation is the process of splitting the audio into voiced and silent parts. Because the training data consists of aligned text, this information will be given at training time: For a given corpus entry the voiced segments and their transcription can easily be derived from metadata and used for training in the ASR-stage of the pipeline. However, there will be no segmentation information available at test time nor in production. The only thing known then will be the entire audio signal and its transcription. \n",
    "\n",
    "What is needed is a way to automatically extract the voiced parts, i.e. the speech segments, from an audio signal. Such speech segments can then be fed to the trained RNN, which will output a potentially faulty transcript which can then be aligned with the transcript of the whole recording later down the pipeline.\n",
    "\n",
    "As stated in [the first notebook](00_introduction.ipynb) the original idea was to perform this chunking by using another RNN, which would learn how to detect speech pauses. However, instead of having to rely on such a RNN, speech pauses can be detected by using an existing VAD (_Voice Activity Detection_) algorithm. A VAD algorithm that is able to detect speech pauses with reasonable accuracy would remove the task of training and tuning an own system and therefore save time.\n",
    "\n",
    "This chapter will compare one state-of-the-art implementation for VAD against the segmentation information from the corpus data that was acquired through manual labelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = r'E:/' # define the path to where the corpus files are located!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from util.corpus_util import *\n",
    "from util.vad_util import *\n",
    "from webrtc_comparison import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import librosa.display\n",
    "\n",
    "default_figsize = (12,5)\n",
    "default_facecolor = 'white'\n",
    "\n",
    "def show_wave(audio, sample_rate, ax=None, title=None):\n",
    "    if not ax:\n",
    "        plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "        \n",
    "    p = librosa.display.waveplot(audio.astype(float), sample_rate)\n",
    "    ax = p.axes\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def show_segments(ax, boundaries, ymin=0, ymax=1, color='red'):\n",
    "    for i, (start_s, end_s) in enumerate(boundaries):\n",
    "        rect = ax.axvspan(start_s, end_s, ymin=ymin, ymax=ymax, color=color, alpha=0.5)\n",
    "        y_0, y_1 = ax.get_ylim()\n",
    "        x = start_s + (end_s - start_s)/2\n",
    "        y = y_0 + 0.01*(y_1-y_0) if ymin==0 else y_1 - 0.05*(y_1-y_0)\n",
    "        ax.text(x, y, str(i+1), horizontalalignment='center', fontdict={'family': 'sans-serif', 'size': 15, 'color': 'white'})          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_corpus = get_corpus('rl')\n",
    "ls_corpus = get_corpus('ls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebRTC\n",
    "\n",
    "[WebRTC](https://webrtc.org/) is a free, open project that provides browsers and mobile applications with Real-Time Communications (RTC) capabilities via simple APIs. The WebRTC components have been optimized to best serve this purpose. There is also a VAD component, whose functionality has been [ported to Python by John Wiseman](https://github.com/wiseman/py-webrtcvad). It uses C code under the hood and is therefore very performant. Unfortunately, there is no information about the inner workings since there is no documentation available. Judging from the [source files](https://webrtc.googlesource.com/) however I suspect a Gaussian Mixture Model (GMM) is used to model the probability of a frame being speech or not.\n",
    "\n",
    "Execute the cell below to compare the pause segments detected by WebRTC together with the pause segments from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_entry = rl_corpus['news170524']\n",
    "# corpus_entry = random.choice(rl_corpus)\n",
    "corpus_entry = rl_corpus[0]\n",
    "\n",
    "audio, rate = corpus_entry.audio, corpus_entry.rate\n",
    "display(Audio(data=audio, rate=rate))\n",
    "\n",
    "# pause boundaries from raw data\n",
    "original_boundaries = calculate_boundaries(corpus_entry)\n",
    "\n",
    "# pause boundaries from WebRTC\n",
    "webrtc_boundaries = calculate_boundaries_webrtc(corpus_entry)\n",
    "\n",
    "# convert frames to seconds\n",
    "original_boundaries = original_boundaries / rate\n",
    "webrtc_boundaries = webrtc_boundaries / rate\n",
    "\n",
    "title = f'Raw wave of {corpus_entry.audio_file}'\n",
    "ax_wave = show_wave(audio, rate, title=title)\n",
    "show_segments(ax_wave, original_boundaries, ymax=0.5, color='green')\n",
    "show_segments(ax_wave, webrtc_boundaries, ymin=0.5, color='blue')\n",
    "\n",
    "pause_segments_original = mpatches.Patch(color='green', alpha=0.6, label=f'original speech segments ({len(original_boundaries)})')\n",
    "pause_segments_webrtc = mpatches.Patch(color='blue', alpha=0.6, label=f'speech segments detected by WebRTC ({len(webrtc_boundaries)})')\n",
    "ax_wave.legend(handles=[pause_segments_original, pause_segments_webrtc], bbox_to_anchor=(0, -0.2, 1., -0.1), loc=3, mode='expand', borderaxespad=0, ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also listen to speech segments detected by WebRTC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voiced_segments = webrtc_voice(corpus_entry.audio, corpus_entry.rate)    \n",
    "for voice in list(voiced_segments)[:10]:\n",
    "    display(Audio(data=voice.audio, rate=voice.rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebRTC vs. manual segmentation\n",
    "\n",
    "By comparing the speech segments produced by WebRTC with the manually defined speech segments we can now calculate how much the speech pauses detected by WebRTC coincide with the speech pauses from raw data. To do this we can compare different metrics of the two results:\n",
    "\n",
    "* **Precision**: Percentage of audio frames in classified as \"speech\" by WebRTC that are were also  classified as \"speech\" by a human\n",
    "* **Recall**: Percentage of manually classified \"speech\" frames that were also detected by WebRTC\n",
    "* **Difference**: Difference between the number of speech segments detected by WebRTC and manual segmentation. A negative value means WebRTC detected fewer speech segments. A positive value means WebRTC detected more speech segments. A value of zero means both methods produced the same number of (but not neccessarily the same) speech segments.\n",
    "\n",
    "These metrics can be calculated for a corpus entry or the whole corpus. Precision and Recall can be further combined to a single value by calculating its **F-Score**:\n",
    "\n",
    "$$ F = 2 \\cdot \\frac{P \\cdot R}{P+R} $$\n",
    "\n",
    "The first two metrics have to be taken with a grain of salt though, because they depend on the definition of a speech pause, which is highly subjective. WebRTC provides a parameter which controls the \"aggressiveness\" of speech detection (values between 0 and 3). A higher value means higher aggressiveness, which results in a higher probability for a frame being classified as \"speech\" and therefore in more speech segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aggressiveness in 0, 1, 2, 3:\n",
    "    print(f'measuring precision/recall for WebRTC-VAD with aggressiveness={aggressiveness}')\n",
    "    p, r, f, n_orig, n_webrtc = precision_recall(corpus_entry, aggressiveness)\n",
    "    print(f'# speech segments (manual): {n_orig}')\n",
    "    print(f'# speech segments (WebRTC): {n_webrtc}')\n",
    "    print(f'precision: {p:.3f}, recall: {r:3f}, F-score: {f:.3f}')\n",
    "    print(f'------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further examine to what degree the speech pauses detected by WebRTC overlap with the speech pauses from the raw data for a whole corpus. We do this by iterating over the whole corpus and perform above calculations for each corpus entry. The results for precision and recall can be averaged to get an idea of how well WebRTC generally performs. The results for the difference must be inspected more closely because the negative and positive values might cancel each other out, yielding an overall difference of zero, which is not correct since we are interested in the average difference of produced speech segments. We therefore differenciate three values for the difference:\n",
    "\n",
    "* **Negative Difference**: Average difference between the number of of speech segments produced by WebRTC and the number of manually defined speech segments for cases where WebRTC produced **fewer** speech segments than a human.\n",
    "* **Positive Difference**: Average difference between the number of of speech segments produced by WebRTC and the number of manually defined speech segments for cases where WebRTC produced **more** speech segments than a human.\n",
    "* **Average Difference**: Average absolute difference between the number of speech segments produced by WebRTC and the number of manually defined speech segments. **All** corpus entries were considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'Comparison of automatic/manual VAD for {rl_corpus.name} corpus'\n",
    "rl_stats = create_corpus_stats(rl_corpus)\n",
    "plot_stats(rl_stats, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'Comparison of automatic/manual VAD for {ls_corpus.name} corpus'\n",
    "ls_stats = create_corpus_stats(ls_corpus)\n",
    "plot_stats(ls_stats, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and interpretation\n",
    "\n",
    "Above cell calculate the metrics for both the _ReadyLingua_ and the _LibriSpeech_ corpus. They do so by splitting the audio signal of each corpus entry into speech parts with WebRTC using all different values for the aggressivenes. Average precision, average recall and differences in number of speech segments can then be calculated by comparing the results with the manually defined speech segments.\n",
    "\n",
    "Since this process can take a lot of time (especially for the LibriSpeech corpus, which contains more than 1000 hours of audio) the values have been calculated beforehand. The following charts show their development with varying values for the aggressiveness. The values of each metric are listed in the following tables. The best value is highlighted. The value for Recall has to be taken with a pinch of salt though: A high value can be easily achieved by treating the whole audio signal as a single speech segment. Also, the audio frames are heavily skewed towards lots of frames containing speech and only a few non-speech frames between the speech parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReadyLingua corpus\n",
    "\n",
    "![WebRTC VAD vs. manual speech segmentation for ReadyLingua corpus](../assets/webrtc_vs_manual_rl.png)\n",
    "\n",
    "We can clearly observe a clear trend towards higher precision and recall with increasing aggressiveness. The best value for the F-score is reached with WebRTC-VAD set to its highest aggressiveness, corresponding to a value for the precision in the mid-eighties.\n",
    "\n",
    "The plot also shows that the difference in the number of speech segments produced by WebRTC-VAD decreases with increasing aggressiveness. However, WebRTC will generally produce fewer speech segments than a human, resulting in an average negative difference which is considerably smaller than the average positive difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LibriSpeech corpus\n",
    "\n",
    "![WebRTC VAD vs. manual speech segmentation for LibriSpeech corpus](../assets/webrtc_vs_manual_ls.png)\n",
    "\n",
    "We can observe a slightly decreasing F-score. This is a direct result of the fact that precision remains constant while recall decreases for higher values of the aggressiveness. However both precision and F-score remain in an interval of 82-88%. These are rather high values. In its highest aggressiveness, WebRTC produces significantly more speech segments than with the next lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg. Precision\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>.768</td>\n",
    "    <td>.787</td>\n",
    "    <td>.816</td>\n",
    "    <td style=\"background-color: lightgreen;\">.846</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>.830</td>\n",
    "    <td>.831</td>\n",
    "    <td>.831</td>\n",
    "    <td style=\"background-color: lightgreen;\">.831</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg. Recall\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>.877</td>\n",
    "    <td>.892</td>\n",
    "    <td>.908</td>\n",
    "    <td style=\"background-color: lightgreen;\">.932</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td style=\"background-color: lightgreen;\">.967</td>\n",
    "    <td>.967</td>\n",
    "    <td>.964</td>\n",
    "    <td>.950</td>    \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-Score\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>.808</td>\n",
    "    <td>.824</td>\n",
    "    <td>.847</td>\n",
    "    <td style=\"background-color: lightgreen;\">.876</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>.886</td>\n",
    "    <td style=\"background-color: lightgreen;\">.886</td>\n",
    "    <td>.885</td>\n",
    "    <td>.879</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in number of speech segments\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Avg. Difference</th>\n",
    "    <th colspan=\"4\">Avg. Difference (neg)</th>\n",
    "    <th colspan=\"4\">Avg. Difference (pos)</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>30.204</td>\n",
    "    <td>29.359</td>\n",
    "    <td>20.081</td>\n",
    "    <td style=\"background-color: lightgreen;\">16.645</td>\n",
    "    <td>-34.756</td>\n",
    "    <td>-34.312</td>\n",
    "    <td>-27.068</td>\n",
    "    <td style=\"background-color: lightgreen;\">-15.677</td>\n",
    "    <td>6.270</td>\n",
    "    <td style=\"background-color: lightgreen;\">6.211</td>\n",
    "    <td>10.052</td>\n",
    "    <td>17.330</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>23.708</td>\n",
    "    <td style=\"background-color: lightgreen;\">23.268</td>\n",
    "    <td>24.779</td>\n",
    "    <td>43.467</td>\n",
    "    <td>-25.317</td>\n",
    "    <td>-24.083</td>\n",
    "    <td>-20.4</td>\n",
    "    <td style=\"background-color: lightgreen;\">-12.870</td>\n",
    "    <td style=\"background-color: lightgreen;\">21.538</td>\n",
    "    <td>22.774</td>\n",
    "    <td>28.850</td>\n",
    "    <td>46.203</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook the state-of-the-art automatic VAD system from WebRTC was compared against manually defined segmentations from different sources. Even though the inner workings remain unclear, the automatically detected speech segments results showed a very good perceived quality for randomly selected samples. \n",
    "\n",
    "The subjectively assessed high similarity to manual segmentation could be verified by measuring precision and recall for the corpora used in this project. Both were in ranges around 90% when set to suitable values for its aggressiveness. Given the highly subjective nature of speech segmentation, this is a very good result which makes WebRTC a valid candidate for the VAD-stage of the pipeline.\n",
    "\n",
    "The performance of WebRTC-VAD can be considered very good, yielding results near-par to human performance when set to highest aggressiveness. The conclusion is to leave the aggressiveness of WebRTC-VAD at its highest setting (`3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
