{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix\n",
    "\n",
    "The purpose of this notebook is to give some orientation when exploring the code base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "The code was written for [Python](https://www.python.org/) `3.6.6` using [Anaconda](https://anaconda.org/) `4.5.8`.\n",
    "\n",
    "### Dependencies\n",
    "* This application uses [Pydub](http://pydub.com/) which means you will need **libav** or **ffmpeg** on your `PATH`. See the [Pydub Repository](https://github.com/jiaaro/pydub#installation) for further instructions.\n",
    "* Visual C++ build tools to work with webrtcvad (google it for download link): must be installed before installing the python requirements (see below)!\n",
    "\n",
    "### Installation\n",
    "1. Clone [the repository](https://github.com/tiefenauer/forced-alignment): `git clone git@github.com:tiefenauer/forced-alignment.git` \n",
    "2. Install Python requirements: `pip install -r requirements.txt`\n",
    "3. Install [TensorFlow](https://www.tensorflow.org/install/): TF is not included in `requirements.txt` because you can choose between the `tensorflow` (no GPU acceleration) and `tensorflow-gpu` (with GPU-acceleration). If your computer does not have a CUDA-supported GPU (like mine does) you will install the former, else the latter. Installing `tensorflow-gpu` on a computer without GPU does not work (at least I did not get it to work).\n",
    "3. Run Jupyter Notebook: `jupyter notebook`\n",
    "\n",
    "### Jupyter Notebook extensions\n",
    "\n",
    "The following extensions for Jupyter Notebook were used:\n",
    "\n",
    "* [jupyter_contrib_nbextensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions): A collection of useful extensions (like a TOC) that also includes a manager that allows enabling/disabling individual extensiosn from the web interface\n",
    "* [cite2c](https://github.com/takluyver/cite2c): For managing citations  (works with [Zotero](https://www.zotero.org/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Structure\n",
    "\n",
    "| Folder | Description |\n",
    "|---|---|\n",
    "| / | root folder containing some Bash scripts to train the RNN |\n",
    "| assets | binary data (images, audio, etc...) used for the Jupyter Notebooks |\n",
    "| demos | HTML-files to visualize the result of the alignment pipeline. |\n",
    "| src | scripts and Python source files containing all application logic. Also, the documentation is stored here |\n",
    "| test | some unit tests |\n",
    "| tmp | temporary folder, e.g. needed for the VAD stage. No persistent files should be stored here as this folder might be deleted at any time by application logic! |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important scripts\n",
    "\n",
    "The following scripts exist in the `src` folder. Type\n",
    "\n",
    "    python {script-name}.py -h\n",
    "    \n",
    "to see how to use the script (arguments, flags, default values, ...). The code is self-documenting as far as possible but has been amended with some helpful comments where necessary. \n",
    "\n",
    "The most important scripts are:\n",
    "\n",
    "* **`create_dataset.py`**: precompute audio features (MFCC, Mel-Spectrogram or Power-Spectrograms) of a corpus\n",
    "* **`create_ls_corpus.py`**: (Re-)create the LibriSpeech corpus from raw data\n",
    "* **`create_rl_corpus.py`**: (Re-)create the ReadyLingua corpus from raw data\n",
    "* **`e2e_demo.py`**: (Re-)create the HTML- and JS-files needed to demonstrate the result of the processing pipeline (end-to-end). This works for both corpus entries or arbitrary combinations of audio/transcript.\n",
    "* **`test_brnn.py`**: Evaluate a trained model by loading it from disk, making predictions and measuring some metrics for data that was not seen during training (test-set)\n",
    "* **`train_brnn.py`**: Train the model used for the ASR stage (simplified DeepSpeech model)\n",
    "* **`train_poc.py`**: Train the PoC (simple unidirectional RNN) with different features \n",
    "* **`vad_demo.py`**: Explore the VAD stage by splitting an audio file into speech segments (either using WebRTC or detecting silent intervals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
