{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix\n",
    "\n",
    "The purpose of this notebook is to give some orientation when exploring the code base.\n",
    "\n",
    "## Structure\n",
    "\n",
    "| Folder | Description |\n",
    "|---|---|\n",
    "| / | root folder containing some Bash scripts to train the RNN |\n",
    "| assets | binary data (images, audio, etc...) used for the Jupyter Notebooks |\n",
    "| demos | HTML-files to visualize the result of the alignment pipeline. |\n",
    "| src | scripts and Python source filesc containing all application logic. Also, the documentation is stored here |\n",
    "| test | some unit tests |\n",
    "| tmp | temporary folder, e.g. needed for the VAD stage. No persistent files should be stored here as this folder might be deleted at any time by application logic! |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important scripts\n",
    "\n",
    "The following scripts exist in the `src` folder. Type\n",
    "\n",
    "    python {script-name}.py -h\n",
    "    \n",
    "to see how to use the script (arguments, flags, default values, ...). The code is self-documenting as far as possible but has been amended with some helpful comments where necessary. \n",
    "\n",
    "The most important scripts are:\n",
    "\n",
    "* **`create_dataset.py`**: precompute audio features (MFCC, Mel-Spectrogram or Power-Spectrograms) of a corpus\n",
    "* **`create_ls_corpus.py`**: (Re-)create the LibriSpeech corpus from raw data\n",
    "* **`create_rl_corpus.py`**: (Re-)create the ReadyLingua corpus from raw data\n",
    "* **`e2e_demo.py`**: (Re-)create the HTML- and JS-files needed to demonstrate the result of the processing pipeline (end-to-end). This works for both corpus entries or arbitrary combinations of audio/transcript.\n",
    "* **`smith-waterman.py`**: Implementation of the Smith-Waterman algorithm for local sequence alignment\n",
    "* **`test_brnn.py`**: Evaluate a trained model by loading it from disk, making predictions and measuring some metrics for data that was not seen during training (test-set)\n",
    "* **`train_brnn.py`**: Train the model used for the ASR stage (simplified DeepSpeech model)\n",
    "* **`train_poc.py`**: Train the PoC (simple unidirectional RNN) with different features \n",
    "* **`vad_demo.py`**: Explore the VAD stage by splitting an audio file into speech segments (either using WebRTC or detecting silent intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
