{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "The purpose of this notebook is to show how Forced Alignment on an unknown audio/transcript pair could be done by combining all the stages from the previous notebooks:\n",
    "\n",
    "- **VAD-Stage**: The speech parts are extracted from the audio signal using WebRTC\n",
    "- **ASR-Stage**: The speech parts are transcribed using an RNN. Because only PoCs were trained for this stage, ceiling analysis is done for this stage by using a state-of-the art model. We will use [Google's Speech-to-Text API](https://cloud.google.com/speech-to-text/) for this.\n",
    "- **LSA-Stage**: The partial transcripts are aligned with the original transcript.\n",
    "\n",
    "All these stages are applied on a single corpus entry for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = r'E:/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def show_audio(corpus_entry):\n",
    "    title = HTML(f\"\"\"\n",
    "    <h3>Sample corpus entry: {corpus_entry.name}</h3>\n",
    "    <p><strong>Path to raw data</strong>: {corpus_entry.original_path}</p>\n",
    "    <p>{len(corpus_entry.speech_segments)} speech segments, {len(corpus_entry.pause_segments)} pause segments</p>\n",
    "    \"\"\")\n",
    "    audio = Audio(data=corpus_entry.audio, rate=corpus_entry.rate)\n",
    "    transcript = widgets.Accordion(children=[widgets.HTML(f'<pre>{corpus_entry.transcript}</pre>')], selected_index=None)\n",
    "    transcript.set_title(0, 'Transcript')\n",
    "    \n",
    "    display(title)\n",
    "    display(audio)\n",
    "    display(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The candidate\n",
    "We will use an English transcript read by a female speaker in US-English. The following entry from the LibriSpeech corpus has been randomly selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from util.corpus_util import load_corpus\n",
    "\n",
    "rl_corpus_root = os.path.join(corpus_root, 'librispeech-corpus')\n",
    "rl_corpus = load_corpus(rl_corpus_root)\n",
    "corpus_entry= rl_corpus['171001']\n",
    "print(f'id: {corpus_entry.id}')\n",
    "print(f'path to raw data: {corpus_entry.original_path}')\n",
    "print(f'name: {corpus_entry.name}')\n",
    "print(f'language: {corpus_entry.language}')\n",
    "print(f'speaker id: {corpus_entry.speaker_id}')\n",
    "print(f'chapter id: {corpus_entry.chapter_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAD Stage\n",
    "The audio signal of the candidate is approximately 30 minutes long and can be split into 548 segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.vad_util import *\n",
    "\n",
    "audio, rate = corpus_entry.audio, corpus_entry.rate\n",
    "voice_activities = extract_voice_activities(audio, rate)\n",
    "print(f'got {len(voice_activities)} voice_activities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASR stage\n",
    "Each segment can now be transcribed by using the Google-STT API. Note that it may take some time to process all segments. Therefore only the first 10 speech segments are transcribed here for demonstration purposes. **Also note that free usage of the API is constrained to a time and/or call limit. Excessively executing the following cell will therefore lead to deplete the usage limit!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.asr_util import *\n",
    "\n",
    "print('transcripts of first 10 speech segments (generated by ASR)')\n",
    "print()\n",
    "partial_transcripts = []\n",
    "for i, va in enumerate(voice_activities[:10], 1):    \n",
    "    partial_transcript = transcribe_audio(va.audio, va.rate)\n",
    "    partial_transcripts.append(partial_transcript)\n",
    "    print(i, partial_transcript)\n",
    "    display(Audio(data=va.audio, rate=va.rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these transcripts with the original transcript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio(corpus_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the generated transcripts are not perfect. By comparing with the original transcript, one can immediately spot some errors in the transcripts generated by the RNN (e.g. _stubbly ratchet_ instead of _doubly wretched_, _traps a prima_ instead of _trap's abramuh_ etc.). Also for the last speech segment the ASR-Model was unable to generate a transcript.\n",
    "\n",
    "On the other hand, we can see that even though the generated transcript contain some errors, their pronunciation is actually very close to the original transcript and could indeed be a valid transcription in other situations (e.g. _LC_ instead of _Elsie_). Also the words and sentences of the transcripts are orthographically and grammatically correct, a clear indication that a language model has been used to improve the raw results from the first pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA-Stage\n",
    "\n",
    "To see whether the output quality of the ASR stage is high enough to align the individual partial transcripts with the original transcript, the Smith-Waterman algorithm from the LSA stage can be applied. The resulting (textual) alignment and the temporal information from the speech segments can then be combined to obtain an alignment between the original audio/transcript pair.\n",
    "\n",
    "For simplicity, the transcripts for the 548 detected speech segments have been transcribed with the Google-STT engine and the result has been savet [to a file](../assets/171001.txt). The following code calculates the alignments for the 10 partial transcripts above. Again, for simplicity, the alignments for all partial transcripts have been pre-calculated for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.lsa_util import *\n",
    "from tabulate import tabulate\n",
    "\n",
    "alignments = align_transcripts(partial_transcripts, corpus_entry.transcript)\n",
    "print(tabulate(alignments, headers=['partial', 'original', 'b']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The so retreived sequence alignment information can be combined with the temporal information to get the alignment between audio signal and transcript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
