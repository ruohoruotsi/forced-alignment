{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Voice Activity Detection\n",
    "\n",
    "The above plots show the segmentation of the audio signal into speech and pause segments using the ground truth derived from the metadata that was provided together with the raw data. However, instead of having to rely on such metadata being present, we could try out detecting speech pauses automatically using a VAD (Voice Activity Detection) algorithm. A VAD algorithm that is able to detect speech pauses with reasonable accuracy would free us from the task of detecting them ourselves (by training an RNN e.g.).\n",
    "\n",
    "## WebRTC\n",
    "\n",
    "[WebRTC](https://webrtc.org/) is a free, open project that provides browsers and mobile applications with Real-Time Communications (RTC) capabilities via simple APIs. The WebRTC components have been optimized to best serve this purpose. There is also a VAD component, whose functionality has been [ported to Python by John Wiseman](https://github.com/wiseman/py-webrtcvad). It uses C code under the hood and is therefore very performant.\n",
    "\n",
    "Execute the cell below to compare the pause segments detected by WebRTC together with the pause segments from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boundaries_webrtc(corpus_entry, aggressiveness=3):\n",
    "    voiced_segments, _ = split_segments(corpus_entry, aggressiveness=aggressiveness)\n",
    "    boundaries = []\n",
    "    for frames in voiced_segments:\n",
    "        start_time = frames[0].timestamp\n",
    "        end_time = (frames[-1].timestamp + frames[-1].duration)\n",
    "        boundaries.append((start_time, end_time))\n",
    "    return 2*np.array(boundaries), voiced_segments\n",
    "\n",
    "# corpus_entry = random.choice(rl_corpus)\n",
    "corpus_entry = rl_corpus['news170524']\n",
    "# corpus_entry = rl_corpus[0]\n",
    "\n",
    "audio, rate = corpus_entry.audio, corpus_entry.rate\n",
    "display(Audio(data=audio, rate=rate))\n",
    "\n",
    "# pause boundaries from raw data\n",
    "original_boundaries = calculate_boundaries(corpus_entry.speech_segments)\n",
    "original_boundaries = original_boundaries / rate\n",
    "\n",
    "# pause boundaries from WebRTC\n",
    "webrtc_boundaries, voiced_segments = calculate_boundaries_webrtc(corpus_entry)\n",
    "\n",
    "title = f'Raw wave of {corpus_entry.audio_file}'\n",
    "ax_wave = show_wave(audio, rate, title=title)\n",
    "show_segments(ax_wave, original_boundaries, ymax=0.5, color='green')\n",
    "show_segments(ax_wave, webrtc_boundaries, ymin=0.5, color='blue')\n",
    "\n",
    "pause_segments_original = mpatches.Patch(color='green', alpha=0.6, label=f'original speech segments ({len(original_boundaries)})')\n",
    "pause_segments_webrtc = mpatches.Patch(color='blue', alpha=0.6, label=f'speech segments detected by WebRTC ({len(webrtc_boundaries)})')\n",
    "ax_wave.legend(handles=[pause_segments_original, pause_segments_webrtc], bbox_to_anchor=(0, -0.2, 1., -0.1), loc=3, mode='expand', borderaxespad=0, ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also listen to speech segments detected by WebRTC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voiced_segments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-51f0cd9a1a46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAudio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mplay_webrtc_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvoiced_segments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-51f0cd9a1a46>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAudio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mplay_webrtc_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvoiced_segments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-51f0cd9a1a46>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAudio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mplay_webrtc_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvoiced_segments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'voiced_segments' is not defined"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def play_webrtc_sample(webrtc_sample):\n",
    "    audio = np.concatenate([frame.audio for frame in webrtc_sample])\n",
    "    display(Audio(data=audio, rate=rate))\n",
    "    \n",
    "[play_webrtc_sample(sample) for sample in (voiced_segments[i] for i in range(10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebRTC vs. manual segmentation\n",
    "\n",
    "We can calculate how much the speech pauses automatically detected by WebRTC coincide with the speech pauses from raw data, which were manually defined. To do this we can compare different metrics of the two results:\n",
    "\n",
    "* **Precision**: Percentage of audio frames in classified as \"speech\" by WebRTC that are were actually manually classified \"speech\"\n",
    "* **Recall**: Percentage of manually classified \"speech\" frames that were also detected by WebRTC\n",
    "* **Difference**: Difference between the number of speech segments detected by WebRTC and manual segmentation. A negative value means WebRTC detected fewer speech segments. A positive value means WebRTC detected more speech segments. A value of zero means both methods produced the same number of (but not neccessarily the same) speech segments.\n",
    "\n",
    "These metrics can be calculated for a corpus entry or the whole corpus. Precision and Recall can be further combined to a single value by calculating its **F-Score**:\n",
    "\n",
    "$$ F = 2 \\cdot \\frac{P \\cdot R}{P+R} $$\n",
    "\n",
    "The first two metrics have to be taken with a grain of salt though, because they depend on the definition of a speech pause, which is highly subjective. WebRTC provides a parameter which controls the \"aggressiveness\" of speech detection (values between 0 and 3). A higher value means higher aggressiveness, which results in a higher probability for a frame being classified as \"speech\" and therefore in more speech segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measuring precision/recall for WebRTC-VAD with aggressiveness=0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'corpus_entry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4aeb96e67549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0maggressiveness\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'measuring precision/recall for WebRTC-VAD with aggressiveness={aggressiveness}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_recall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_entry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggressiveness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'precision is: {p}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'recall is: {r}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus_entry' is not defined"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def getOverlap(a, b):\n",
    "    return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n",
    "\n",
    "def calc_intersection(a, b):\n",
    "    a = sorted(a, key=itemgetter(0))\n",
    "    b = sorted(b, key=itemgetter(0))\n",
    "    for start_a, end_a in a:\n",
    "        x = set(range(start_a, end_a + 1))\n",
    "        for start_b, end_b in ((s, e) for (s, e) in b if getOverlap((s, e), (start_a, end_a))):\n",
    "            y = range(start_b, end_b + 1)\n",
    "            intersection = x.intersection(y)\n",
    "            if intersection:\n",
    "                yield min(intersection), max(intersection)\n",
    "\n",
    "def precision_recall(corpus_entry, aggressiveness):\n",
    "    boundaries_original = calculate_boundaries(corpus_entry.speech_segments)\n",
    "    boundaries_webrtc, _ = calculate_boundaries_webrtc(corpus_entry, aggressiveness=aggressiveness)\n",
    "    boundaries_webrtc = boundaries_webrtc * corpus_entry.rate # convert to frames\n",
    "    boundaries_webrtc = boundaries_webrtc.astype(int)\n",
    "    \n",
    "    intersections = calc_intersection(boundaries_original, boundaries_webrtc)\n",
    "    n_frames_intersection = sum(len(range(start, end + 1)) for start, end in intersections)\n",
    "    n_frames_original = sum(len(range(start, end + 1)) for start, end in boundaries_original)\n",
    "    n_frames_webrtc = sum(len(range(start, end + 1)) for start, end in boundaries_webrtc)\n",
    "    \n",
    "    p = n_frames_intersection / (n_frames_webrtc + 1e-3)\n",
    "    r = n_frames_intersection / (n_frames_original + 1e-3)\n",
    "    f = 2.0 * p * r / (p + r + 1e-3)\n",
    "    d = len(boundaries_webrtc) - len(boundaries_original)\n",
    "    \n",
    "    return p, r, f, d\n",
    "\n",
    "for aggressiveness in 0,1,2,3:\n",
    "    print(f'measuring precision/recall for WebRTC-VAD with aggressiveness={aggressiveness}')\n",
    "    p, r, f, d = precision_recall(corpus_entry, aggressiveness)\n",
    "    print(f'precision is: {p}')\n",
    "    print(f'recall is: {r}')\n",
    "    print(f'F-score is: {f}')\n",
    "    print(f'difference: {d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further examine to what degree the speech pauses detected by WebRTC overlap with the speech pauses from the raw data for a whole corpus. We do this by iterating over the whole corpus and perform above calculations for each corpus entry. The results for precision and recall can be averaged to get an idea of how well WebRTC generally performs. The results for the difference must be inspected more closely because the negative and positive values might cancel each other out, yielding an overall difference of zero, which is not correct since we are interested in the average difference of produced speech segments. We therefore differenciate three values for the difference:\n",
    "\n",
    "* **Absolute Difference**: Average of the absolute values of the differences over all corpus entries\n",
    "* **Negative Difference**: Average of the negative values of the differences over all corpus entries (corpus entries where WebRTC produced less speech segments than a human)\n",
    "* **Positive Difference**: Average of the positive values of the differences over all corpus entries (corpus entries where WebRTC produced more speech segments than a human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rl_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-13b69fc252b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'Comparison of automatic/manual VAD for {rl_corpus.name} corpus'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[0mplot_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_corpus_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rl_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from util.log_util import print_to_file_and_console\n",
    "\n",
    "def compare_corpus(corpus, corpus_root, aggressiveness):\n",
    "#     np.seterr(all='raise')\n",
    "    p_r_f_d = list(tqdm((precision_recall(corpus_entry, aggressiveness) for corpus_entry in corpus[:1]), total=len(corpus)))\n",
    "    p_r_f_d = np.asarray(p_r_f_d)\n",
    "    avg_p, avg_r, avg_f, avg_d = np.abs(p_r_f_d).mean(axis=0)\n",
    "    ds = p_r_f_d[:,3]\n",
    "    avg_d_neg = np.extract(ds < 0, ds).mean()\n",
    "    avg_d_pos = np.extract(ds > 0, ds).mean()\n",
    "\n",
    "    return avg_p, avg_r, avg_f, avg_d, avg_d_neg, avg_d_pos\n",
    "\n",
    "def create_corpus_stats(corpus):\n",
    "    print(f'Comparing automatic/manual VAD for {corpus.name} corpus')\n",
    "    stats = {'Aggressiveness': [0,1,2,3], 'Precision': [], 'Recall': [], 'F-Score': [], 'Difference (absolute)': [], 'Difference (negative)': [], 'Difference (positive)': []}\n",
    "    for aggressiveness in stats['Aggressiveness']:\n",
    "        print(f'precision/recall with aggressiveness={aggressiveness}\\n')\n",
    "        avg_p, avg_r, avg_f, avg_d, avg_d_neg, avg_d_pos = compare_corpus(rl_corpus, rl_corpus_root, aggressiveness)\n",
    "        stats['Precision'].append(avg_p)\n",
    "        stats['Recall'].append(avg_r)\n",
    "        stats['F-Score'].append(avg_f)\n",
    "        stats['Difference (absolute)'].append(avg_d)\n",
    "        stats['Difference (negative)'].append(avg_d_neg)\n",
    "        stats['Difference (positive)'].append(avg_d_pos)\n",
    "\n",
    "    stats_file = os.path.join(corpus.root_path, 'corpus.stats')\n",
    "    if os.path.exists(stats_file):\n",
    "        os.remove(stats_file)\n",
    "    print(f'Writing results to {stats_file}')\n",
    "    f = print_to_file_and_console(stats_file)        \n",
    "    print(tabulate(stats, headers='keys'))\n",
    "    f.close()\n",
    "    return stats\n",
    "\n",
    "def plot_stats(stats, title=None):\n",
    "    x = stats['Aggressiveness']\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=default_figsize, facecolor=default_facecolor)\n",
    "    if title:\n",
    "        ax1.set_title(title)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xlabel('aggressiveness')\n",
    "    ax1.set_ylabel('precision/recall/F-score')\n",
    "    p, = ax1.plot(x, np.array(stats['Precision']), color='r', label='Precision')\n",
    "    r, = ax1.plot(x, np.array(stats['Recall']), color='g', label='Recall')\n",
    "    r, = ax1.plot(x, np.array(stats['F-Score']), color='b', label='F-Score')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('difference')\n",
    "    d_abs, = ax2.plot(x, np.array(stats['Difference (absolute)']), color='c', label='Difference (absolute)')\n",
    "    d_neg, = ax2.plot(x, np.array(stats['Difference (negative)']), color='m', label='Difference (negative)')\n",
    "    d_pos, = ax2.plot(x, np.array(stats['Difference (positive)']), color='y', label='Difference (positive)')\n",
    "    \n",
    "    plt.legend(handles=[p, r, d_abs, d_neg, d_pos], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "title = f'Comparison of automatic/manual VAD for {rl_corpus.name} corpus'\n",
    "plot_stats(create_corpus_stats(rl_corpus), title=title)\n",
    "\n",
    "# title = f'Comparison of automatic/manual VAD for {ls_corpus.name} corpus'\n",
    "# plot_stats(create_corpus_stats(ls_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results and interpretation\n",
    "\n",
    "Aboce cell compares the manual and automatic segmentation by calculating the average precision, average recall and average difference in number of speech segments created. The comparison has been made for each corpus and for all levels of aggressiveness. Since this process takes some time, the following figures and table show the result of a previous run. The best results are marked green.\n",
    "\n",
    "###### Avg. Precision\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>.849</td>\n",
    "    <td>.850</td>\n",
    "    <td>.873</td>\n",
    "    <td style=\"background-color: lightgreen;\">.901</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "###### Avg. Recall\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>.988</td>\n",
    "    <td>.987</td>\n",
    "    <td>.982</td>\n",
    "    <td style=\"background-color: lightgreen;\">.970</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>    \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "###### F-Score\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Aggressiveness</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>.456</td>\n",
    "    <td>.457</td>\n",
    "    <td>.462</td>\n",
    "    <td style=\"background-color: lightgreen;\">.467</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "###### Differences in number of speech segments\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Corpus</th>\n",
    "    <th colspan=\"4\">Avg. Difference (abs)</th>\n",
    "    <th colspan=\"4\">Avg. Difference (neg)</th>\n",
    "    <th colspan=\"4\">Avg. Difference (pos)</th>\n",
    "  </th>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReadyLingua</td>\n",
    "    <td>30</td>\n",
    "    <td>29</td>\n",
    "    <td>20</td>\n",
    "    <td style=\"background-color: lightgreen;\">17</td>\n",
    "    <td>-29</td>\n",
    "    <td>-18</td>\n",
    "    <td>-16</td>\n",
    "    <td style=\"background-color: lightgreen;\">-6</td>\n",
    "    <td style=\"background-color: lightgreen;\">1</td>\n",
    "    <td style=\"background-color: lightgreen;\">1</td>\n",
    "    <td>4</td>\n",
    "    <td>11</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LibriSpeech</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "    <td>tbd</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "###### ReadyLingua corpus\n",
    "\n",
    "The following plot visualizes the results for the ReadyLingua corpus. We can clearly observe that the precision increases quite significantly with increasing aggressiveness. At the same time, recall decreases, but not to the same rate. In its highest setting for aggressiveness WebRTC is able to detect speech segments with an F-Score of 0.467, which corresponds to values for Precision and Recall of over 90%.\n",
    "\n",
    "The average difference in number of speech segments also approaches to zero with increasing aggressiveness. The average difference of corpus entries, where WebRTC would produce less speech segments than a human is at only -6, meaning that when WebRTC produces fewer speech segments than a human there are on average 6 speech segments less than a human would produce. Again, this is valid for highest aggressiveness. On the other hand the average difference when WebRTC produces more segments than a human, the difference starts to increase with increasing aggressiveness.  However, the sum of absolute values of difference is still lowest with a value of 3 for the agressivenes. We can conclude that generally WebRTC will produce more speech segments with increasing aggressiveness.\n",
    "\n",
    "Generally speaking the performance of WebRTC-VAD can be considered very good, yielding results near-par to human performance when set to highest aggressiveness. The conclusion is to leave the aggressiveness of WebRTC-VAD at its highest setting (`3`).\n",
    "\n",
    "![WebRTC VAD vs. manual speech segmentation](../assets/webrtc_vs_manual_rl.png)\n",
    "\n",
    "###### LibriSpeech corpus\n",
    "\n",
    "tbd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
