{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP8: Creation of Labelled Data\n",
    "The RNN will be trained on spectrograms of the audio data from the created corpora. Since this process is computationally expensive and requires a lot of time. To speed up the iterations when training the RNN and get feedback faster, the input data (the spectrograms) are pre-computed and stored on disk. Also, the labels (the information about speech pauses) need to be encoded in a suitable format. This notebook describes how this is done.\n",
    "\n",
    "Before we start, define a path to an empty directory with enough free storage where the labelled data can be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_root = r'E:/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's do the imports and some helper functions before we start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, Audio\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from create_labelled_data import create_X_Y\n",
    "from util.corpus_util import *\n",
    "from util.audio_util import *\n",
    "from util.webrtc_util import *\n",
    "\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "from IPython.display import HTML, Audio\n",
    "import librosa.display\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "\n",
    "rl_corpus_root = os.path.join(target_root, 'readylingua-corpus')\n",
    "ls_corpus_root = os.path.join(target_root, 'librispeech-corpus')\n",
    "\n",
    "rl_data_root = os.path.join(target_root, 'readylingua-data')\n",
    "ls_data_root = os.path.join(target_root, 'librispeech-data')\n",
    "\n",
    "default_figsize = (12,4)\n",
    "default_facecolor = 'white'\n",
    "default_font = {'family': 'serif', \n",
    "                'weight': 'normal', \n",
    "#                 'size': 12\n",
    "               }\n",
    "\n",
    "plt.rc('font', **default_font)\n",
    "\n",
    "def show_labelled_data(corpus_entry, data_root):\n",
    "    display(HTML(f'<h3>{corpus_entry.name} (id={corpus_entry.id})</h3>'))\n",
    "    display(HTML(f'{len(corpus_entry.speech_segments)} speech segments, {len(corpus_entry.pause_segments)} pause segments'))\n",
    "    \n",
    "    # audio data\n",
    "    display(Audio(data=corpus_entry.audio, rate=corpus_entry.rate))\n",
    "    \n",
    "    fig = plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "    \n",
    "    # plot spectrogram\n",
    "    ax_spec = fig.add_subplot(211)\n",
    "    title = f'Spectrogram of ' + corpus_entry.audio_file\n",
    "    freqs, times, spec = corpus_entry.spectrogram()\n",
    "    ax_spec, extent = show_spectrogram(freqs, times, spec, ax_spec, title)\n",
    "    \n",
    "    # plot raw wave\n",
    "    ax_wave = fig.add_subplot(212)\n",
    "    title = f'Raw wave of {corpus_entry.audio_file} with speech pauses'\n",
    "    ax_wave = show_wave(corpus_entry.audio, corpus_entry.rate, ax_wave, title)\n",
    "    \n",
    "    # overlay pauses\n",
    "    left, right, bottom, top = extent\n",
    "    boundaries_frames = calculate_pause_boundaries_from_ground_truth(corpus_entry)\n",
    "\n",
    "    show_pause_segments(ax_spec, (right-left) * boundaries_frames / len(corpus_entry.audio))\n",
    "    show_pause_segments(ax_wave, boundaries_frames)\n",
    "        \n",
    "    return ax_spec, ax_wave\n",
    "\n",
    "def show_spectrogram(freqs, times, spec, ax=None, title=None):\n",
    "    if not ax:\n",
    "        plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "\n",
    "    extent = [times.min(), times.max(), freqs.min(), freqs.max()]\n",
    "    \n",
    "    print(f'spec.shape = (f, T_x) = {spec.shape}')\n",
    "    im = plt.imshow(spec, aspect='auto', origin='lower', extent=extent)\n",
    "    \n",
    "    ax = im.axes\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    ax.set_xlim(times.min(), times.max())\n",
    "    ax.set_yticks(freqs[::16])\n",
    "    ax.set_xticks(times[::int(len(times)/10)])\n",
    "    \n",
    "    ax.set_xlabel('Seconds')\n",
    "    ax.set_ylabel('Freqs in Hz')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return im.axes, extent    \n",
    "        \n",
    "# def show_spectrogram(spec, sample_rate, step_size, scale=None, title=None):\n",
    "#     y_axis = scale if scale else 'hz'\n",
    "#     ax = librosa.display.specshow(spec, sr=sample_rate, hop_length=step_size, \n",
    "#                                   cmap='viridis', x_axis='time', y_axis=y_axis)\n",
    "    \n",
    "#     if scale:\n",
    "#         plt.colorbar(format='%+2.0f dB')\n",
    "#     if title:\n",
    "#         plt.title(title)\n",
    "#     plt.tight_layout()    \n",
    "#     return ax\n",
    "    \n",
    "def show_spectrogram_3d(spec, window_size=320, step_size=160, sample_rate=16000, title=None):\n",
    "    times = np.arange(window_size/2, spec.shape[-1] - window_size/2 + 1, window_size - step_size)/float(sample_rate)\n",
    "    data = [go.Surface(z=spec)]\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        scene = dict(\n",
    "            xaxis = dict(title='Time', range=times),\n",
    "            yaxis = dict(title='Frequencies', range=freqs),\n",
    "            zaxis = dict(title='Log amplitude'),\n",
    "            ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)      \n",
    "    \n",
    "# def show_wave(audio, sample_rate, title=None):\n",
    "#     p = librosa.display.waveplot(audio.astype(float), sample_rate)\n",
    "#     ax = p.axes\n",
    "#     ax.set_ylabel('Amplitude')\n",
    "#     plt.title(title)\n",
    "#     plt.tight_layout()\n",
    "#     return ax\n",
    "\n",
    "def show_wave(audio, sample_rate, ax=None, title=None):\n",
    "    if not ax:\n",
    "        plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "        ax = plt.axes()\n",
    "        \n",
    "    ax.set_xlim(0, len(audio))\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.set_xlabel('Audio frames')\n",
    "    ax.plot(np.linspace(0, len(audio), len(audio)), audio)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "    \n",
    "def show_pause_segments(ax, boundaries, ymin=0, ymax=1, color='red'):\n",
    "    for pause_start, pause_end in boundaries:\n",
    "        ax.axvspan(pause_start, pause_end, ymin=ymin, ymax=ymax, color=color, alpha=0.8)\n",
    "    \n",
    "def calculate_pause_boundaries_from_ground_truth(corpus_entry):\n",
    "    \"\"\"calculates the boundaries of pause segments in x given a label vector y.\n",
    "    \n",
    "    :y: numpy array of shape (1, T_y) containing the labels (\"speech\"/\"no speech\") for a RNN\n",
    "    :x: numpy array of shape (T_x, ) containing the audio signal for a RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    x = corpus_entry.audio\n",
    "    y = corpus_entry.labels\n",
    "    \n",
    "    num_x = len(x)\n",
    "    \n",
    "    # pause boundaries as binary vector: [0,0,1,1,1,0,...]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    # pause boundaries as indices of 1-groups in y (start and end indices of group): [[2,4], ...]\n",
    "    boundaries = np.flatnonzero(np.diff(np.r_[0,y,0]) != 0).reshape(-1,2) - [0,1]\n",
    "    \n",
    "    # pause boundaries as indices of 1-groups in x (calculated from relative position of frames in y)\n",
    "    boundaries = len(x) * boundaries / len(y)\n",
    "    \n",
    "    # no fractional indices\n",
    "    return boundaries.astype(int)\n",
    "\n",
    "def calculate_boundaries_from_webrtc(segments, audio, sample_rate):\n",
    "    boundaries = []\n",
    "    for frames in segments:\n",
    "        start_frame = frames[0].timestamp * sample_rate\n",
    "        end_frame = (frames[-1].timestamp + frames[-1].duration) * sample_rate\n",
    "        boundaries.append((start_frame, end_frame))\n",
    "    return 2 * np.array(boundaries).astype(int)\n",
    "    \n",
    "def on_create_data_rl_button_click(sender):\n",
    "    rl_target_root = os.path.join(target_root, 'readylingua-data')\n",
    "    create_X_Y(ls_corpus, rl_target_root)\n",
    "    \n",
    "def on_create_data_ls_button_click(sender):\n",
    "    ls_target_root = os.path.join(target_root, 'librispeech-data')\n",
    "    create_X_Y(ls_corpus, ls_target_root)      \n",
    "    \n",
    "# UI elements\n",
    "layout = widgets.Layout(width='250px', height='50px')\n",
    "create_data_rl_btn = widgets.Button(description=\"Create labelled data for ReadyLingua\", button_style='info', layout=layout, icon='download')\n",
    "create_data_rl_btn.on_click(on_create_data_rl_button_click)\n",
    "create_data_ls_btn = widgets.Button(description=\"Create labelled data for LibriSpeech\", button_style='info', layout=layout, icon='download')\n",
    "create_data_ls_btn.on_click(on_create_data_ls_button_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having created the corpora from raw data we can now start creating labelled data (spectrograms and labels) for the RNN. This data is stored as numpy arrays whose dimensions partially depend on the proposed network architecture. \n",
    "\n",
    "The RNN is trained on audio data (sequence of frames) and will output whether a specific section in the audio signal is speech or pause (sequence of labels). Because both the input and the output is a sequence, it is a sequence-to-sequence model with a **many-to-many** architecture. This means we have the following values to consider:\n",
    "\n",
    "* $T_x$: Number of sequence tokens in an individual sample. This value may be different for each sample!\n",
    "* $T_y$: Number of sequence tokens in the output. This value is always the same for each sample but may be different from $T_x$\n",
    "\n",
    "In the following sections the following variable names are used to denote the two components of the labelled data:\n",
    "\n",
    "* `X`: The actual data, i.e. the spectrograms. One spectrogram is created per corpus entry and saved to disk. The saved data consists of three components:\n",
    "  * `freqs`: The frequencies used in the spectrogram (array of shape $(161, 1)$)\n",
    "  * `times`: The time steps used in the spectrogram (array of shape $(T_x, 1)$)\n",
    "  * `spec`: The spectrogram data (array of shape $(T_x, 161)$)\n",
    "  'freqs' and 'times' are only needed to plot the spectrogram along in a Cartesian coordinate system, where the time steps will be plotted along the x-axis and  frequencies along the y-axis. For training, only the `spec` part is needed.\n",
    "* `Y`: The labels, i.e. the information about speech- or pause segments. The labels are encoded as 1-dimensional binary vectors of shape $(1, T_y)$. A speech segment will be encoded as a sequence of zeroes and a pause segment as a sequence of ones. Pause sections may contain some signal (e.g. background noise) but no spoken text from the transcript.\n",
    "\n",
    "Let's load the created corpora to make them available to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_corpus = load_corpus(rl_corpus_root)\n",
    "ls_corpus = load_corpus(ls_corpus_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev/Test split\n",
    "The labelled data is split into subsets for training (_train-set_), parameter tuning (_dev-set_) and model evaluation (_test-set_). Since the corpora were create from different sources of raw data, they vary in size and probability distribution (number of languages, homogeneity of the recording quality, ratio of male vs. female speakers, presence of distortions like reverb or overdrive, and many more). Since the starting point for the creation of the corpus was so variable, different approaches were taken to split the corpus up into train-, dev- and test-set.\n",
    "\n",
    "### ReadyLingua corpus\n",
    "The raw data exhibits a high variance with respect to relevant features (recording quality, length of samples, presence of distortion, ...). Since the corpus is rather small there may be only one sample for a specific feature value (e.g. only one recording with reverb). Therefore to keep things simple the split into train-, dev- and test-set was done with a 80/10/10-rule without closer examination of the underlying data. This might not result in an optimal split since it would be possible for example that all the female speakers will be put in one subset.\n",
    "\n",
    "Improvements could be made by manually assigning each sample to a specific set by carefully inspecting the relevant features. The corpus could also be extended by creating synthetisized data, e.g. creating samples with reverb from the original samples. Because the LibriSpeech corpus looks much more promising at the moment, this time was not invested.\n",
    "\n",
    "### LibriSpeech corpus\n",
    "The LibriSpeech raw data is already split into train-, dev- and test-set. Each chapter is read by a different speaker. Each speaker is only contained in one of the subsets. Efforts have been made to keep the different sets within the same probability distributions (regarding to accents, ratio of male/female speakers, ...). The information about the subset has been preserved when creating the corpora from raw data. To leverage the efforts made by the LibriSpeech project, the corresponding labelled data will be kept in the same subset.\n",
    "\n",
    "---\n",
    "\n",
    "You can explore the size of the subsets for each corpus by executing the cell below to see the number of samples (corpus entries) in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_train, ls_dev, ls_test = ls_corpus.train_dev_test_split()\n",
    "print(f'LibriSpeech corpus ({len(ls_corpus)} samples): #train-samples: {len(ls_train)}, #dev-samples: {len(ls_dev)}, #test-samples: {len(ls_test)}')\n",
    "\n",
    "rl_train, rl_dev, rl_test = rl_corpus.train_dev_test_split()\n",
    "print(f'ReadyLingua corpus ({len(rl_corpus)} samples): #train-samples: {len(rl_train)}, #dev-samples: {len(rl_dev)}, #test-samples: {len(rl_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature extraction\n",
    "\n",
    "In order to train an RNN, each sample needs to be converted into some sort of sequence. In this case the samples are the audio files from the corpus entries that were converted to wave files (`*.wav`) and downsampled to 16kHz (mono).\n",
    "\n",
    "### Raw waves\n",
    "As the name suggests the wave files contain the audio signal as a raw wave, which is just a series of discrete sample values. Because we used a sampling rate of 16kHz we get 16'000 sample values per second. A sample value corresponds to the amplitude of the waveform at the given time step. These values can be stored in a 1-dimensional Numpy array and plotted in two dimension (time vs. amplitude).\n",
    "\n",
    "For example consider the a raw wave for a random speech segment. Feel free to change the first line to visualize the raw wave for a specific corpus entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry = rl_corpus['20161124weeklyaddressthanksgiving']\n",
    "speech_segment = corpus_entry.speech_segments[49]\n",
    "\n",
    "# corpus_entry = random.choice(rl_corpus) # uncomment for random corpus entry\n",
    "# speech_segment = random.choice(corpus_entry.speech_segments) # uncomment for random speech segment\n",
    "\n",
    "print(f'number of sampling points: {speech_segment.audio.shape[0]}, sampling rate: {speech_segment.rate}')\n",
    "print(f'transcript: {speech_segment.transcript}')\n",
    "\n",
    "display(Audio(data=speech_segment.audio, rate=speech_segment.rate))\n",
    "title = f'Raw wave of speech segments in {corpus_entry.id}.wav'\n",
    "\n",
    "fig = plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "show_wave(speech_segment.audio, speech_segment.rate, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From raw waves to spectrograms\n",
    "\n",
    "Although already a sequence, training on the raw wave would not be very useful since we would only have one feature (the amplitude) per time step. However, an audio signal just a bunch of overlaying frequencies of different phases and amplitudes. For a given time slot (_window_), the raw signal can be decomposed into its underlying frequencies using Fourier Transformation, yielding the amplitude of each frequency. \n",
    "These values can be stored in a 1-D array of shape $(f \\times 1)$, whereas $f$ denotes the number of frequencies.\n",
    "\n",
    "Since we will be using spectrograms as input values `X` to train an RNN, $T_x$ denotes the number of windows that can be calculated from the audio signal. Hence all the windows together form a matrix of shape ($f \\times T_x$) where each entry corresponds to the amplitude of frequency $f$ in window $T_x$. Such a matrix is called a **spectrogram**. A spectrogram can be visualized by color-coding the values. Consider the following spectrogram derived from the raw wave above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs, times, spec = log_specgram(speech_segment.audio, speech_segment.rate)\n",
    "show_spectrogram(freqs, times, spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values have been put on a logarithmic scale. Such a spectrogram can now be calculated for every speech segment. The following table contains all relevant parameters:\n",
    "\n",
    "| Symbol | Variable in code | Value | Description |\n",
    "|---|---|---|---|\n",
    "| $n$ | `num_values` | - | number of discrete sampling values in audio signal |\n",
    "| $r$ | `sample_rate` | - | sampling rate of audio signal |\n",
    "| $w_{ms}$ | `window_size_ms` | 20 | Window length in ms |\n",
    "| $w$ | `window_size` | 320 | Window length in frames |\n",
    "| $s_{ms}$ | `step_size_ms` | 10 | Step length in ms |\n",
    "| $s$ | `step_size` | 160 | Step length in frames $(s = \\frac{r \\cdot s_{ms}}{1000})$ |\n",
    "\n",
    "Note that the window and step length in frames unit can be derived from their values in milliseconds by calculating $w = \\frac{r \\cdot w_{ms}}{1000}$ or $s = \\frac{r \\cdot s_{ms}}{1000}$ respectively.\n",
    "\n",
    "To calculate the spectrogram for an audio signal, a sliding window of size $w$ is moved over the sample values with step size $s$. Note that the step size is usually smaller than the window size which means the windows will overlap to a certain degree. For any given audio signal $x$ the number of windows $T_x$ can be calculated by dividing the number of sampling values by the size of the overlap:\n",
    "\n",
    "$$\n",
    "T_x = \\left\\lfloor \\frac{n}{(w-s)} + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "The flooring is needed because the window size might not match up exactly with the number of sample values, resulting in fractional values for $T_x$. Since we will use the windows of the spectrogram as input to an RNN, $T_x$ corresponds to the number of training samples. Therefore only whole numbers make sense.\n",
    "\n",
    "According to the [Nyquist theorem](https://en.wikipedia.org/wiki/Nyquist_rate) the sampling frequency of an audio signal must be (at least) twice the frequency of the signal frequency in order to being able to reconstruct the original signal from the discrete sampling values. Since our sampling rate is 16kHz this means the maximum frequency that can be reproduced is 8kHz. Therefore the frequencies in our spectrogram are all in the range $0..\\frac{r}{2} = 0..8000$ Hz. This interval can be divided into equally sized sections. Including the borders of these sections this gives us $f$ equidistant sampling frequencies. The value for $f$ can be calculated as follows:\n",
    "\n",
    "$$\n",
    "f = \\frac{w}{2} + 1\n",
    "$$\n",
    "\n",
    "Note that we add 1 at the end because the borders (lowest and the highest frequency) are both included.\n",
    "\n",
    "Since the frequency band of the spectrogram will be spaced equally, the distance between two sample frequencies is $\\frac{r}{2\\cdot (f - 1)}$. This means that frequency phase $F_i$ in the frequency band can be calculated as follows:\n",
    "\n",
    "$$\n",
    "F_i = i \\cdot \\frac{r}{2 \\cdot (f - 1)}\n",
    "$$\n",
    "\n",
    "**Example**:\n",
    "\n",
    "For this project all audio signals were re-sampled with with a sampling rate $r=16000$. To calculate the spectrogram we use a sliding window of $w_{ms}=20ms$ length and a step size of $s_{ms}=10ms$ . In frame units this gives us the values $w=\\frac{16000 \\cdot 20 ms}{1000 ms} = 320$ and $r=\\frac{16000 \\cdot 10 ms}{1000 ms} = 160$.\n",
    "\n",
    "As stated above the frequencies all lie in the interval $[0..8000]$. This band is now divided into sections giving us $f = \\frac{320}{2} + 1 = 161$ sample frequencies, whereas the distance between each frequency is $\\Delta f = \\frac{16000}{2 \\cdot (161 - 1)} = 50 Hz$. The $i$-th sample frequency can therefore be calculated as. $F_i = i \\cdot 50$. The frequencies in the spectrogram are then:\n",
    "\n",
    "    [0, 50, 100, 150, ... , 7950, 8000]\n",
    "\n",
    "The raw wave for the example speech sequence above consists of $n = 9760$ sample values. Using a window size of $w=320$ frames and a step size of $s=160$ frames we arrive at a value of $T_x = \\left\\lfloor \\frac{9760}{(320-160)} + 1 \\right\\rfloor = 62$ training samples.\n",
    "\n",
    "We can verify this for the above spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_ms, step_size_ms, num_vals = 20, 10, speech_segment.audio.shape[0]\n",
    "\n",
    "print(f'n = {num_vals}\\t(number of sample values)')\n",
    "print(f'r = {speech_segment.rate}\\t(sample rate)')\n",
    "print(f'w_ms = {window_size_ms}\\t(window size in ms)')\n",
    "print(f's_ms = {step_size_ms}\\t(step size in ms)')\n",
    "print()\n",
    "\n",
    "window_size = ms_to_frames(window_size_ms, speech_segment.rate)\n",
    "step_size = ms_to_frames(step_size_ms, speech_segment.rate)\n",
    "\n",
    "print(f'w = {window_size}\\t\\t(window size in frames)')\n",
    "print(f's = {step_size}\\t\\t(step size in frames)')\n",
    "print()\n",
    "\n",
    "f, T_x = spec.shape\n",
    "print(f'spec.shape = (f, T_x) = ({f}, {T_x})')\n",
    "print()\n",
    "\n",
    "delta_f = int(speech_segment.rate / (2 * (f - 1)))\n",
    "print(f'delta_f = {delta_f}\\t(difference between sample frequencies in Hz)')\n",
    "print()\n",
    "\n",
    "freqs = np.array(range(0, f*delta_f, delta_f))\n",
    "print('Frequencies (y-Axis):')\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power spectrograms\n",
    "\n",
    "We can measure the power spectrum of a spectrogram by putting the values on a logarithmic scale (decibel units). We can visualize the results by plotting the DB values along the two axes (time and frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_log = pow_specgram(speech_segment.audio, window_size, step_size)\n",
    "title = f'3D spectrogram of speech segments in {corpus_entry.id}.wav'\n",
    "show_spectrogram_3d(spec, window_size=window_size, step_size=step_size, sample_rate=speech_segment.rate, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce the above 3D-plot by one dimension by flattening it along the z-axis (amplitude). We don't lose any information because the third dimension (dB value) is color-encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(freqs, times, spec, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel power spectrograms\n",
    "\n",
    "Alternatively, we can calculate the features on the Mel-Scale.\n",
    "\n",
    "Note that the number of features is usually smaller than when calculating the spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=f'Mel-spectrogram of speech segment in {corpus_entry.id}.wav'\n",
    "mel_spec = mel_specgram(speech_segment.audio, speech_segment.rate, window_size=window_size, step_size=step_size, n_mels=120)\n",
    "show_spectrogram(mel_spec, scale='mel', sample_rate=speech_segment.rate, step_size=step_size, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: MFCC\n",
    "\n",
    "As an alternative to Spectrograms we could use Mel Frequency Cepstral Coefficients (MFCC) as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a spectrogram is now created as a matrix `x` for every single corpus entry. A label vector `y` is also created for each corpus entry. This leaves us with two files for each entry. Since the spectrograms can become quite big, separate files are created for each entry. The files share a common naming pattern to identify their type (spectrogram or label), subset membership (train-, dev- or test-set) and corresponding corpus entry (ID of the corpus entry):\n",
    "\n",
    "`{id}.{X|Y}.{train|dev|test}.npy`\n",
    "\n",
    "For example the following files will be created for a corpus entry in the dev-set with ID `1234`:\n",
    "\n",
    "```\n",
    "1234.X.dev.npy\n",
    "1234.Y.dev.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precomputing the featuers\n",
    "\n",
    "To speed up the training the spectrograms can be pre-computed and stored on disk. Creating the labelled data might take some time (around 15 minutes for the ReadyLingua corpus up to several hours for the LibriSpeech corpus). Click the button below to start computing the spectrograms and label vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(widgets.HBox([create_data_rl_btn, create_data_ls_btn]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the labelled data\n",
    "\n",
    "After the labelled data has been created, we can explore an entire corpus entry by visualizing its spectrogram together with the segmentation information. Execute the following cell to explore a random sample from the ReadyLingua corpus. In contrast to the spectrograms above this will not calculate the spectrogram for the entire entry, not just a single speech segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# corpus_entry = random.choice(rl_corpus)\n",
    "# corpus_entry = rl_corpus[0]\n",
    "corpus_entry = rl_corpus['news170524']\n",
    "show_labelled_data(corpus_entry, rl_data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can detect speech pauses using a VAD (Voice Activity Detection) algorithm like the one from [WebRTC](https://webrtc.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_entry = random.choice(rl_corpus)\n",
    "\n",
    "display(Audio(data=corpus_entry.audio, rate=corpus_entry.rate))\n",
    "\n",
    "# pause boundaries from raw data\n",
    "original_boundaries = calculate_pause_boundaries_from_ground_truth(corpus_entry)\n",
    "\n",
    "# pause boundaries from WebRTC\n",
    "voiced_segments, unvoiced_segments = split_segments(corpus_entry)\n",
    "webrtc_boundaries = calculate_boundaries_from_webrtc(unvoiced_segments, corpus_entry.audio, corpus_entry.rate)\n",
    "\n",
    "title = f'Raw wave of {corpus_entry.audio_file} with {len(original_boundaries)} original speech pauses (red) and {len(webrtc_boundaries)} speech pauses detected by WebRTC (green)'\n",
    "ax_wave = show_wave(corpus_entry.audio, corpus_entry.rate, title=title)\n",
    "show_pause_segments(ax_wave, original_boundaries, ymax=0.5)\n",
    "show_pause_segments(ax_wave, webrtc_boundaries, ymin=0.5, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate how much the speech pauses detected by WebRTC coincide with the speech pauses from raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_overlaps(original_boundaries, webrtc_boundaries):\n",
    "    boundaries = np.concatenate((original_boundaries, webrtc_boundaries))\n",
    "    \n",
    "    prev_start = -1\n",
    "    prev_end = -1\n",
    "    num_overlaps = 0\n",
    "    overlaps = []\n",
    "    for start, end in np.sort(boundaries, axis=0):\n",
    "        if start <= prev_end: # we have an overlap\n",
    "            overlap_start, overlap_end = (start, min(end, prev_end))\n",
    "            \n",
    "            # calculate number of frames that overlap\n",
    "            overlap_frames = overlap_end - overlap_start\n",
    "            \n",
    "            # calculate degree of overlap\n",
    "            segment_frames = prev_end - prev_start\n",
    "            overlap_ratio = overlap_frames/segment_frames\n",
    "            \n",
    "            overlaps.append((overlap_frames, overlap_ratio))\n",
    "        prev_start = start\n",
    "        prev_end = end\n",
    "        \n",
    "    return overlaps\n",
    "\n",
    "def compare_corpus_entry(corpus_entry, data_root):\n",
    "    entry_stats = {}\n",
    "    original_boundaries = calculate_pause_boundaries_from_ground_truth(corpus_entry)\n",
    "    \n",
    "    entry_stats['raw'] = {}\n",
    "    entry_stats['raw']['#pauses'] = len(corpus_entry.pause_segments)\n",
    "    entry_stats['raw']['#pause_frames'] = sum([end_frame - start_frame for start_frame, end_frame in original_boundaries])\n",
    "    \n",
    "    voiced_segments, unvoiced_segments = split_segments(corpus_entry)\n",
    "    webrtc_boundaries = calculate_boundaries_from_webrtc(unvoiced_segments, corpus_entry.audio, corpus_entry.rate)    \n",
    "    \n",
    "    entry_stats['webrtc'] = {}\n",
    "    entry_stats['webrtc']['#pauses'] = len(unvoiced_segments)\n",
    "    entry_stats['webrtc']['#pause_frames'] = sum([end_frame - start_frame for start_frame, end_frame in webrtc_boundaries])\n",
    "\n",
    "    overlaps = calculate_overlaps(original_boundaries, webrtc_boundaries)\n",
    "    \n",
    "    entry_stats['#overlaps'] = len(overlaps)\n",
    "    entry_stats['#overlap_frames'] = np.sum([overlap_frames for overlap_frames, _ in overlaps])\n",
    "    entry_stats['avg_overlap'] = np.mean([overlap_ratio for _, overlap_ratio in overlaps])\n",
    "    \n",
    "    return entry_stats\n",
    "    \n",
    "entry_stats = compare_corpus_entry(corpus_entry, rl_data_root)\n",
    "\n",
    "num_pauses_raw = entry_stats['raw']['#pauses']\n",
    "sum_pauses_raw = entry_stats['raw']['#pause_frames']\n",
    "print(f'Length of all {num_pauses_raw} speech pauses in corpus entry (ground truth): {sum_pauses_raw}')\n",
    "\n",
    "num_pauses_webrtc = entry_stats['webrtc']['#pauses']\n",
    "sum_pauses_webrtc = entry_stats['webrtc']['#pause_frames']\n",
    "print(f'Length of all {num_pauses_webrtc} speech pauses in corpus entry (WebRTC): {sum_pauses_webrtc}')\n",
    "\n",
    "num_overlaps = entry_stats['#overlaps']\n",
    "sum_overlaps = entry_stats['#overlap_frames']\n",
    "print(f'WebRTC overlaps with original pauses {num_overlaps}/{num_pauses_raw} times ({100*num_overlaps/num_pauses_raw:.2f}%)')\n",
    "print(f'WebRTC overlaps with original pauses in {sum_overlaps}/{sum_pauses_raw} frames ({100*sum_overlaps/sum_pauses_raw:.2f}%)')\n",
    "\n",
    "avg_overlap = entry_stats['avg_overlap']\n",
    "print(f'WebRTC pauses coincide with original pauses with {avg_overlap}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further examine to what degree the speech pauses detected by WebRTC overlap with the speech pauses from the raw data for a whole corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compare_corpus(corpus, data_root):\n",
    "    total_orig_pauses = 0\n",
    "    total_orig_pause_frames = 0\n",
    "    total_webrtc_pauses = 0\n",
    "    total_webrtc_pause_frames = 0\n",
    "\n",
    "    entry_stats = []\n",
    "    for corpus_entry in tqdm(corpus, unit=' entries'):\n",
    "        entry_stat = compare_corpus_entry(corpus_entry, data_root)\n",
    "        entry_stats.append(entry_stat)\n",
    "\n",
    "    total_overlaps = len(entry_stats)\n",
    "    \n",
    "    corpus_stats = {}\n",
    "    \n",
    "    corpus_stats['raw'] = {}\n",
    "    corpus_stats['raw']['#pauses'] = sum(entry_stat['raw']['#pauses'] for entry_stat in entry_stats)\n",
    "    corpus_stats['raw']['#pause_frames'] = sum(entry_stat['raw']['#pause_frames'] for entry_stat in entry_stats)\n",
    "    \n",
    "    corpus_stats['webrtc'] = {}\n",
    "    corpus_stats['webrtc']['#pauses'] = sum(entry_stat['webrtc']['#pauses'] for entry_stat in entry_stats)\n",
    "    corpus_stats['webrtc']['#pause_frames'] = sum(entry_stat['webrtc']['#pause_frames'] for entry_stat in entry_stats)\n",
    "    \n",
    "    corpus_stats['#overlaps'] = sum(entry_stat['#overlaps'] for entry_stat in entry_stats)\n",
    "    corpus_stats['#overlap_frames'] = sum(entry_stat['#overlap_frames'] for entry_stat in entry_stats)\n",
    "    corpus_stats['avg_overlap'] = np.mean([entry_stat['avg_overlap'] for entry_stat in entry_stats])\n",
    "    \n",
    "    num_pauses_raw = corpus_stats['raw']['#pauses']\n",
    "    sum_pauses_raw = corpus_stats['raw']['#pause_frames']\n",
    "    print(f'Length of all {num_pauses_raw} speech pauses in corpus (ground truth): {sum_pauses_raw}')\n",
    "\n",
    "    num_pauses_webrtc = corpus_stats['webrtc']['#pauses']\n",
    "    sum_pauses_webrtc = corpus_stats['webrtc']['#pause_frames']\n",
    "    print(f'Length of all {num_pauses_webrtc} speech pauses in corpus (WebRTC): {sum_pauses_webrtc}')\n",
    "\n",
    "    num_overlaps = corpus_stats['#overlaps']\n",
    "    sum_overlaps = corpus_stats['#overlap_frames']\n",
    "\n",
    "    print(f'WebRTC overlaps with original pauses {num_overlaps}/{num_pauses_raw} times ({100*num_overlaps/num_pauses_raw:.2f}%)')\n",
    "    print(f'WebRTC overlaps with original pauses in {sum_overlaps}/{sum_pauses_raw} frames ({100*sum_overlaps/sum_pauses_raw:.2f}%)')\n",
    "\n",
    "    return corpus_stats\n",
    "\n",
    "corpus_stats = compare_corpus(rl_corpus, rl_data_root)\n",
    "# corpus_stats = compare_corpus(ls_corpus, ls_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
