{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results and Conclusion\n",
    "\n",
    "Pipelining the audio signal and transcript through various stages has proven to be successful, provided the quality of the audio signal is high enough and the speaker makes long enough pauses to be picked up in the VAD stage. The critical component is the ASR stage, because all downstream stages and the end result depend on it. \n",
    "\n",
    "It is somewhat unsatisfactory that no own ASR system could be trained on the available data. The reason for this are manifold:\n",
    "\n",
    "* high requirements for computational power (a lot of time was invested into engineering a highly optimized system and to precompute features as far as possible)\n",
    "* therefore extremely long feedback cycles (50+ days training time when trained on the _LibriSpeech_ corpus)\n",
    "* vast amounts of data are needed (the few hours from _ReadyLingua_ did not seem to suffice)\n",
    "\n",
    "Although an ASR system could be trained on the 27 hours of audio data in the _ReadyLingua_ corpus, this system did not produce transcriptions that came close to the real text when evaluated on a test set. It is unclear whether this is only a result of above points, lack of data, data quality, the chosen architecture or something else.\n",
    "\n",
    "## Outlook and further work\n",
    "\n",
    "The chosen approach could be validated in general by replacing the critical part with a state-of-the-art system from Google. First results were promising and showed that the pipeline can work under these circumstances. Systematic evaluation of the resulting alignment was not made. This may be something for future research.\n",
    "\n",
    "Another possible extension to the work presented here is to investigate in how far the ASR stage can be simplified into only recognizing a few key sounds for alignment. With the current implementation, a lot of information from the ASR stage is thrown away, because the generated partial transcript only needs to be _\"good enough\"_ for the LSA stage. It might be enough to train an RNN that only focuses on key sounds, but those with high probability. Those sounds could then be aligned with the transcript, from which the same sounds needed to be detected somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6174726/STDCGILD": {
     "author": [
      {
       "family": "Graves",
       "given": "Alex"
      },
      {
       "family": "Fernández",
       "given": "Santiago"
      },
      {
       "family": "Gomez",
       "given": "Faustino"
      }
     ],
     "container-title": "In Proceedings of the International Conference on Machine Learning, ICML 2006",
     "id": "6174726/STDCGILD",
     "issued": {
      "year": 2006
     },
     "page": "369–376",
     "page-first": "369",
     "title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
     "type": "paper-conference"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
