{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "In the [first notebook](01_corpus_creation.ipynb) corpora containing standardized corpus entries were created. Each corpus entry corresponds to a recording for which the following two pieces of information are available:\n",
    "\n",
    "1. **Segmentation information**: Information _where_ in the recording something is being said (speech segments)\n",
    "1. **Partial transcripts**: one transcript for each speech segment\n",
    "\n",
    "The RNN will be trained on features of the speech segments. Although it is possible to directly transcribe raw speech waveforms with RNNs <cite data-cite=\"6174726/DDKDDT5P\"></cite> this is rarely done as computational cost is high and performance is somewhat limited. Instead, features are extracted from the raw waveform and training is done on those features. Two popular methods that are frequently found in academic research papers are:\n",
    "\n",
    "- spectrograms (portion of each frequency within a short time frame of the waveform)\n",
    "- Mel-Frequency Cepstral Coefficients (MFCC)\n",
    "\n",
    "Both methods are explained below. Whatever method is used, feature extraction is computationally expensive and requires a lot of time. To speed up the iterations when training the RNN and get feedback faster, the input data (the spectrograms) are pre-computed and stored on disk so they do not need to be calculated at training time. Also, the labels (the information about speech pauses) need to be encoded in a suitable format. This notebook describes how both is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to import modules and define functions used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "from util.corpus_util import *\n",
    "from util.audio_util import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "\n",
    "default_figsize = (12,5)\n",
    "default_facecolor = 'white'\n",
    "default_font = {'family': 'serif', \n",
    "                'weight': 'normal', \n",
    "#                 'size': 12\n",
    "               }\n",
    "plt.rc('font', **default_font)\n",
    "\n",
    "def show_labelled_data(corpus_entry):\n",
    "    display(HTML(f'<h3>{corpus_entry.name} (id={corpus_entry.id})</h3>'))\n",
    "    display(HTML(f'{len(corpus_entry.speech_segments)} speech segments, {len(corpus_entry.pause_segments)} pause segments'))\n",
    "    \n",
    "    # audio data\n",
    "    audio, rate = corpus_entry.audio, corpus_entry.rate\n",
    "    display(Audio(data=audio, rate=rate))\n",
    "    \n",
    "    fig = plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "\n",
    "    # plot raw wave\n",
    "    ax_wave = fig.add_subplot(212)\n",
    "    title = f'Raw wave of {corpus_entry.audio_file} with speech pauses'\n",
    "    ax_wave = show_wave(audio, rate, ax_wave, title)\n",
    "    \n",
    "    # plot spectrogram\n",
    "    window_size_ms, step_size_ms = 20, 10\n",
    "    window_size, step_size = ms_to_frames(window_size_ms, rate), ms_to_frames(step_size_ms, rate)\n",
    "    ax_spec = fig.add_subplot(211)\n",
    "    title = f'Spectrogram of ' + corpus_entry.audio_file\n",
    "    spec = corpus_entry.spectrogram(window_size=window_size_ms, step_size=step_size_ms)\n",
    "    ax_spec = show_spectrogram(spec, rate, step_size, ax_spec, title=title, scale=None)\n",
    "    \n",
    "    # overlay speech and pause segments\n",
    "    speech_boundaries = calculate_boundaries(corpus_entry.speech_segments)\n",
    "    speech_boundaries_u = calculate_boundaries(corpus_entry.speech_segments_unaligned)\n",
    "    pause_boundaries = calculate_boundaries(corpus_entry.pause_segments)\n",
    "    \n",
    "    # rescale boundaries from frames to seconds\n",
    "    speech_boundaries = speech_boundaries / corpus_entry.rate\n",
    "    speech_boundaries_u = speech_boundaries_u / corpus_entry.rate\n",
    "    pause_boundaries = pause_boundaries / corpus_entry.rate\n",
    "    \n",
    "#     show_segments(ax_spec, speech_boundaries, color='green')\n",
    "    show_segments(ax_wave, speech_boundaries, color='green')\n",
    "#     show_segments(ax_spec, speech_boundaries_u, color='yellow')\n",
    "    show_segments(ax_wave, speech_boundaries_u, color='yellow')\n",
    "#     show_segments(ax_spec, pause_boundaries, color='red')\n",
    "    show_segments(ax_wave, pause_boundaries, color='red')\n",
    "    \n",
    "    speech_segments = mpatches.Patch(color='green', alpha=0.6, label='speech segments')\n",
    "    speech_segments_u = mpatches.Patch(color='yellow', alpha=0.6, label='speech segments unaligned')\n",
    "    pause_segments = mpatches.Patch(color='red', alpha=0.6, label='pause segments')\n",
    "    ax_wave.legend(handles=[speech_segments, speech_segments_u, pause_segments], bbox_to_anchor=(0, -0.5, 1., -0.4), loc=3, mode='expand', borderaxespad=0, ncol=3)\n",
    "    \n",
    "    return ax_spec, ax_wave\n",
    "\n",
    "def show_wave(audio, sample_rate, ax=None, title=None):\n",
    "    if not ax:\n",
    "        plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "        \n",
    "    p = librosa.display.waveplot(audio.astype(float), sample_rate)\n",
    "    ax = p.axes\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def show_spectrogram(spec, sample_rate, step_size, ax=None, title=None, scale='db'):\n",
    "    if not ax:\n",
    "        plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "\n",
    "    ax = librosa.display.specshow(spec, sr=sample_rate, hop_length=step_size, \n",
    "                                  x_axis='time', y_axis='hz', cmap='viridis')\n",
    "    if scale == 'db':\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()    \n",
    "    return ax\n",
    "    \n",
    "def show_spectrogram_3d(spec, title=None):\n",
    "    data = [go.Surface(z=spec)]\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        scene = dict(\n",
    "            xaxis = dict(title='Time'),\n",
    "            yaxis = dict(title='Frequencies'),\n",
    "            zaxis = dict(title='Log amplitude'),\n",
    "            ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)      \n",
    "\n",
    "\n",
    "def show_segments(ax, boundaries, ymin=0, ymax=1, color='red'):\n",
    "    for i, (start_frame, end_frame) in enumerate(boundaries):\n",
    "        rect = ax.axvspan(start_frame, end_frame, ymin=ymin, ymax=ymax, color=color, alpha=0.5)\n",
    "        y_0, y_1 = ax.get_ylim()\n",
    "        x = start_frame + (end_frame - start_frame)/2\n",
    "        y = y_0 + 0.01*(y_1-y_0) if ymin==0 else y_1 - 0.05*(y_1-y_0)\n",
    "        ax.text(x, y, str(i+1), horizontalalignment='center', fontdict={'family': 'sans-serif', 'size': 15, 'color': 'white'})\n",
    "\n",
    "def calculate_boundaries(segments):\n",
    "    start_frames = (seg.start_frame for seg in segments)\n",
    "    end_frames = (seg.end_frame for seg in segments)\n",
    "    return np.array(list(zip(start_frames, end_frames)))\n",
    "\n",
    "def on_create_data_rl_button_click(sender):\n",
    "    print(f'tbd: create RL-features in HDF5')\n",
    "    \n",
    "def on_create_data_ls_button_click(sender):\n",
    "    print(f'tbd: create LS-features in HDF5')\n",
    "    \n",
    "# UI elements\n",
    "layout = widgets.Layout(width='250px', height='50px')\n",
    "create_data_rl_btn = widgets.Button(description=\"Create labelled data for ReadyLingua\", button_style='info', layout=layout, icon='download')\n",
    "create_data_rl_btn.on_click(on_create_data_rl_button_click)\n",
    "create_data_ls_btn = widgets.Button(description=\"Create labelled data for LibriSpeech\", button_style='info', layout=layout, icon='download')\n",
    "create_data_ls_btn.on_click(on_create_data_ls_button_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data (features and labels) is stored as numpy arrays whose dimensions partially depend on the proposed network architecture and the kind of features used (spectrograms or MFCC). Regardless of the kind of features used, the raw wave (as a sequence of sampling points) will be converted to a sequence of audio frames.\n",
    "\n",
    "The RNN is trained on audio data (sequence of frames) and will output whether a specific section in the audio signal is speech or pause (sequence of labels). Because both input and the output are sequences, this is a sequence-to-sequence model with a **many-to-many** architecture. The most important parameters for this kind of model are:\n",
    "\n",
    "* $T_x$: Number of sequence tokens in an individual sample. This value may vary between samples!\n",
    "* $T_y$: Number of sequence tokens in the output. This value may be different from $T_x$ and also vary between samples!\n",
    "* $N$: Number of training samples\n",
    "* $K$: Number of different output labels (i.e. the output alphabet)\n",
    "\n",
    "In the following sections the following variables are used to denote the two components of the labelled data:\n",
    "\n",
    "| Formal symbol | Variable name | Type | Shape | description |\n",
    "|---|---|---|---|---|\n",
    "| $X$ | `X` | 2D-Array | $(N \\times T_x)$ | The spectrograms of a speech segment |\n",
    "| $Y$ | `Y` | 2D-Array | $(T_y \\times K)$ | The one-hot encoded transcript of a speech segment | \n",
    "\n",
    "Let's load the created corpora to make them available to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_corpus = get_corpus('rl')\n",
    "ls_corpus = get_corpus('ls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev/Test split\n",
    "The labelled data is split into subsets for training (_train-set_), parameter tuning (_dev-set_) and model evaluation (_test-set_). Because the raw data is distributed differently for each source (number of languages, homogeneity of the recording quality, ratio of male vs. female speakers, presence of distortions like reverb or overdrive, and many more) it must be ensured that this distribution is represented in each subset.For the _LibriSpeech_ corpus this is already done, whereas a good split needs to be determined first for the _ReadyLingua_ corpus.  Also the corpus sizes are highly different in terms of size (24.997 speech segments/~22h audio for _ReadyLingua_ vs. 334.345 entries/~1.140h audio). \n",
    "\n",
    "### ReadyLingua corpus\n",
    "The raw data exhibits high variance with respect to relevant features (recording quality, length of samples, presence of distortion, ...). Since the corpus is rather small there may be only one sample for a specific feature value (e.g. only one recording with reverb). Therefore to keep things simple the split into train-, dev- and test-set was done with a 80/10/10-rule without closer examination of the underlying data. This will probably not result in an optimal split since it would be possible for example that all the female speakers will be put in one subset. However, as a first attempt this fact is ignored.\n",
    "\n",
    "Improvements could be made to the training process by manually assigning each sample to a specific set by carefully inspecting the relevant features. The corpus could also be extended by creating synthetisized data, e.g. creating samples with reverb from the original samples. Because the LibriSpeech corpus looked much more promising for a proof of concept, this time was not invested.\n",
    "\n",
    "### LibriSpeech corpus\n",
    "The LibriSpeech raw data is already split into train-, dev- and test-set. Each chapter is read by a different speaker. Each speaker is only contained in one of the subsets. Efforts have been made to keep the different sets within the same probability distributions (regarding to accents, ratio of male/female speakers, ...). The information about what subset a particular entry belongs to was preserved during corpus creation. To leverage the efforts made by the LibriSpeech project, this train/dev/test-set split was not changed.\n",
    "\n",
    "---\n",
    "\n",
    "You can explore the size of the subsets for each corpus by executing the cell below to see the number of samples (corpus entries) in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_train, rl_dev, rl_test = rl_corpus.train_dev_test_split()\n",
    "print(f'ReadyLingua corpus ({len(rl_train + rl_dev + rl_test)} samples): #train-samples: {len(rl_train)}, #dev-samples: {len(rl_dev)}, #test-samples: {len(rl_test)}')\n",
    "\n",
    "ls_train, ls_dev, ls_test = ls_corpus.train_dev_test_split()\n",
    "print(f'LibriSpeech corpus ({len(ls_train + ls_dev + ls_test)} samples): #train-samples: {len(ls_train)}, #dev-samples: {len(ls_dev)}, #test-samples: {len(ls_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature extraction from audio signal\n",
    "\n",
    "In order to train an RNN, each training sample needs to be converted into some sort of sequence of features. In this case the samples are the audio files from the corpus entries that were converted to WAV files (`*.wav`) and downsampled to 16kHz (mono). This chapter describes different ways of extracting features from the audio signal that can be used for training.\n",
    "\n",
    "### Raw waves\n",
    "As the file extension suffix suggests the wave files contain the audio signal as a raw wave, which is just a series of discrete sample values. Because a sampling rate of 16kHz was used we get 16'000 sample values per second. A sample value corresponds to the _amplitude_ of the waveform at the given time step. These values can be stored in a 1-dimensional Numpy array and plotted in two dimension (time vs. amplitude).\n",
    "\n",
    "For example consider the a raw wave for a spefic speech segment. Feel free to change the first line to visualize the raw wave for a random corpus entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry = rl_corpus['20161124weeklyaddressthanksgiving']\n",
    "speech_segment = corpus_entry.speech_segments[49]\n",
    "\n",
    "# uncomment the following lines for random corpus entry and/or speech segment\n",
    "# corpus_entry = random.choice(rl_corpus)\n",
    "# speech_segment = random.choice(corpus_entry.speech_segments)\n",
    "\n",
    "print(f'number of sampling points: {speech_segment.audio.shape[0]}, sampling rate: {speech_segment.rate}')\n",
    "print(f'transcript: {speech_segment.transcript}')\n",
    "\n",
    "display(Audio(data=speech_segment.audio, rate=speech_segment.rate))\n",
    "title = f'Raw wave of speech segments in {corpus_entry.id}.wav'\n",
    "\n",
    "fig = plt.figure(figsize=default_figsize, facecolor=default_facecolor)\n",
    "show_wave(speech_segment.audio, speech_segment.rate, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From raw waves to spectrograms\n",
    "\n",
    "Although already a sequence, training on the raw wave would not be very useful since we would only have one feature (the amplitude) per time step. However, an audio signal just a bunch of overlaying frequencies of different phases and amplitudes. For a given time slot (_window_), the raw signal can be decomposed into its underlying frequencies using _Fourier Transformation_, yielding the amplitude of each frequency. \n",
    "These values can be stored in a 1-D array of shape $(f \\times 1)$, whereas $f$ denotes the number of frequencies.\n",
    "\n",
    "Since we will be using spectrograms as input values `X` to train an RNN, $T_x$ denotes the number of windows that can be calculated from the audio signal. All the windows together form the **spectrogram** which can be represented as a matrix of shape ($f \\times T_x$) whereas each entry corresponds to the _magnitude_ of frequency $f$ in window $T_x$. A spectrogram can be visualized by color-coding the values. Consider the following spectrogram derived from the raw wave above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_spec = speech_segment.mag_specgram()\n",
    "print(mag_spec.shape)\n",
    "title=f'Amplitude-spectrogram of speech segment in {corpus_entry.id}.wav'\n",
    "show_spectrogram(mag_spec, speech_segment.rate, ms_to_frames(10, speech_segment.rate), scale=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a spectrogram can now be calculated for every speech segment. The following table contains all relevant parameters:\n",
    "\n",
    "| Symbol | Variable in code | Value | Description |\n",
    "|---|---|---|---|\n",
    "| $n$ | `num_values` | - | number of discrete sampling values in audio signal |\n",
    "| $r$ | `sample_rate` | - | sampling rate of audio signal |\n",
    "| $w_{ms}$ | `window_size_ms` | 20 | Window length in ms |\n",
    "| $w$ | `window_size` | 320 | Window length in frames |\n",
    "| $s_{ms}$ | `step_size_ms` | 10 | Step length in ms |\n",
    "| $s$ | `step_size` | 160 | Step length in frames |\n",
    "\n",
    "Note that the window and step length in frames unit can be derived from their values in milliseconds by calculating $w = \\frac{r \\cdot w_{ms}}{1000}$ or $s = \\frac{r \\cdot s_{ms}}{1000}$ respectively.\n",
    "\n",
    "To calculate the spectrogram for an audio signal, a sliding window of size $w$ is moved over the sample values with step size $s$. Note that the step size is usually smaller than the window size which means the windows will overlap to a certain degree. For any given audio signal $x$ the number of windows $T_x$ can be calculated by dividing the number of sampling values by the size of the overlap:\n",
    "\n",
    "$$\n",
    "T_x = \\left\\lfloor \\frac{n}{(w-s)} + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "The flooring is needed because the window size might not match up exactly with the number of sample values, resulting in fractional values for $T_x$. Since the windows of the spectrogram will be used as input to an RNN, $T_x$ corresponds to the number of training samples. Therefore only whole numbers make sense. Alternatively, padding could be used to make the number of frames result in an integer number.\n",
    "\n",
    "According to the [Nyquist theorem](https://en.wikipedia.org/wiki/Nyquist_rate) the sampling frequency of an audio signal must be (at least) twice the frequency of the signal frequency to be able to reconstruct the original signal from the discrete sampling values. Since our sampling rate is 16kHz this means the maximum frequency that can be reproduced is 8kHz. Therefore the frequencies in our spectrogram are all in the range $0..\\frac{r}{2} = 0..8000$ Hz. This interval can be divided into equally sized sections. Including the borders of these sections this gives us $f$ equidistant sampling frequencies. The value for $f$ can be calculated as follows:\n",
    "\n",
    "$$\n",
    "f = \\frac{w}{2} + 1\n",
    "$$\n",
    "\n",
    "Note that we add 1 at the end because the borders (lowest and the highest frequency) are both included.\n",
    "\n",
    "Since the frequency band of the spectrogram will be spaced equally, the distance between two sample frequencies is \n",
    "\n",
    "$$\n",
    "\\Delta f = \\frac{r}{2\\cdot (f - 1)}\n",
    "$$\n",
    "\n",
    "This means that frequency phase $F_i$ in the frequency band can be calculated as follows:\n",
    "\n",
    "$$\n",
    "F_i = i \\cdot \\Delta f\n",
    "$$\n",
    "\n",
    "**Example**:\n",
    "\n",
    "For this project all audio signals were re-sampled with with a sampling rate $r=16000$. To calculate the spectrogram we use a sliding window of $w_{ms}=20ms$ length and a step size of $s_{ms}=10ms$ . In frame units this gives us the values $w=\\frac{16000 \\cdot 20 ms}{1000 ms} = 320$ frames per window and $s=\\frac{16000 \\cdot 10 ms}{1000 ms} = 160$ frames per step.\n",
    "\n",
    "As stated above the frequencies all lie in the interval $[0..8000]$. This band is now divided into sections giving us $f = \\frac{320}{2} + 1 = 161$ sample frequencies, whereas the distance between each frequency is $\\Delta f = \\frac{16000}{2 \\cdot (161 - 1)} = \\frac{8000}{160} = 50 Hz$. The $i$-th sample frequency can therefore be calculated as. $F_i = i \\cdot 50$. The frequencies in the spectrogram are then:\n",
    "\n",
    "    [0, 50, 100, 150, ... , 7950, 8000]\n",
    "\n",
    "The raw wave for the example speech sequence above consists of $n = 9760$ sample values. Using a window size of $w=320$ frames and a step size of $s=160$ frames we arrive at a value of $T_x = \\left\\lfloor \\frac{9760}{(320-160)} + 1 \\right\\rfloor = 62$  frames in the spectrogram.\n",
    "\n",
    "We can verify this for the above spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_ms, step_size_ms, num_vals = 20, 10, speech_segment.audio.shape[0]\n",
    "window_size = ms_to_frames(window_size_ms, speech_segment.rate)\n",
    "step_size = ms_to_frames(step_size_ms, speech_segment.rate)\n",
    "\n",
    "print(f'n = {num_vals}\\t(number of sample values)')\n",
    "print(f'r = {speech_segment.rate}\\t(sample rate)')\n",
    "print(f'w_ms = {window_size_ms}\\t(window size in ms)\\t\\t ==> w = {window_size}\\t\\t(window size in frames)')\n",
    "print(f's_ms = {step_size_ms}\\t(step size in ms)\\t\\t ==> s = {step_size}\\t\\t(step size in frames)')\n",
    "print()\n",
    "\n",
    "f, T_x = mag_spec.shape\n",
    "print(f'spec.shape = (f, T_x) = ({f}, {T_x})')\n",
    "print()\n",
    "\n",
    "delta_f = int(speech_segment.rate / (2 * (f - 1)))\n",
    "print(f'delta_f = {delta_f}\\t(difference between sample frequencies in Hz)')\n",
    "print()\n",
    "\n",
    "freqs = np.array(range(0, f*delta_f, delta_f))\n",
    "print('Frequencies (y-Axis):')\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power-Spectrograms\n",
    "\n",
    "Above spectrogram visualizes the amplitude of the frequencies. Because of the way people hear, we recalculate the values to decibel (dB) units. To do this we take the logarithm of the squared amplitude and get so-called power spectrograms. The logarithmic scale of decibels corresponds with how humans perceive loudness: To double the perceived volume of a sound you would need to put 8 times more energy in it.\n",
    "\n",
    "We can visualize the results by plotting the log-scaled dB-values along the two axes (time and frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_spec = speech_segment.pow_specgram()\n",
    "print(pow_spec.shape)\n",
    "title = f'3D Power-Spectrogram of speech segments in {corpus_entry.id}.wav'\n",
    "show_spectrogram_3d(librosa.power_to_db(pow_spec, ref=np.max), title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further reduce the above 3D-plot by one dimension by flattening it along the z-axis (log ampliltude). We don't lose any information because the third dimension (dB value) is color-coded. From this plot we clearly see that most of the high-valued entries of the spectrogram all lie within a frequency range of approximately 300-3400 Hz, which corresponds to the usable voice frequency band [used e.g. in telephony](https://en.wikipedia.org/wiki/Voice_frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=f'Power-spectrogram of speech segment in {corpus_entry.id}.wav'\n",
    "show_spectrogram(librosa.power_to_db(pow_spec, ref=np.max), speech_segment.rate, step_size=step_size, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel-Spectrograms\n",
    "\n",
    "In the power spectrogram above the features were extracted by putting the values on the Hertz scale. This scale is logarithmic, i.e. a value of 2000Hz is considered twice as high than a value of 1000 Hz, which in turn is twice as high as a value of 500 Hz. In other words: doubling a Hertz value corresponds to setting the pitch of a tone an octave higher. Consider the following examples for reference:\n",
    "\n",
    "| 500Hz | 1000Hz | 2000Hz\n",
    "|---|---|---\n",
    "|<audio src=\"../assets/500Hz.wav\" style=\"width: 150px;\" controls preload></audio>|<audio src=\"../assets/1000Hz.wav\" style=\"width: 150px;\" controls preload></audio>|<audio src=\"../assets/2000Hz.wav\" style=\"width: 150px;\" controls preload></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively the values could be put on the the [Mel scale](https://en.wikipedia.org/wiki/Mel_scale) which is also logarithmic but based on psycho-acoustic findings about how pitches of equal distance are perceived by humans. It turns out that that until approximately 500Hz the Mel scale corresponds roughly with the Hertz scale, whereas for sounds above 500Hz the intervals between two sounds must increase in order to be perceived the same distance from another. In other words: lower frequencies are perceptually more important than higher frequencies. Thus the Mel scale is more discriminative for sounds on low frequencies and less discriminative for sounds at high frequencies. As a result, four octaves on the hertz scale above 500 Hz are judged to comprise about two octaves on the mel scale. The reference point is set at 1000Hz, which corresponds to 1000MEL. \n",
    "\n",
    "Consider the following plot which maps values on the Hertz-scale to their counterparts on the Mel-scale:\n",
    "\n",
    "![Mel scale](../assets/mel-scale.svg)\n",
    "_Source: Wikipedia_\n",
    "\n",
    "There are various formulas to convert Hz to MEL. One possible choice is the one from Douglas O'Shaughnessy <cite data-cite=\"6174726/NQC4HAR8\"></cite>:\n",
    "\n",
    "$$\n",
    "m = 2995 \\log_{10}\\left( 1 + \\frac{f}{700} \\right) = 1127 \\ln \\left( 1 + \\frac{1}{700} \\right)\n",
    "$$\n",
    "\n",
    "Using this formula we arrive at the following values for the above values on the Hertz scale:\n",
    "\n",
    "| ~607MEL = 500Hz | 1000MEL = 1000Hz| ~1521MEL = 2000Hz\n",
    "|---|---|---\n",
    "|<audio src=\"../assets/607Hz.wav\" style=\"width: 150px;\" controls preload></audio>|<audio src=\"../assets/1000Hz.wav\" style=\"width: 150px;\" controls preload></audio>|<audio src=\"../assets/1521Hz.wav\" style=\"width: 150px;\" controls preload></audio>\n",
    "\n",
    "We can now use the Mel scale instead of the Hertz scale to create a **Mel power spectrogram** as an alternative to the \"normal\" spectrogram. To do this, each bin in the spectrogram is divided into chunks of different sizes. For lower frequencies, the chunks are smaller because the human ear is able to discern more subtle changes in frequency in low-frequency areas (i.e. the Mel scale is more discriminative here). For higher frequencies the chunks become larger. The values in each chunk are averaged.\n",
    "\n",
    "Note that the number of features is usually smaller than when calculating the spectrograms because the frequencies have been binned. The number of bins determines the number of frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = speech_segment.mel_specgram()\n",
    "print(mel_spec.shape)\n",
    "title=f'Mel-spectrogram of speech segment in {corpus_entry.id}.wav'\n",
    "show_spectrogram(librosa.power_to_db(mel_spec, ref=np.max), sample_rate=speech_segment.rate, step_size=step_size, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCC\n",
    "\n",
    "As an alternative to Spectrograms we could use Mel Frequency Cepstral Coefficients (MFCC) as features. Because MFCC allow for a more compact representation of speech features than spectrograms, MFCC have a long-standing tradition in speech recognition and are mentioned often in papers. MFCC use the same psychoacoustic findings as described above for the Mel-Spectrograms and can be computed by performing the following steps (see <cite data-cite=\"6174726/YWWQ86H5\"></cite>):\n",
    "\n",
    "1. Divide the signal into frames: usually some small time frame like 20ms is used\n",
    "1. Obtain the amplitude spectrum: done using Fourier Transformation\n",
    "1. Take the logarithm: because the perceived loudness of a signal has been found to be approximately logarithmic\n",
    "1. Convert to Mel spectrum: The frequencies are put into mel-spaced bins, whereas the bins for lower frequencies are smaller and become larger for higher frequencies. The frequency components in each bin are averaged, which leads to a smoothed spectrum of frequencies from the original spectrogram.\n",
    "1. The number of features is reduced. PCA could be used for this transformation, but usually Discrete Cosine Transform (DCT) is used to approximate it.\n",
    "\n",
    "Note that the first 4 steps are the same steps that are needed to calculate the Mel-Spectrogram. Therefore MFCC can be easily calculated from Mel-Spectrogram by performing the feature reduction (step 5). The number of principal components can be chosen. The resulting feature matrix usually contains much less features than the power-spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = speech_segment.mfcc().T\n",
    "print(mfccs.shape)\n",
    "title=f'MFCC of speech segment in {corpus_entry.id}.wav'\n",
    "show_spectrogram(mfccs, sample_rate=speech_segment.rate, step_size=step_size, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of features\n",
    "\n",
    "The following table compares the number of features produced for each feature type (Power-Spectrograms, Mel-Spectrograms and MCC) with the default values used in this project:\n",
    "\n",
    "| Feature Type | #Features |\n",
    "|---|---|\n",
    "| Power-Spectrograms | 161 |\n",
    "| Mel-Spectrograms | 40 |\n",
    "| MFCC | 13 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label extraction from transcripts\n",
    "\n",
    "Because our approach is to train an RNN that can produce a transcript for a given audio signal (ASR) we need to derive the target labels for each speech sequence. We can derive those labels from the transcript, which is included for each speech sequence in the corpora. However, because an RNN only works with numerical values, we must find a way to convert the representation from alphanumeric to numeric. Additionally, since transcripts often contain unwanted characters and to reduce the number of possible target labels, some preprocessing needs to be done. This chapter documents how both is done.\n",
    "\n",
    "### Normalizing the transcript\n",
    "\n",
    "In order to facilitate learning, the alphabet of target characters in the transcript is limited to the 26 lowercase characters of the alphabet (`[a..z]`). Since this limitation results in less classes to compute probabilities for, it is expected to speed up the learning process and improve the quality of the result. \n",
    "\n",
    "The limitation of target classes is done by normalizing the transcript. Normalization involves the following steps:\n",
    "\n",
    "1. remove leading and trailing whitespaces (trimming)\n",
    "2. remove multiple subsequent occurences of whitespace within the transcript\n",
    "3. replacing accentuated characters with their base character from the alphabet (e.g. _é_/_è_/_ê_/...->e, _ß_->ss, etc...)\n",
    "4. removing non-alphanumeric characters (removes punctuation)\n",
    "5. make everything lowercase\n",
    "\n",
    "You can edit/execute the cell below with your own examples to see the result of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.string_util import *\n",
    "samples = [ 'Crème-brûlée', 'Außerirdische', ' foo    bar   ']\n",
    "for sample in samples:\n",
    "    print(f'{sample} ==> {normalize(sample)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the transcript\n",
    "\n",
    "In order to arrive at the target labels for a speech sequence, its normalized transcript needs to be tokenized first. Tokenizing involves splitting the transcription first into words and then characters. The tokens are the characters of the transcription, whereas a special token `<space>` is used between the characters of two words.\n",
    "\n",
    "The mapping of audio to text is actually a classification problem: Parts of the audio signal are mapped each to a specific character (i.e. _token_). Since an RNN resp. TensorFlow only works with numeric data, token need to be numerically encoded to put them on an ordinal scale. The following table shows how the encoding is done:\n",
    "\n",
    "| **Token**    | `<space>` | `a` | `b` | `c` | ... | `z`  |\n",
    "|--------------|:---------:|:---:|:---:|:---:|:---:|:----:|\n",
    "| **Encoding** | `0`       | `1` | `2` | `3` | ... | `26` |\n",
    "\n",
    "The following table shows how an example transcript is converted to its encoded form:\n",
    "\n",
    "| **Original transcript** | The quick, brown fox jumps over the lazy dog!  |\n",
    "|-------------------------|------------------------------------------------|\n",
    "| **Normalized transcript** | the quick brown fox jumps over the lazy dog |\n",
    "| **Tokenized transcript** | `['t', 'h', 'e', '<space>, 'q', 'u', 'i', 'c', 'k', '<space>', 'b', 'r', 'o', 'w', 'n', ...]` |\n",
    "| **Encoded transcript** | `[ 20, 8, 5, 0, 17, 21, 9, 3, 11, 0, 2, 18, 15, 23, 14, ...]` |\n",
    "\n",
    "#### The issue with numbers\n",
    "\n",
    "Numbers in transcript pose a special problem, since their pronunciation differs fundamentally from their lexical representation, if written with digits (which is usually the case). Consider the number `8`, which is represented textually by the digit `'8'` and is pronounced as _'eight'_. In this case, the actual sequence of characters (`'e', 'i', 'g', 'h', 't')` is replaced by a single character `'8'` and can therefore not be approximated like ordinary words.\n",
    "\n",
    "The problem becomes even harder since compound number are sometimes pronounced differently than their individual parts would be pronounced. Consider the number `13` which is pronounced `'thirteen'` (and not `'onethree'`!). This becomes especially important in languages like German which swap the decimal part (e.g. `'21'` is pronounced as `'one-and-twenty'`).\n",
    "\n",
    "Since numbers are a problem of their own we want to limit their influence on the training process by training the RNN only on transcripts without numbers. We can filter those out by using the corpus entry as a function and pass in the `numeric=False` argument to get only those speech segments whose transcripts do not contain numbers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter showed how to extract features from standardized corpus entries that can be used to train an RNN. Two types of spectrograms (Power and Mel-Power spectrograms) and MFCC were identified as possible features. The number of features is generally smallest for MFCC and highest for Power-Spectrograms. This might be important because training on more features also means more training data is required.\n",
    "\n",
    "A feature matrix `X` can be created for each speech segment, together with the target vector `Y`, which is just an numerically encoded form of the normalized transcript. Normalization is needed to limit the influence of various side effects caused e.g. by the specialized textual representation of numbers and also to improve the learning process.\n",
    "\n",
    "Both `X` and `Y` make up the labelled data which can be further subdivided into three different sets (train-, dev- and test-set) used to train, validate and evaluate an RNN. For the _LibriSpeech_ corpus this is easy because the subdivision is already made by the creators of the raw data. For _ReadyLingua_, a split of 80/10/10% was made for each language without deeper analyzation of properties of the recordings (speaker gender, recording quality, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<div class=\"cite2c-biblio\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6174726/DDKDDT5P": {
     "ISBN": "978-3-642-24796-5",
     "URL": "https://doi.org/10.1007/978-3-642-24797-2",
     "author": [
      {
       "family": "Graves",
       "given": "Alex"
      }
     ],
     "collection-title": "Studies in Computational Intelligence",
     "id": "6174726/DDKDDT5P",
     "issued": {
      "year": 2012
     },
     "note": "DOI: 10.1007/978-3-642-24797-2",
     "publisher": "Springer",
     "title": "Supervised Sequence Labelling with Recurrent Neural Networks",
     "type": "book",
     "volume": "385"
    },
    "6174726/NQC4HAR8": {
     "author": [
      {
       "family": "O'Shaughnessy",
       "given": "Douglas"
      }
     ],
     "id": "6174726/NQC4HAR8",
     "issued": {
      "year": 1987
     },
     "number-of-pages": "150",
     "publisher": "Addison-Wesley",
     "title": "Speech communication: human and machine",
     "type": "book"
    },
    "6174726/YWWQ86H5": {
     "author": [
      {
       "family": "Logan",
       "given": "Beth"
      }
     ],
     "container-title": "In International Symposium on Music Information Retrieval",
     "id": "6174726/YWWQ86H5",
     "issued": {
      "year": 2000
     },
     "title": "Mel Frequency Cepstral Coefficients for Music Modeling",
     "type": "paper-conference"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
