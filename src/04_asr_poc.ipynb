{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ASR stage\n",
    "\n",
    "From the previous notebooks the following data was created:\n",
    "\n",
    "1. **audio signal**: resampled to 16kHz (mono)\n",
    "1. **segmentation**: parts of the audio signal containing speech, derived from the raw data or through VAD\n",
    "1. **features**: Power-spectrograms, Mel-spectrograms or MFCC of the audio signal to use as training data\n",
    "1. **labels**: transcriptions for the raw speech segments in original and normalized form\n",
    "\n",
    "The set of features and labels is called the _labelled data_. The input data is often denoted as  `X` and the labels as `Y`. To train the network only a part of the labelled data is used. This part is referred to as **training set**. By training on the training set, an RNN for ASR that will learn the relationship between an audio signal and its textual representation. The RNN will use the [CTC loss function](https://www.cs.toronto.edu/~graves/icml_2006.pdf) as described in the [introduction](00_introduction.ipynb). \n",
    "\n",
    "To prevent overfitting on this set we use another part of the labelled data to form a **validation/dev set**. The trained RNN can be validated by feeding it previously unseen instances from the dev set. From observation of the results decisions can be made about what to optimize next. Because conclusions are drawn from the validation, the validation set is indirectly used for training and should not be used for evaluation.\n",
    "\n",
    "Finally, there is the **test-set**, which contains completely unseen instances. The RNN can be evaluated with instances from this set. Because evaluation should be the last step when creating an Neural Network, instances from this set should be held off until training is finished.\n",
    "\n",
    "An RNN trained for ASR is supposed to learn the relationship between an audio signal and its transcription, i.e. if trained properly it should be able to generate a transcription for any unseen audio signal (_prediction_) that is roughly equivalent to the acutal transcription (_ground truth_). However, this notebook describes the training of a RNN prototype (_Proof of Concept_ or _PoC_). The purpose of this prototype is not to be trained into a fully-fletched ASR model but rather to examine the influence of different properties of the data on the training process. By comparing the learning progress with different inputs, valuable information should be gained that can be used to train the actual RNN for ASR later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to import some modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from util.corpus_util import *\n",
    "from util.audio_util import *\n",
    "\n",
    "from IPython.display import HTML, Audio\n",
    "\n",
    "rl_corpus = get_corpus('rl')\n",
    "ls_corpus = get_corpus('ls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data synthetization\n",
    "\n",
    "In Deep Learning, additional training data can often help getting better results. Additional data can be artificially created through synthetization. Audio data is particularly easy to synthetisize by altering the original signal, e.g. by applying the following changes:\n",
    "\n",
    "* change of tempo\n",
    "* change of loudness\n",
    "* change of pitch\n",
    "* adding echo/reverb\n",
    "* adding background noise\n",
    "\n",
    "Data synthetization can help improving the performance of an RNN, especially if labelled data is scarce. However, this can only be done to a certain extent, because the distribution of training data should still reflect the distribution of the data the trained RNN is later used for. Synthetization must also be done carefully, especially when adding background noise. A certain background noise may be applied only once, otherwise the RNN will learn how to subtract it from the given signal.\n",
    "\n",
    "For this notebook, synthetic data is produced by adding some distortion to the original signal. Different speaking rates can be simulated by changing the tempo. Different voices can be simulated by changing the pitch. Execute the following cell to hear some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry = rl_corpus['edznachrichten180111']\n",
    "\n",
    "original, rate = corpus_entry.audio, corpus_entry.rate\n",
    "display(HTML('original signal:'))\n",
    "display(Audio(data=original, rate=rate))\n",
    "\n",
    "fast_reader = distort(original, rate, tempo=1.5)\n",
    "display(HTML('fast reader:'))\n",
    "display(Audio(data=fast_reader, rate=rate))\n",
    "\n",
    "high_pitched = distort(original, rate, pitch=True)\n",
    "display(HTML('high pitched voice:'))\n",
    "display(Audio(data=high_pitched, rate=rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "The pronunciation of a given piece of text is highly dependent on the language. ASR is therefore highly sensitive to the language. An RNN for ASR is usually only valid for the language it was trained an and not be able to recognize words from a different language. For that reason, the PoC trained in this notebook is only trained on one language at a time. For each language the RNN is trained using the different features. Each combination of language and feature type is trained using either only the original data or the original data that has been augmented using synthetized variants. Different combinations of language, feature type and data type yield different profiles for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training profiles\n",
    "\n",
    "The properties _language_, _feature type_ and _data type_ have the following value ranges:\n",
    "\n",
    "| configuration item | possible values |\n",
    "|---|---|\n",
    "| language | German or English |\n",
    "| feature type | MFCC, Mel- or Power-Spectrograms |\n",
    "| data type | original or synthetisized data |\n",
    "\n",
    "This gives us the following matrix of training profiles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Profile | language | feature type | data type |\n",
    "|---|---|---|---|\n",
    "| Poc#1 | German | MFCC | original |\n",
    "| Poc#2 | German | MFCC | synthesized|\n",
    "| Poc#3 | German | Mel-Spectrogram | original |\n",
    "| Poc#4 | German | Mel-Spectrogram | synthesized|\n",
    "| Poc#5 | German | Power-Spectrogram | original |\n",
    "| Poc#6 | German | Power-Spectrogram | synthesized|\n",
    "| Poc#7 | English | MFCC | original |\n",
    "| Poc#8 | English | MFCC | synthesized|\n",
    "| Poc#9 | English | Mel-Spectrogram | original |\n",
    "| Poc#10| English | Mel-Spectrogram | synthesized|\n",
    "| Poc#11 | English | Power-Spectrogram | original |\n",
    "| Poc#12 | English | Power-Spectrogram | synthesized|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between subsequent profiles only ever one configuration item is changed. Comparing the results of two PoCs trained on different profiles should give some insight about how that particular property affects the learning progress. E.g. by comparing the results of PoC#1 and PoC#2 we get an intuition for how synthetisized data will impact the speed of learning with features and language being identical. By comparing Poc#1 with PoC#3 the efficiency in the learning progress can be assessed when using MFCC or Mel-Spectrograms as features. By Comparing Poc#1 with Poc#7 the impact of a change in language can be estimated.\n",
    "\n",
    "The results of the PoCs serve a basis for further decisions. By changing only one single configuration item in each iteration, the impact of each change can be analyzed in isolation and conclusions for the following steps can be drawn. This follows the principle of incremental changes and should prevent spending too much project time on a highly sophisticated setup that may or may not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Concept (PoC)\n",
    "\n",
    "To see whether the PoC is even able to learn something useful, the following simplifications were made:\n",
    "\n",
    "* the RNN is only trained on five speech segments of a single corpus entry\n",
    "* only speech segments that do not contain numbers are considered\n",
    "\n",
    "Note that for the German and English training samples speech segments from the _ReadyLingua_ corpus were drawn. You can listen to the corpus entries by executing the cell below. Note that the two recordings exhibit similar quality and are both read by a female speaker. This should limit the impact of recording quality and gender on the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry_de = rl_corpus['andiefreudehokohnerauschenrein']\n",
    "segments_de = corpus_entry_de.speech_segments_not_numeric\n",
    "av_seg_len_de = sum(s.audio_length for s in segments_de) / len(segments_de)\n",
    "av_trn_len_de = sum(len(s.text) for s in segments_de) / len(segments_de)\n",
    "\n",
    "corpus_entry_en = rl_corpus['sunday22ohnerauschen']\n",
    "segments_en = corpus_entry_en.speech_segments_not_numeric\n",
    "av_seg_len_en = sum(s.audio_length for s in segments_en) / len(segments_en)\n",
    "av_trn_len_en = sum(len(s.text) for s in segments_en) / len(segments_en)\n",
    "\n",
    "display(HTML('German corpus entry ({len(segments_de)} segments):'))\n",
    "display(HTML(f'Average speech segment length: {av_seg_len_de:.3f} seconds'))\n",
    "display(HTML(f'Average transcript length: {av_trn_len_de:.3f} characters'))\n",
    "display(Audio(data=corpus_entry_de.audio, rate=corpus_entry_de.rate))\n",
    "for s in corpus_entry_de.speech_segments_not_numeric[:5]:\n",
    "    print(s.text)\n",
    "    \n",
    "display(HTML('English corpus entry  ({len(segments_en)} segments):'))\n",
    "display(HTML(f'Average speech segment length: {av_seg_len_en:.3f} seconds'))\n",
    "display(HTML(f'Average transcript length: {av_trn_len_en:.3f} characters'))\n",
    "display(Audio(data=corpus_entry_en.audio, rate=corpus_entry_en.rate))\n",
    "for s in corpus_entry_en.speech_segments_not_numeric[:5]:\n",
    "    print(s.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "The PoC is a RNN with a simple architecture. It only has one layer which is time-distributed over $T_x$ time steps. The value for $T_x$ depends on the length of the input signal. The number of units $n$ depends on the number of features. For MFCC features this value is $13$, for Mel-Spectrograms $40$ and for Power-Spectrograms $161$ (see [notebook 3](03_feature_extraction.ipynb#From-raw-waves-to-spectrograms) on how to calculate those values). The RNN uses [LSTM cells](http://colah.github.io/posts/2015-08-Understanding-LSTMs). LSTM cells have the ability to not only learn from previous time steps by adding information, but also to remove (_\"forget\"_) information from previous steps. They do so by having trainable parameters to control how much information flows from one time step to the next (_forget gate_).\n",
    "\n",
    "![PoC architecture](../assets/poc_architecture.jpg)\n",
    "\n",
    "### Training and validation\n",
    "\n",
    "For profiles that do not use synthesized data, the training data simply consists of the audio signal and labels of the five speech segments. For profiles that use synthesized data, this set was augmented by adding another five speech segments that were artificially created by adding some distortion as described above. Because the distortion was made randomly, this resulted in a slightly different training set for each epoch.\n",
    "\n",
    "The validation set was generated from randomly shifted and distorted audio signals of the training set.\n",
    "\n",
    "### Performance\n",
    "The RNN performance is measured with two metrics:\n",
    "\n",
    "* CTC-loss\n",
    "* Label Error Rate (LER)\n",
    "\n",
    "The calculation of the CTC-loss has been described as part of the description of CTC in [the introduction](00_Introduction.ipynb#Calculating-the-CTC-loss-by-creating-valid-alignments-with-dynamic-programming). The LER is calculated as follows.\n",
    "\n",
    "#### Label Error Rate (LER)\n",
    "The LER is defined as the [edit distance](https://www.tensorflow.org/api_docs/python/tf/edit_distance) between prediction (_hypothesis_) and actual labels (_ground truth_), also called the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). Observe for example the LER for the hypothesis `hello` and the truth `hallo`: The two strings differ in 1 of 5 characters, therefore the LER is `0.2`.\n",
    "\n",
    "The LER can also be computed for strings of different lengths. The following table gives an overview of a few samples.\n",
    "\n",
    "| Hypothesis | Truth | LER |\n",
    "|---|---|---|\n",
    "| 'hello' | 'hallo' | 0.2\n",
    "| 'hell' | 'hallo' | 0.4\n",
    "| 'hel' | 'hallo' | 0.6\n",
    "| 'helloo' | 'hallo' | 0.4\n",
    "| 'hellooo' | 'hallo' | 0.6\n",
    "| 'helo' | 'hallo' | 0.4\n",
    "| 'heloo' | 'hallo' | 0.4\n",
    "| 'helooo' | 'hallo' | 0.6\n",
    "| 'allo' | 'hallo' | 0.2\n",
    "| 'elo' | 'hallo' | 0.6\n",
    "\n",
    "You can also execute the cell below to see how the values are calculated with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    ('hello', 'hallo'),\n",
    "    ('hell', 'hallo'),\n",
    "    ('hel', 'hallo'),\n",
    "    ('helloo', 'hallo'),\n",
    "    ('hellooo', 'hallo'),\n",
    "    ('helo', 'hallo'),\n",
    "    ('heloo', 'hallo'),\n",
    "    ('helooo', 'hallo'),\n",
    "    ('allo', 'hallo'),\n",
    "    ('elo', 'hallo'),\n",
    "]\n",
    "\n",
    "import tensorflow as tf\n",
    "from util.rnn_util import *\n",
    "\n",
    "print('hypothesis'.ljust(15) + 'truth'.ljust(15) + 'LER'.ljust(10))\n",
    "print('-'.join('' for _ in range(40)))\n",
    "for hypothesis, truth in samples:\n",
    "    h_values = encode(hypothesis)\n",
    "    h_indices = [[0, i] for i in range(len(h_values))]\n",
    "    h_shape = [1, len(h_values)]\n",
    "    h_tensor = tf.SparseTensor(indices=h_indices, values=h_values, dense_shape=h_shape)\n",
    "    \n",
    "    t_values = encode(truth)\n",
    "    t_indices = [[0, i] for i in range(len(t_values))]\n",
    "    t_shape = [1, len(h_values)]\n",
    "    t_tensor = tf.SparseTensor(indices=t_indices, values=t_values, dense_shape=t_shape)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        ler = tf.edit_distance(h_tensor, t_tensor)\n",
    "        edit_distance = sess.run(ler)\n",
    "        print(f'{hypothesis.ljust(15)}{truth.ljust(15)}{str(edit_distance[0]).ljust(10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of convergence\n",
    "\n",
    "The RNN is trained until convergence. To decide when to stop, the term _convergence_ needs to be defined first. This can be done by observing the trend of the cost curve. For the PoCs in this notebook, _convergence_ was reached if the LER cost either met both of the following two criteria or 10'000 epochs have passed:\n",
    "\n",
    "1. **Criterion 1**: the prediction must be accurate enough.\n",
    "1. **Criterion 2**: the average LER cost must have plateaued, i.e. not change more than 1% over the last 10 epochs\n",
    "\n",
    "The first criterion is measured by calculating the mean LER-cost over the last 10 epochs. It must be below 0.05 to fullfill the criterion. This ensures that the predicted transcriptions diverge from the actual transcriptions by 5% at most (in terms of edit distance). The value of 5% is somewhat arbitrarily chosen. However it lies within the range of the LERs found for the best STT systems found in research papers.\n",
    "\n",
    "The second criterion ensures that training is not stopped too soon. Learning should still continue if the gradient of the average LER cost is still negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #1: MFCC, German, original data\n",
    "For the _ReadyLingua_ corpus the first corpus entry in German is the poem _\"An die Freude\"_ from F. Schiller. The first PoC is trained exclusively on the first five speech segments. These segments have the following transcript (normalized):\n",
    "\n",
    "    1. an die freude von friedrich schiller\n",
    "    2. freude schoner gotterfunken\n",
    "    3. tochter aus elysium\n",
    "    4. wir betreten feuertrunken himmlische dein heiligtum\n",
    "    5. deine zauber binden wieder\n",
    "    \n",
    "The RNN is trained by repeatedly feeding it these training samples. One iteration over the whole training set is called _epoch_. The RNN is trained until convergence as defined above. \n",
    "\n",
    "After each epoch, the training progress is validated by calculating the LER- and CTC-cost. For this, an artificial validation set is created by randomly shifting the audio signals from the training set by some milliseconds. Shifting is done by cropping a random number between 1 and 2000 samples from the beginning. By doing so, the audio signal is translated to the left by 125ms at most (with a sampling rate of 16kHz).\n",
    "\n",
    "It is evident that by training on such a small training set the RNN will hopelessly overfit to those 5 samples. Also, the validation set consist of variants of the training set and is therefore not representative because the RNN will have already seen the samples in some similar form. However, although the result are not representative, this PoC is a quick and cheap way to validate if the RNN is able to learn anything at all. The results can serve as a baseline for comparison.\n",
    "\n",
    "#### Results\n",
    "\n",
    "By comparing the actual transcription (_ground truth_) with the decoded output of the RNN (_prediction_) we can see that the RNN indeed learns how to recognize speech. The following log extract shows the learning progress during various stages of the learning progress.\n",
    "\n",
    "    Epoch 1:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       e    \n",
    "    \n",
    "    Epoch 5:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       irie e eiei e e e e ei \n",
    "    \n",
    "    Epoch 25:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       tirberete euernke uishe en heitum\n",
    "    \n",
    "    Epoch 38:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       wir betreten feuernkenfmimische ein heiligtum\n",
    "\n",
    "    Epoch 63:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       wir betreten feuertrunken himlische dein heiligum\n",
    "    \n",
    "It is evident that in the first few epochs the generated transcript does not make much sense. It consists mainly of the letter `e` which happens to be the most frequent character in a German text. With further iterations however, the generated transcripts become clearer until they match up almost perfectly with the actual transcripts. After just 63 epochs the RNN produces transcripts that are very similar to the originl ones. Note that in the last two epochs above the RNN has actually unlearned how to recognize the word _heiligtum_. This is possible in LSTM-networks, because LSTM-cells include a trainable parameter $\\Gamma_f$ that controls how much of the cell value of the previous time step is used to calculate the cell value in the current time step (forget-gate). For more information about LSTM-cells see [Christopher Olah's blog about understanding LSTM cells](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "The learning progress is also reflected in the plots for the CTC- and LER-cost:    \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc1_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc1_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC#2: MFCC, German, synthesized data\n",
    "\n",
    "To get a better intuition on how fast the RNN learns, it was trained on the same data again using MFCC-features. However, the original training data was augmented by adding synthesized speech segments that were created by distorting the audio signal as described above. The transcript remained the same.\n",
    "\n",
    "#### Results\n",
    "\n",
    "Because the RNN did not converge, training was aborted after more than 13.000 epochs. By that time the LER rate oscillated around a value between 0.07 and 0.08 with average change rates below 0.1%, which is close to the convergence criteria. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc2_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc2_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "This is also reflected in the results of the last epochs:\n",
    "\n",
    "    Ground truth (train-set):     an die freude von friedrich schiller\n",
    "    Prediction (train-set):       an die freude von f friedrich schiler            \n",
    "    Ground truth (train-set):     freude schoner gotterfunken\n",
    "    Prediction (train-set):       freude schoner goterfunken                       \n",
    "    Ground truth (train-set):     tochter aus elysium\n",
    "    Prediction (train-set):       tochter aus elysium                              \n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       wr betren feuertrunken himlische dein h heiligtum\n",
    "    Ground truth (train-set):     deine zauber binden wieder\n",
    "    Prediction (train-set):       deine zauber binde wieder    \n",
    "  \n",
    "#### Interpretation\n",
    "\n",
    "We can see that the RNN still learns the relationships between audio signal and transcription when trained on synthesized data. In contrast to PoC#1 the training batches were similar but no two training samples were exactly the same. This reduced overfitting and the risk of the RNN learning the mapping by heard, but required a considerably larger number of training epochs to get comparable results (more than 13k epochs compared to only a few dozen before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #3: German, Mel-Spectrograms, original data\n",
    "\n",
    "For the third PoC we use a corpus entry with similar properties like in PoC #1. The first corpus entry in the _LibriSpeech_ corpus is ... . Five speech segments with similar lenghts as in PoC #1 have the following transcripts:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc3_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc3_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Results\n",
    "Training the RNN the same way like shown above gives us the following progress:\n",
    "\n",
    "    tbd...   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #4: German, Mel-Spectrograms, synthesized data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc4_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc4_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #5: German, Power-Spectrograms, original data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc5_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc5_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #6: German, Power-Spectrograms, synthesized data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc6_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc6_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #7: English, MFCC, original data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc7_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc7_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #8: English, MFCC, synthesized data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc8_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc8_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #9: English, Mel-Spectrograms, original data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc9_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc9_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #10: English, Mel-Spectrograms, synthesized data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc10_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc10_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #11: English, Power-Spectrograms, original data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc11_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc11_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #12: English, Power-Spectrograms, Synthesized data\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc12_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc12_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook showed how a PoC was trained in order to gain insight about the usefulness of the different feature types (MFCC, Mel-Spectrograms and Power-Spectrograms. It also tried to assess the influence of language and synthetisized data on the training process.\n",
    "\n",
    "The results showed that the PoC was able to learn from MFCC features for German samples without synthetisized data. For this combination, the RNN converged pretty quickly and was able to make good predictions for audio data that was created using shifted and distorted versions of the training data.\n",
    "\n",
    "For other combinations the PoC did not converge and in some cases produce somewhat random results. Generally, the training curves exhibited high variance, meaning that there was a gap between training and validation loss. This is an indication for severe overfit that is probably a result of the lack of training data. This is also true for feature types with a small number of features (MFCC)-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
