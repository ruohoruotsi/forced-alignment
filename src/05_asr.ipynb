{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ASR-Stage\n",
    "\n",
    "In this Notebook a state-of-the-art model for ASR is presented. A simplified version of this model was implemented to be used in the ASR stage of the pipeline.\n",
    "\n",
    "## Deep Speech\n",
    "In 2014 <cite data-cite=\"undefined\"></cite> presented a RNN model called _Deep Speech_ to recognize speech from possible noisy environments. Since the model used CTC, it did non require pre-alignment e.g. by supplying a phonetic transcript. The model was trained on various corpora (_WSJ_, _Switchbboard_, _Fisher_, _Baidu_) containing both conversational and read speech. This data was enhanced by artificially adding background noise (_data synthesis_). Transcripts for sequences of audio frames were learned using a target alphabet that consisted of 29 characters (`a..z`, `space`, apostrophe and _blank_). Performance was measured with _Label Error Rate_ and _Word Error Rate_. The model architecture was remarkabily simple, consisting of only 5 layers, one of which was an RNN layer:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../assets/deep_speech_architecture.png\" />\n",
    "    <caption>DeepSpeech architecture (source: <cite data-cite=\"undefined\"></cite>)</caption>\n",
    "</figure>\n",
    "\n",
    "### Model layers\n",
    "The first three layers are _convolutional_ layers and not recurrent. The _clipped ReLU_ function was used as activation function, which is defined as:\n",
    "\n",
    "$$\n",
    "g(z) = \\min(\\max(0,z), 20)\n",
    "$$\n",
    "\n",
    "The fourth layer is recurrent with forward and backward recurrence (_bi-directional_). Hence the units in this layer share weights over time and depend on both the time frames before and after each time step. Note that no LSTM-cells were used in favor of a simpler model that requires less data. _Clipped ReLU_ was also used as activation functionfor this layer.\n",
    "\n",
    "The last layer is again non-recurrent, consisting of densely connected units (_fully-connected_) As activation function, softmax was used, yielding a probability for each character of the target alphabet. From the matrix of probabilities ($(T_x \\times 29)$) the loss was calculated by measuring the prediction error with CTC as described before.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "To prevent overfitting, dropout with values between 5% and 10% were applied for the first three (non-recurrent) layers. The values of the input signals were centered by subtracting the global mean and then scaled through division by the standard deviation. Additionally, each audio signal was shifted 5ms to the left and right to calculate two additional values per time frame. The probabilities were then calculated by averaging over all three values. Finally, an ensemble of several RNN was used at test time.\n",
    "\n",
    "### Model features and performance\n",
    "\n",
    "Spectrograms were used as features. Despite its simple architecture, the model outperformed previously published systems of that time. This was also possible because a language model (LM) was used to model the probabilities between sequences of letters and words with _n-grams_. This language model fixed error in transcript that were the result of learning plausible renderings of words, that were grammatically incorrect, like the following example taken from the original paper:\n",
    "\n",
    "| RNN output | Actual transcription |\n",
    "|---|---|\n",
    "| bostin | Boston |\n",
    "| arther n tickets | are there any tickets |\n",
    "\n",
    "Additionally, performance was improved by introducing a novel approach to parallelize calculations on multiple GPUs. Furthermore, computational effort was reduced through halving the time steps by only using every second time step in the bidirectional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation for this project\n",
    "\n",
    "Because training a state-of-the-art model was not required (and also not feasible) for this project, a simpler model should be trained. The idea was to find out whether the pipelined approach would still work when using a less capable model in the ASR stage. The ASR-model used in this project is therefore a simplified version of the Deep Speech model as presented in the original paper. Simplification was made in the following aspects:\n",
    "\n",
    "* no LM was used\n",
    "* no data synthetization was done, i.e. no audio translation, distortion or superposition of background noise\n",
    "* the first three layers are not convolutional, i.e. no striding in the spectrograms was applied to halve the time steps. This also means that no context frames were used to calculate the features for each frame. Instead, simple FC layers with a dropout of `0.1` were used.\n",
    "* the apostrophe was not part of the target alphabet, thus the target alphabet consisted of 28 characters (`a..z`, `space`, `blank`)\n",
    "* No ensembling was used\n",
    "* less training data was available (some hundred hours compared to some thousand for _DeepSpeech_\n",
    "\n",
    "Implementation was done in Python using [Keras](https://keras.io). The model was trained on both corpora (ReadyLingua and LibriSpeech) using different types of features (MFCC, Mel-Spectrograms, Power-Spectrograms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "usage: ipykernel_launcher.py [-h] [-c {rl,ls}] [-l LANGUAGE] [-b [BATCH_SIZE]]\n",
      "                             [-f [{mfcc,mel,pow}]] [-t [TARGET_ROOT]]\n",
      "                             [-e [NUM_EPOCHS]] [--train_steps [TRAIN_STEPS]]\n",
      "                             [--valid_steps [VALID_STEPS]]\n",
      "                             [--architecture [{ds1,ds2,x}]]\n",
      "ipykernel_launcher.py: error: argument -f/--feature_type: invalid choice: 'C:\\\\Users\\\\Daniel\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-f67fa66f-7e4e-487b-91e8-7f7b7aaa079d.json' (choose from 'mfcc', 'mel', 'pow')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from train_brnn import *\n",
    "\n",
    "model = deep_speech_model(num_features=13)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6174726/R96YXYYN": {
     "URL": "http://arxiv.org/abs/1412.5567",
     "author": [
      {
       "family": "Hannun",
       "given": "Awni Y."
      },
      {
       "family": "Case",
       "given": "Carl"
      },
      {
       "family": "Casper",
       "given": "Jared"
      },
      {
       "family": "Catanzaro",
       "given": "Bryan"
      },
      {
       "family": "Diamos",
       "given": "Greg"
      },
      {
       "family": "Elsen",
       "given": "Erich"
      },
      {
       "family": "Prenger",
       "given": "Ryan"
      },
      {
       "family": "Satheesh",
       "given": "Sanjeev"
      },
      {
       "family": "Sengupta",
       "given": "Shubho"
      },
      {
       "family": "Coates",
       "given": "Adam"
      },
      {
       "family": "Ng",
       "given": "Andrew Y."
      }
     ],
     "container-title": "CoRR",
     "id": "undefined",
     "issued": {
      "year": 2014
     },
     "title": "Deep Speech: Scaling up end-to-end speech recognition",
     "type": "article-journal",
     "volume": "abs/1412.5567"
    },
    "undefined": {
     "URL": "http://arxiv.org/abs/1412.5567",
     "author": [
      {
       "family": "Hannun",
       "given": "Awni Y."
      },
      {
       "family": "Case",
       "given": "Carl"
      },
      {
       "family": "Casper",
       "given": "Jared"
      },
      {
       "family": "Catanzaro",
       "given": "Bryan"
      },
      {
       "family": "Diamos",
       "given": "Greg"
      },
      {
       "family": "Elsen",
       "given": "Erich"
      },
      {
       "family": "Prenger",
       "given": "Ryan"
      },
      {
       "family": "Satheesh",
       "given": "Sanjeev"
      },
      {
       "family": "Sengupta",
       "given": "Shubho"
      },
      {
       "family": "Coates",
       "given": "Adam"
      },
      {
       "family": "Ng",
       "given": "Andrew Y."
      }
     ],
     "container-title": "CoRR",
     "id": "undefined",
     "issued": {
      "year": 2014
     },
     "title": "Deep Speech: Scaling up end-to-end speech recognition",
     "type": "article-journal",
     "volume": "abs/1412.5567"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
