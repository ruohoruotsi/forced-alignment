{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ASR-Stage\n",
    "\n",
    "In this Notebook a state-of-the-art model for ASR is presented. A simplified version of this model was implemented to be used in the ASR stage of the pipeline.\n",
    "\n",
    "## Deep Speech\n",
    "In 2014 <cite data-cite=\"undefined\"></cite> presented a RNN model called _Deep Speech_ to recognize speech from possible noisy environments. Since the model used CTC, it did non require pre-alignment e.g. through a phonetic transcript. The model was trained on various corpora (_WSJ_, _Switchbboard_, _Fisher_, _Baidu_) containing both conversational and read speech. This data was augmented by artificially adding background noise (_data synthesis_). The model was trained on spectrograms of this audio data. Transcripts for sequences of audio frames were learned using a target alphabet that consisted of 29 characters (`a..z`, `space`, apostrophe and _blank_). Performance was measured with _Label Error Rate_ and _Word Error Rate_. The model architecture was remarkabily simple, consisting of only 5 layers, one of which was an RNN layer:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../assets/deep_speech_architecture.png\" />\n",
    "    <caption>DeepSpeech architecture (source: <cite data-cite=\"undefined\"></cite>)</caption>\n",
    "</figure>\n",
    "\n",
    "### Model layers\n",
    "The first three layers are dense layers (_fully connected_) and not recurrent. The first layer is a bit special in that it implements a form of convolution by _striding_ the input with a step size of 2. This means that only every second time frame of the original input was taken, but this frame depended on context frames to the left and right. This convolution was done to optimize the training process because it shortens the length of the recurrent layer.\n",
    "\n",
    "For the first three layers, the _clipped ReLU_ function was used as activation function, which is defined as:\n",
    "\n",
    "$$\n",
    "g(z) = \\min(\\max(0,z), 20)\n",
    "$$\n",
    "\n",
    "The fourth layer is recurrent with forward and backward recurrence (_bi-directional_). Hence the units in this layer share weights over time and depend on both the time frames before and after each time step. Note that no LSTM-cells were used in favor of a simpler model that requires less data. _Clipped ReLU_ was again used as activation function for this layer.\n",
    "\n",
    "The last layer is again dense and non-recurrent. As activation function, softmax was used, yielding a probability for each character of the target alphabet. From the matrix of probabilities with dimensions $(T_x \\times 29)$ the loss was calculated by measuring the prediction error with CTC as described before.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "To prevent overfitting, dropout with values between 5% and 10% were applied for the first three (non-recurrent) layers. The values of the input signals were centered by subtracting the global mean and then scaled through division by the standard deviation. Additionally, each audio signal was shifted 5ms to the left and right to calculate two additional values per time frame. The probabilities for this time frame were then calculated by averaging over all three values. Finally, an ensemble of several RNN was used at test time.\n",
    "\n",
    "### Model features and performance\n",
    "\n",
    "The features used for training were extracted from the audio by calculating spectrograms with a window size of 20ms and a stride of 10ms. The research team did not resort to more sophisticated features like Mel-Spectrograms or MFCC. Despite its simplicity in architecture or feature engineering, the model outperformed previously published systems of that time. This was also possible because a language model (LM) was used to model the probabilities between sequences of letters and words with _n-grams_. This language model fixed error in transcript that were the result of learning plausible renderings of words, that were grammatically incorrect, like the following example taken from the original paper:\n",
    "\n",
    "| RNN output | Actual transcription |\n",
    "|---|---|\n",
    "| bostin | Boston |\n",
    "| arther n tickets | are there any tickets |\n",
    "\n",
    "Additionally, performance was improved by introducing a novel approach to parallelize calculations on multiple GPUs. Furthermore, computational effort was reduced through halving the time steps by only using every second time step in the bidirectional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation for this project\n",
    "\n",
    "Because training a state-of-the-art model was not required (and also not feasible) for this project, a simpler model should be trained. A simpler model also allowed for shorter training times and therefore faster feedback cycles. This was crucial for this project as only one GPU was available and the available project time for this stage of the pipeline was very limited. The idea was to find out whether the pipelined approach would still work when using a less capable model in the ASR stage. The ASR-model used in this project is therefore a simplified version of the Deep Speech model as presented in the original paper. Simplification was made in the following aspects:\n",
    "\n",
    "* no LM was used\n",
    "* no data synthetization was done, i.e. no audio translation, distortion or superposition of background noise\n",
    "* the first layer is a simple FC layer, i.e. no striding in the spectrograms was applied to halve the time steps. This also means that no context frames were used to calculate the features for each frame.\n",
    "* the apostrophe was not part of the target alphabet, thus the target alphabet consisted of 28 characters (`a..z`, `space`, `blank`)\n",
    "* No ensembling was used\n",
    "* less training data was available (some hundred hours compared to some thousand for _DeepSpeech_)\n",
    "\n",
    "The architecture was inspired by code in [this repository](https://github.com/igormq/asr-study). Implementation was done in Python using [Keras](https://keras.io) with [TensorFlow](http://tensorflow.org) backend. The model was trained on both corpora (ReadyLingua and LibriSpeech) using different types of features (MFCC, Mel-Spectrograms, Power-Spectrograms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and optimization\n",
    "\n",
    "Similar to the original DeepSpeech model, the simplified model consist of 5 layers, whereas the first three layers are fully connected with a dropout of `0.1`.\n",
    "\n",
    "Optimization was done using the ADAM (Adaptive Moment Estimation) Optimizer <cite data-cite=\"6174726/8INI335A\"></cite>, which is a combination Gradient Descent with Momentum (GDM) and Root Mean Square prop (RMSprop). This Optimizer uses parameters $\\beta_1$ and $\\beta_2$ as weights for the moving average (from GDM) and the damping (from RMSprop) and has been shown to work well with a wide range of learning problems. Values were set to $b_1=0.9$ and $b_2=0.999$ together with a learning rate of $\\alpha=0.1$. For more information see the [Keras API Documentation for Adam](https://keras.io/optimizers/#adam).\n",
    "\n",
    "Training was done in batches for 20 epochs. After each epoch the CTC loss and the LER was measured, but only the CTC loss was optimized. Decoding was done non-greedily with beam search using a beam width of 100.\n",
    "\n",
    "#### Input layer\n",
    "The input layer has shape $(N, T_x, f)$, where $N$ is the batch size, $T_x$ the sequence length and $f$ the number of features. A batch size of $N=5$ was chosen for training. Since the ASR model is trained on speech segments of different lengths, $T_x$ is determined by the longest segment in each batch. Shorter segments are zero-padded to match $T_x$. However, the value of $T_x$ may vary between batches. Like with the PoC, the number of features $f$ (and therefore the number of units in the input layer) depends on the type of features used for training. It was set to default values of $f=13$ for MFCC-features, $f=40$ for Mel-Spectrograms and $f=161$ for power-spectrograms.\n",
    "\n",
    "Because calculating the features is time-consuming, they were pre-calculated and stored in a [HDF5](https://h5py.org)-file to speed up the training process. According to the [Space-time tradeoff](https://en.wikipedia.org/wiki/Space%E2%80%93time_tradeoff) these files can become quite big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other layers\n",
    "\n",
    "The other layers correspond more or less to the layers of the _DeepSpeech_ model with the simplifications described above. Execute the following cell to get the Keras summary describing the architecture for a model that can be trained on MFCC-features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.brnn_util import *\n",
    "\n",
    "model = deep_speech_model(num_features=13)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<div class=\"cite2c-biblio\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6174726/8INI335A": {
     "URL": "http://arxiv.org/abs/1412.6980",
     "author": [
      {
       "family": "Kingma",
       "given": "Diederik P."
      },
      {
       "family": "Ba",
       "given": "Jimmy"
      }
     ],
     "container-title": "CoRR",
     "id": "6174726/8INI335A",
     "issued": {
      "year": 2014
     },
     "title": "Adam: A Method for Stochastic Optimization",
     "type": "article-journal",
     "volume": "abs/1412.6980"
    },
    "6174726/R96YXYYN": {
     "URL": "http://arxiv.org/abs/1412.5567",
     "author": [
      {
       "family": "Hannun",
       "given": "Awni Y."
      },
      {
       "family": "Case",
       "given": "Carl"
      },
      {
       "family": "Casper",
       "given": "Jared"
      },
      {
       "family": "Catanzaro",
       "given": "Bryan"
      },
      {
       "family": "Diamos",
       "given": "Greg"
      },
      {
       "family": "Elsen",
       "given": "Erich"
      },
      {
       "family": "Prenger",
       "given": "Ryan"
      },
      {
       "family": "Satheesh",
       "given": "Sanjeev"
      },
      {
       "family": "Sengupta",
       "given": "Shubho"
      },
      {
       "family": "Coates",
       "given": "Adam"
      },
      {
       "family": "Ng",
       "given": "Andrew Y."
      }
     ],
     "container-title": "CoRR",
     "id": "undefined",
     "issued": {
      "year": 2014
     },
     "title": "Deep Speech: Scaling up end-to-end speech recognition",
     "type": "article-journal",
     "volume": "abs/1412.5567"
    },
    "undefined": {
     "URL": "http://arxiv.org/abs/1412.5567",
     "author": [
      {
       "family": "Hannun",
       "given": "Awni Y."
      },
      {
       "family": "Case",
       "given": "Carl"
      },
      {
       "family": "Casper",
       "given": "Jared"
      },
      {
       "family": "Catanzaro",
       "given": "Bryan"
      },
      {
       "family": "Diamos",
       "given": "Greg"
      },
      {
       "family": "Elsen",
       "given": "Erich"
      },
      {
       "family": "Prenger",
       "given": "Ryan"
      },
      {
       "family": "Satheesh",
       "given": "Sanjeev"
      },
      {
       "family": "Sengupta",
       "given": "Shubho"
      },
      {
       "family": "Coates",
       "given": "Adam"
      },
      {
       "family": "Ng",
       "given": "Andrew Y."
      }
     ],
     "container-title": "CoRR",
     "id": "undefined",
     "issued": {
      "year": 2014
     },
     "title": "Deep Speech: Scaling up end-to-end speech recognition",
     "type": "article-journal",
     "volume": "abs/1412.5567"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
