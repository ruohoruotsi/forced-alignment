{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ASR-Stage\n",
    "\n",
    "In this Notebook a state-of-the-art model for ASR is presented. A simplified version of this model was implemented to be used in the ASR stage of the pipeline.\n",
    "\n",
    "## Deep Speech\n",
    "In 2014 <cite data-cite=\"undefined\"></cite> presented a RNN model called _Deep Speech_ to recognize speech from possible noisy environments. Since the model used CTC, it did non require pre-alignment e.g. by supplying a phonetic transcript. The model was trained on various corpora (_WSJ_, _Switchbboard_, _Fisher_, _Baidu_) containing both conversational and read speech. This data was augmented by artificially adding background noise (_data synthesis_). Transcripts for sequences of audio frames were learned using a target alphabet that consisted of 29 characters (`a..z`, `space`, apostrophe and _blank_). Performance was measured with _Label Error Rate_ and _Word Error Rate_. The model architecture was remarkabily simple, consisting of only 5 layers, one of which was an RNN layer:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../assets/deep_speech_architecture.png\" />\n",
    "    <caption>DeepSpeech architecture (source: <cite data-cite=\"undefined\"></cite>)</caption>\n",
    "</figure>\n",
    "\n",
    "### Model layers\n",
    "The first three layers are _convolutional_ layers and not recurrent. The _clipped ReLU_ function was used as activation function, which is defined as:\n",
    "\n",
    "$$\n",
    "g(z) = \\min(\\max(0,z), 20)\n",
    "$$\n",
    "\n",
    "The fourth layer is recurrent with forward and backward recurrence (_bi-directional_). Hence the units in this layer share weights over time and depend on both the time frames before and after each time step. Note that no LSTM-cells were used in favor of a simpler model that requires less data. _Clipped ReLU_ was also used as activation functionfor this layer.\n",
    "\n",
    "The last layer is again non-recurrent, consisting of densely connected units (_fully-connected_) As activation function, softmax was used, yielding a probability for each character of the target alphabet. From the matrix of probabilities ($(T_x \\times 29)$) the loss was calculated by measuring the prediction error with CTC as described before.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "To prevent overfitting, dropout with values between 5% and 10% were applied for the first three (non-recurrent) layers. The values of the input signals were centered by subtracting the global mean and then scaled through division by the standard deviation. Additionally, each audio signal was shifted 5ms to the left and right to calculate two additional values per time frame. The probabilities were then calculated by averaging over all three values. Finally, an ensemble of several RNN was used at test time.\n",
    "\n",
    "### Model features and performance\n",
    "\n",
    "Spectrograms were used as features. Despite its simple architecture, the model outperformed previously published systems of that time. This was also possible because a language model (LM) was used to model the probabilities between sequences of letters and words with _n-grams_. This language model fixed error in transcript that were the result of learning plausible renderings of words, that were grammatically incorrect, like the following example taken from the original paper:\n",
    "\n",
    "| RNN output | Actual transcription |\n",
    "|---|---|\n",
    "| bostin | Boston |\n",
    "| arther n tickets | are there any tickets |\n",
    "\n",
    "Additionally, performance was improved by introducing a novel approach to parallelize calculations on multiple GPUs. Furthermore, computational effort was reduced through halving the time steps by only using every second time step in the bidirectional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation for this project\n",
    "\n",
    "Because training a state-of-the-art model was not required (and also not feasible) for this project, a simpler model should be trained. A simpler model also allowed for shorter training times and therefore faster feedback cycles. This was crucial for this project as only one GPU was available and the available project time for this stage of the pipeline was very limited. The idea was to find out whether the pipelined approach would still work when using a less capable model in the ASR stage. The ASR-model used in this project is therefore a simplified version of the Deep Speech model as presented in the original paper. Simplification was made in the following aspects:\n",
    "\n",
    "* no LM was used\n",
    "* no data synthetization was done, i.e. no audio translation, distortion or superposition of background noise\n",
    "* the first three layers are not convolutional, i.e. no striding in the spectrograms was applied to halve the time steps. This also means that no context frames were used to calculate the features for each frame. Instead, simple FC layers with a dropout of `0.1` were used.\n",
    "* the apostrophe was not part of the target alphabet, thus the target alphabet consisted of 28 characters (`a..z`, `space`, `blank`)\n",
    "* No ensembling was used\n",
    "* less training data was available (some hundred hours compared to some thousand for _DeepSpeech_\n",
    "\n",
    "Implementation was done in Python using [Keras](https://keras.io) with [TensorFlow](http://tensorflow.org) backend. The model was trained on both corpora (ReadyLingua and LibriSpeech) using different types of features (MFCC, Mel-Spectrograms, Power-Spectrograms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "Similar to the original DeepSpeech model, the simplified model consist of 5 layers, whereas the first three layers are fully connected (in contrast to convolutional in the original model). This architecture was inspired by code in [this repository](https://github.com/igormq/asr-study). \n",
    "\n",
    "#### Input layer\n",
    "The input layer has shape $(N, T_x, f)$, where $N$ is the batch size, $T_x$ the sequence length and $f$ the number of features. A batch size of $N=32$ was arbitrarily chosen for training. Since the ASR model is trained on speech segments of different lengths, T_x is determined by the longest segment in each batch. Shorter segments are zero-padded to match $T_x$. However, the value of $T_x$ may vary between batches. The number of features $f$ depends on the type of features used for training and was set to default values of $f=13$ for MFCC-features, $f=40$ for Mel-Spectrograms and $f=161$ for power-spectrograms.\n",
    "\n",
    "Because calculating the features is time-consuming, they were pre-calculated and stored in a [HDF5](https://h5py.org)-file to speed up the training process. According to the [Space-time tradeoff](https://en.wikipedia.org/wiki/Space%E2%80%93time_tradeoff) these files can become quite big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other layers\n",
    "\n",
    "The other layers correspond more or less to the layers of the _DeepSpeech_ model with the simplifications described above. Execute the following cell to get the Keras summary describing the architecture for a model that can be trained on MFCC-features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.brnn_util import *\n",
    "\n",
    "model = deep_speech_model(num_features=13)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6174726/R96YXYYN": {
     "URL": "http://arxiv.org/abs/1412.5567",
     "author": [
      {
       "family": "Hannun",
       "given": "Awni Y."
      },
      {
       "family": "Case",
       "given": "Carl"
      },
      {
       "family": "Casper",
       "given": "Jared"
      },
      {
       "family": "Catanzaro",
       "given": "Bryan"
      },
      {
       "family": "Diamos",
       "given": "Greg"
      },
      {
       "family": "Elsen",
       "given": "Erich"
      },
      {
       "family": "Prenger",
       "given": "Ryan"
      },
      {
       "family": "Satheesh",
       "given": "Sanjeev"
      },
      {
       "family": "Sengupta",
       "given": "Shubho"
      },
      {
       "family": "Coates",
       "given": "Adam"
      },
      {
       "family": "Ng",
       "given": "Andrew Y."
      }
     ],
     "container-title": "CoRR",
     "id": "undefined",
     "issued": {
      "year": 2014
     },
     "title": "Deep Speech: Scaling up end-to-end speech recognition",
     "type": "article-journal",
     "volume": "abs/1412.5567"
    },
    "undefined": {
     "URL": "http://arxiv.org/abs/1412.5567",
     "author": [
      {
       "family": "Hannun",
       "given": "Awni Y."
      },
      {
       "family": "Case",
       "given": "Carl"
      },
      {
       "family": "Casper",
       "given": "Jared"
      },
      {
       "family": "Catanzaro",
       "given": "Bryan"
      },
      {
       "family": "Diamos",
       "given": "Greg"
      },
      {
       "family": "Elsen",
       "given": "Erich"
      },
      {
       "family": "Prenger",
       "given": "Ryan"
      },
      {
       "family": "Satheesh",
       "given": "Sanjeev"
      },
      {
       "family": "Sengupta",
       "given": "Shubho"
      },
      {
       "family": "Coates",
       "given": "Adam"
      },
      {
       "family": "Ng",
       "given": "Andrew Y."
      }
     ],
     "container-title": "CoRR",
     "id": "undefined",
     "issued": {
      "year": 2014
     },
     "title": "Deep Speech: Scaling up end-to-end speech recognition",
     "type": "article-journal",
     "volume": "abs/1412.5567"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
