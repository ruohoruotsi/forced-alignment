{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "The purpose of this notebook is to show how Forced Alignment on an unknown audio/transcript pair could be done by combining all the stages from the previous notebooks:\n",
    "\n",
    "- **VAD-Stage**: The speech parts are extracted from the audio signal using WebRTC\n",
    "- **ASR-Stage**: The speech parts are transcribed using an RNN. Because only PoCs were trained for this stage, ceiling analysis is done for this stage by using a state-of-the art model. We will use [Google's Speech-to-Text API](https://cloud.google.com/speech-to-text/) for this.\n",
    "- **LSA-Stage**: The partial transcripts are aligned with the original transcript using the Smith-Waterman algorithm and the Levenshtein Similarity.\n",
    "\n",
    "All these are applied on individual pairs of audio/transcript for demonstration purposes. The alignments are visualized in a HTML page containing an audio player and the transcript, where the alignments are highlighted as the audio plays. In order to be able to see these pages, you need to start a server, which can be done by running the following command from the source directory of this project:\n",
    "\n",
    "    python ./demos/server.py\n",
    "    \n",
    "A list of already prepared alignments can den be found under [http://localhost:8000](http://localhost:8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.audio_util import *\n",
    "from util.corpus_util import *\n",
    "from util.vad_util import *\n",
    "from util.asr_util import *\n",
    "from util.lsa_util import *\n",
    "\n",
    "import librosa\n",
    "from pattern3.metrics import levenshtein_similarity\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "demo_dir = join('..', 'assets', 'demo_files')\n",
    "\n",
    "def show_link(url, text):\n",
    "    display(HTML(f\"<a href='{url}'>{text}</a>\"))\n",
    "\n",
    "def vad(audio, rate):\n",
    "    voice_segments = extract_voice(audio, rate)\n",
    "    print(f'got {len(voice_segments)} voice_segments')\n",
    "    return voice_segments\n",
    "\n",
    "def asr(voice_segments, max_segments=10, language='en'):\n",
    "    voice_segments = transcribe(voice_segments[:max_segments], language)\n",
    "\n",
    "    print(f'ASR-transcripts of first {max_segments} voiced segments:')\n",
    "    print()\n",
    "    for i, voice in enumerate(voice_segments, 1):    \n",
    "        print(i, voice.transcript)\n",
    "        display(Audio(data=voice.audio, rate=voice.rate))\n",
    "        \n",
    "    return voice_segments\n",
    "\n",
    "def lsa(voice_segments, transcript):\n",
    "    alignments = align(voice_segments, transcript)\n",
    "    for al in alignments:\n",
    "        partial_transcript = al.transcript\n",
    "        alignment_text = al.alignment_text\n",
    "        edit_distance = levenshtein_similarity(partial_transcript.upper(), alignment_text.upper())\n",
    "        print(f'similarity: {edit_distance}, transcript: «{partial_transcript}», aligned text: «{alignment_text}»')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "We will use a recording and a transcript of Donald Trump's weekly address made on February 11, 2018. Audio and text were downloaded [here](https://www.whitehouse.gov/briefings-statements/president-donald-j-trumps-weekly-address-27/). Apart from extracting the audio from the video as MP3, no processing was done. Also the example has not been used in any way before.\n",
    "\n",
    "You can see the unaligned audio and transcript below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_audio_path = join(demo_dir, 'address.mp3')\n",
    "example_audio_transcript = join(demo_dir, 'address.txt')\n",
    "\n",
    "audio, rate = read_audio(example_audio_path)\n",
    "transcript = Path(example_audio_transcript).read_text(encoding='utf-8')\n",
    "\n",
    "display(Audio(data=audio, rate=rate))\n",
    "display(HTML(transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAD Stage\n",
    "The audio signal of the candidate is approximately 2:30 minutes long and can be split into 41 voiced segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_segments = vad(audio, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR stage\n",
    "Each segment can now be transcribed by using the Google-STT API. Note that it may take some time to process all segments. Therefore only the first 10 voiced segments are transcribed here for demonstration purposes. **Also note that free usage of the API is constrained to a time and/or call limit. Excessively executing the following cell will therefore lead to deplete the usage limit!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_segments = asr(voice_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the STT-API provides very good transcriptions, except for number `3` where the term _and lawmakers joined me_ was transcribed as _hello Americans join me_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA-Stage\n",
    "\n",
    "To see whether the output quality of the ASR stage is high enough to align the individual partial transcripts with the original transcript, the Smith-Waterman algorithm from the LSA stage can be applied. The resulting (textual) alignment and the temporal information from the speech segments can then be combined to obtain an alignment between the original audio/transcript pair.\n",
    "\n",
    "The following code calculates the alignments for the 10 partial transcripts above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa(voice_segments, transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The so retreived sequence alignment information can be combined with the temporal information to get the alignment between audio signal and transcript. This has been done for the whole sample (all 41 voiced segments). While doing so, the following intermediary results have been saved:\n",
    "\n",
    "* [file containing the results of the ASR stage (partial transcripts)](../demos/htdocs/address/transcript_asr.txt): blank lines mean no transcript could be generated\n",
    "* [file containing the results of the LSA stage (alignments)](../demos/htdocs/address/alignment.txt): This includes the score for the Leventhstein Similarity\n",
    "\n",
    "The end result can be viewed [here](http://localhost:8000/address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more difficult example\n",
    "\n",
    "To see how the pipeline performs on more challenging examples, we will use a sample from the Librivox corpus (ID=171001). This sample exhibits the following difficulties:\n",
    "\n",
    "* the recording is much longer (approximately 30 minutes)\n",
    "* the recording does not always match up with the transcript (e.g. theres the Librivox preamble about licenses, which is not included in the transcript)\n",
    "* the recording contains passages with quite some slang or old English using words that are not used anymore today. Those passages might be hard to recognize in the ASR stage and have been transcribed using non-standard syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_audio_path = join(demo_dir, '171001.mp3')\n",
    "example_audio_transcript = join(demo_dir, '171001.txt')\n",
    "\n",
    "# only load first 64 seconds from audio\n",
    "audio, rate = librosa.load(example_audio_path, duration=64)\n",
    "# read first 24 lines from transcript\n",
    "with open(example_audio_transcript, 'r', encoding='utf-8') as myfile:\n",
    "    transcript = '\\n'.join([next(myfile) for x in range(24)])\n",
    "\n",
    "display(Audio(data=audio, rate=rate))\n",
    "display(HTML(transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAD stage\n",
    "\n",
    "The VAD stage yields much more voiced segments than are present in the corpups' metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_segments = vad(audio, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR stage\n",
    "\n",
    "Again, the first 10 voiced segments are transcribed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_segments = asr(voice_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these transcripts with the original transcript it becomes clear that the generated transcripts are less perfect than in the simple example above. Some errors in the ASR-generated transcripts can immediately be spotted (e.g. _stubbly ratchet_ instead of _doubly wretched_, _traps a prima_ instead of _trap's abramuh_ etc.). Also, although not visible from the 10 first examples, for some segments the STT-API was unable to generate a transcript.\n",
    "\n",
    "On the other hand, we can see that even though some of the generated transcript contain errors, their pronunciation is actually very close to the original transcript and could indeed be a valid transcription in other situations (e.g. _LC_ or _else be_ instead of _Elsie_). Also the words and sentences of the transcripts are orthographically and grammatically correct, a clear indication that a language model has been used to improve the raw results from the first pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA stage\n",
    "\n",
    "Finally, the transcribed 10 segments are aligned with the original transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa(voice_segments, transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, the whole pipeline was processed for the whole example (all 521 voiced segments) and the results have been saved to:\n",
    "\n",
    "* [file containing the results of the ASR stage (partial transcripts)](../demos/htdocs/171001/transcript_asr.txt): blank lines mean no transcript could be generated\n",
    "* [file containing the results of the LSA stage (alignments)](../demos/htdocs/171001/alignment.txt): This includes the score for the Leventhstein Similarity\n",
    "\n",
    "The end result can be viewed [here](http://localhost:8000/171001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example in German\n",
    "\n",
    "For the sake of completeness, an audio/transcription pair in a language other than English shall be aligned. The poem _An die Freude_ by Friedrich Schiller is used for this with the following audio and transcript: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_audio_path = join(demo_dir, 'andiefreude.mp3')\n",
    "example_audio_transcript = join(demo_dir, 'andiefreude.txt')\n",
    "\n",
    "audio, rate = read_audio(example_audio_path)\n",
    "transcript = Path(example_audio_transcript).read_text(encoding='utf-8')\n",
    "\n",
    "display(Audio(data=audio, rate=rate))\n",
    "display(HTML(transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAD + ASR + LSA stage\n",
    "\n",
    "The following cell contains the code to put the first 10 speech segments through the pipeline. As before, the alignment for the whole poem can be viewed [here](http://localhost:8000/andiefreude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_segments = vad(audio, rate)\n",
    "voice_segments = asr(voice_segments, language='de')\n",
    "lsa(voice_segments, transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own example\n",
    "\n",
    "To see how the pipeline works with your own examples, provide an absolute path to an audio file and a transcription. For example you can record a [random article on Wikipedia](https://en.wikipedia.org/wiki/Special:Random) and save both the text and the recording in `yourname.mp3` and `yourname.txt`. For the recording MP3 and WAV files are supported formats. The quality of the recording will have an impact on the alignment result. The transcription must be given as a simple UTF8-encoded text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = r'D:\\code\\ip8\\assets\\demo_files\\daniel.mp3' # enter path to audio file her (MP3 or WAV)\n",
    "transcript_path = r'D:\\code\\ip8\\assets\\demo_files\\daniel.txt' # enter path to transcript here (.txt file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.e2e_util import *\n",
    "\n",
    "url = create_demo(audio_path, transcript_path)\n",
    "show_link(url, 'Click here to go to your alignment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook the pipeline approach proposed by this project was evaluated on different combinations of audio/transcriptions. A ceiling analysis was performed by using an API Google's STT-engine instead of an own implementation for an ASR-engine. By replacing the most critical component with a state-of-the-art model for ASR, the pipeline was able to produce fairly good, although not perfect examples. Alignments were particularly difficult to generate if the audio contained slang or if the recording quality was bad.\n",
    "\n",
    "Despite the pipeline being highly dependent on the quality of the ASR stage, the hypothesis [formulated at the beginning](00_Introduction.ipynb#Plan-and-Hypothesis) could be verified. The pipeline can be sait to generally work provided the quality of the partial transcriptions is high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
