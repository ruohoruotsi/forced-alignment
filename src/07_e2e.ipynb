{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "The purpose of this notebook is to see if the pipeline works. For this wee choose a audio/transcript pair and run it through all stages described in the previous notebooks:\n",
    "\n",
    "- **VAD-Stage**: The speech parts are extracted from the audio signal using WebRTC\n",
    "- **ASR-Stage**: The speech parts are transcribed using an RNN. Because only PoCs were trained for this stage, ceiling analysis is done for this stage by using a state-of-the art model. We will use [Google's Speech-to-Text API](https://cloud.google.com/speech-to-text/) for this.\n",
    "- **LSA-Stage**: The partial transcripts are aligned with the original transcript using the Smith-Waterman algorithm and the Levenshtein Similarity.\n",
    "\n",
    "The alignments are visualized in a HTML page containing an audio player and the transcript, where the alignments are highlighted as the audio plays. If you checked out the code yourself, you need to start a server In order to be able to see these pages. Run the following command from the source directory of this project:\n",
    "\n",
    "    python ./demos/server.py\n",
    "    \n",
    "The server is then available under [http://localhost:8000](http://localhost:8888).\n",
    "    \n",
    "If you read this page as part of the official project documentation ([http://ip8.tiefenauer.info](http://ip8.tiefenauer.info)) you don't have to do anything. A list of already prepared alignments can den be found under [http://ip8.tiefenauer.info:8888](http://ip8.tiefenauer.info:8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.audio_util import *\n",
    "from util.corpus_util import *\n",
    "from util.vad_util import *\n",
    "from util.asr_util import *\n",
    "from util.lsa_util import *\n",
    "\n",
    "import librosa\n",
    "from pattern3.metrics import levenshtein_similarity\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML, Audio\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "demo_dir = join('..', 'assets', 'demo_files')\n",
    "\n",
    "def show_link(url, text):\n",
    "    display(HTML(f\"<a href='{url}'>{text}</a>\"))\n",
    "\n",
    "def vad(audio, rate):\n",
    "    voice_segments = extract_voice(audio, rate)\n",
    "    print(f'got {len(voice_segments)} voice_segments')\n",
    "    return voice_segments\n",
    "\n",
    "def asr(voice_segments, max_segments=10, language='en'):\n",
    "    voice_segments = transcribe(voice_segments[:max_segments], language)\n",
    "\n",
    "    print(f'ASR-transcripts of first {max_segments} voiced segments:')\n",
    "    print()\n",
    "    for i, voice in enumerate(voice_segments, 1):    \n",
    "        print(i, voice.transcript)\n",
    "        display(Audio(data=voice.audio, rate=voice.rate))\n",
    "        \n",
    "    return voice_segments\n",
    "\n",
    "def lsa(voice_segments, transcript):\n",
    "    alignments = align(voice_segments, transcript)\n",
    "    for al in alignments:\n",
    "        partial_transcript = al.transcript\n",
    "        alignment_text = al.alignment_text\n",
    "        edit_distance = levenshtein_similarity(partial_transcript.upper(), alignment_text.upper())\n",
    "        print(f'similarity: {edit_distance}, transcript: «{partial_transcript}», aligned text: «{alignment_text}»')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example in English\n",
    "\n",
    "We will use a recording and a transcript of Donald Trump's weekly address made on February 11, 2018. Audio and text were downloaded [here](https://www.whitehouse.gov/briefings-statements/president-donald-j-trumps-weekly-address-27/). Apart from extracting the audio from the video as MP3, no processing was done. Also the example has not been used in any way before.\n",
    "\n",
    "You can see the unaligned audio and transcript below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_audio_path = join(demo_dir, 'address.mp3')\n",
    "example_audio_transcript = join(demo_dir, 'address.txt')\n",
    "\n",
    "audio, rate = read_audio(example_audio_path)\n",
    "transcript = Path(example_audio_transcript).read_text(encoding='utf-8')\n",
    "\n",
    "display(Audio(data=audio, rate=rate))\n",
    "display(HTML(transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAD Stage\n",
    "The audio signal of the candidate is approximately 2:30 minutes long and can be split into 41 voiced segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_segments = vad(audio, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR stage\n",
    "Each segment can now be transcribed by using the Google-STT API. Note that it may take some time to process all segments. Therefore only the first 10 voiced segments are transcribed here for demonstration purposes. **Also note that free usage of the API is constrained to a time and/or call limit. Excessively executing the following cell will therefore lead to deplete the usage limit!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_segments = asr(voice_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the STT-API provides very good transcriptions, except for number `3` where the term _and lawmakers joined me_ was transcribed as _hello Americans join me_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA-Stage\n",
    "\n",
    "To see whether the output quality of the ASR stage is high enough to align the individual partial transcripts with the original transcript, the Smith-Waterman algorithm from the LSA stage can be applied. The resulting (textual) alignment and the temporal information from the speech segments can then be combined to obtain an alignment between the original audio/transcript pair.\n",
    "\n",
    "The following code calculates the alignments for the 10 partial transcripts above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa(voice_segments, transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The so retreived sequence alignment information can be combined with the temporal information to get the alignment between audio signal and transcript. This has been done for the whole sample (all 41 voiced segments). The following intermediary results have been saved:\n",
    "\n",
    "* [file containing the results of the ASR stage (partial transcripts)](../demos/htdocs/address/transcript_asr.txt): blank lines mean no transcript could be generated\n",
    "* [file containing the results of the LSA stage (alignments)](../demos/htdocs/address/alignment.txt): This includes the score for the Leventhstein Similarity\n",
    "\n",
    "The end result can be viewed [here](http://ip8.tiefenauer.info:8888/address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example in German\n",
    "\n",
    "For the sake of completeness, an audio/transcription pair in a language other than English shall be aligned. The poem _An die Freude_ by Friedrich Schiller is used for this with the following audio and transcript: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_audio_path = join(demo_dir, 'andiefreude.mp3')\n",
    "example_audio_transcript = join(demo_dir, 'andiefreude.txt')\n",
    "\n",
    "audio, rate = read_audio(example_audio_path)\n",
    "transcript = Path(example_audio_transcript).read_text(encoding='utf-8')\n",
    "\n",
    "display(Audio(data=audio, rate=rate))\n",
    "display(HTML(transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAD + ASR + LSA stage\n",
    "\n",
    "The following cell contains the code to put the first 10 speech segments through the pipeline. As before, the alignment for the whole poem can be viewed [here](http://ip8.tiefenauer.info:8888/andiefreude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_segments = vad(audio, rate)\n",
    "voice_segments = asr(voice_segments, language='de')\n",
    "lsa(voice_segments, transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own example\n",
    "\n",
    "To see how the pipeline works with your own examples, go to the [assets folder](/tree/assets/demo_files) and upload fhe following two files:\n",
    "\n",
    "* an audio file (MP3 or WAV)\n",
    "* a transcription file (TXT, UTF-8 encoded text)\n",
    "\n",
    "For example you can record a [random article on Wikipedia](https://en.wikipedia.org/wiki/Special:Random). The quality of the recording will have an impact on the alignment result.\n",
    "\n",
    "When you have uploaded both files, provide the file names below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = r'myexample.mp3' # enter name of audio file her (MP3 or WAV)\n",
    "trans_file = r'myexample.txt' # enter name of transcript here (.txt file)\n",
    "\n",
    "import os\n",
    "from os.path import join, exists\n",
    "audio_path = join(demo_dir, audio_file)\n",
    "trans_path = join(demo_dir, trans_file)\n",
    "\n",
    "if not exists(audio_path):\n",
    "    print(f'error: audio file does not exist: {audio_path}')\n",
    "else:\n",
    "    print(f'using audio file: {audio_path}')\n",
    "    \n",
    "if not exists(trans_path):\n",
    "    print(f'error: transcription file does not exist: {trans_path}')\n",
    "else:\n",
    "    print(f'using transcription file: {audio_path}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cell to start the pipeline. You will see a link with the URL when the process is finished pointing to `https://ip8.tiefenauer.info`. When reading this running the Jupyter Notebook server on your own machine you have to change this part to your local loopback (`http://localhost...`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.e2e_util import *\n",
    "\n",
    "url = create_demo(audio_path, trans_path)\n",
    "show_link(url, 'Click here to go to your alignment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook the pipeline approach proposed by this project was evaluated on different combinations of audio/transcriptions. Ceiling analysis on the ASR stage was performed by using an API Google's STT-engine instead of an own implementation for an ASR-engine. By replacing the most critical component with a state-of-the-art model for ASR, the pipeline was able to produce fairly good, although not perfect examples. Alignments were particularly difficult to generate if the audio contained slang or if the recording quality was bad.\n",
    "\n",
    "Despite the pipeline being highly dependent on the quality of the ASR stage, the hypothesis [formulated at the beginning](00_Introduction.ipynb#Plan-and-Hypothesis) could be verified. The pipeline can be said to generally work provided the quality of the partial transcriptions is high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
