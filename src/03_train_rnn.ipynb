{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 3: Train an RNN\n",
    "\n",
    "From the previous steps we have corpora containing corpus entries with the following contents:\n",
    "\n",
    "1. audio data sampled to 16kHz (mono)\n",
    "2. segmentation of the audio into speech- and pause-segments detected by WebRTC\n",
    "3. segmentation of the audio into speech- and pause-segments derived from the raw data\n",
    "4. transcriptions for the raw speech segments in original and normalized form\n",
    "5. spectrograms of the audio to use as training data\n",
    "6. labels for the spectrogram-frames ot use as training labels\n",
    "\n",
    "We can now train an RNN that will learn the relationship between an audio signal and its textual representation using the [CTC loss function](https://www.cs.toronto.edu/~graves/icml_2006.pdf). \n",
    "\n",
    "The raw speech segments (3.) and their transcriptions (4.) are called the _labelled data_. This data consists of the input data  `X` and the labels `Y`. To train the network we use only a part of this labelled data. This part is referred to as **training set**. To prevent overfitting on this set we use another part of the labelled data to form a **validation/dev set**.\n",
    "\n",
    "If the training is successful, the RNN will be able to output a transcription for unknown instances that is roughly equivalent to the actual transcription. We can evaluate the RNN's performance by comparing its output with the actual transcription on a **test-set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = r'E:/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's do the imports and some helper functions before we start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from corpus_util import *\n",
    "from audio_util import *\n",
    "from data_util import *\n",
    "\n",
    "rl_corpus_root = os.path.join(corpus_root, 'readylingua-corpus')\n",
    "ls_corpus_root = os.path.join(corpus_root, 'librispeech-corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the pronunciation of a given piece of text is highly dependent on the language, we train the RNN only on a specific language of the corpus. Therefore, let's load the corpus and extract the entries in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_corpus_de = load_corpus(rl_corpus_root)(languages='de')\n",
    "rl_corpus_de.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the transcript\n",
    "\n",
    "In order to limit the number of target classes to the characters of the alphabet we need to normalize the transcripts. Normalizing involves the following steps:\n",
    "\n",
    "1. remove leading and trailing whitespaces (trimming)\n",
    "2. remove multiple subsequent occurences of whitespace within the transcript\n",
    "3. replacing accentuated characters with character from the alphabet (e.g. _é_/_è_/_ê_/...->e, _ß_->ss, etc...)\n",
    "4. removing non-alphanumeric characters (removes punctuation)\n",
    "5. make everything lowercase\n",
    "\n",
    "You can edit/execute the cell below with your own examples to see the result of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string_utils import *\n",
    "samples = [ 'Crème-brûlée', 'Außerirdische', ' foo    bar   ']\n",
    "for sample in samples:\n",
    "    print(f'{sample} ==> {normalize(sample)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the transcript\n",
    "\n",
    "In order to use the transcripts as training labels, it needs to be tokenized first. By tokenizing we mean splitting the transcription into words and then into characters. The tokens are the characters of the transcription, whereas a special token `<space>` is used between the characters of two words.\n",
    "\n",
    "The mapping of audio to text is actually a classification problem: Parts of the audio signal are mapped each to a specific character (i.e. _token_). Since RNN resp. TensorFlow work best with numeric data, we need to encode the tokens to put them on an ordinal scale. The following table shows how the encoding is done:\n",
    "\n",
    "| **Token**    | `<space>` | `a` | `b` | `c` | ... | `z`  |\n",
    "|--------------|:---------:|:---:|:---:|:---:|:---:|:----:|\n",
    "| **Encoding** | `0`       | `1` | `2` | `3` | ... | `26` |\n",
    "\n",
    "The following table shows how a transcript is converted to its encoded form:\n",
    "\n",
    "| **Original transcript** | The quick, brown fox jumps over the lazy dog!  |\n",
    "|-------------------------|------------------------------------------------|\n",
    "| **Normalized transcript** | the quick brown fox jumps over the lazy dog |\n",
    "| **Tokenized transcript** | `['t', 'h', 'e', '<space>, 'q', 'u', 'i', 'c', 'k', '<space>', 'b', 'r', 'o', 'w', 'n', ...]` |\n",
    "| **Encoded transcript** | `[ 20, 8, 5, 0, 17, 21, 9, 3, 11, 0, 2, 18, 15, 23, 14]` |\n",
    "\n",
    "## The issue with numbers\n",
    "\n",
    "Numbers in transcript pose a special problem, since their pronunciation differs fundamentally from their textual representation, if written with digits (which is usually the case). Consider the number `8`, which is represented textually by the digit `'8'` and is pronounced as `eight`. In this case, the actual sequence of characters (`'e', 'i', 'g', 'h', 't')` is replaced by a single character `'8'` and can therefore not be approximated like ordinary words.\n",
    "\n",
    "The problem becomes even harder since compound number are sometimes pronounced differently than their individual parts would be pronounced. Consider the number `13` which is pronounced `'thirteen'` (and not `'onethree'`!). This becomes especially important in languages like German which swap the decimal part (e.g. `'21'` is pronounced as `'one-and-twenty'`).\n",
    "\n",
    "Since numbers are a problem of their own we want to limit their influence on the training process by training the RNN only on transcripts without numbers. We can filter those out by using the corpus entry as a function and pass in the `numeric=False` argument to get only those speech segments whose transcripts do not contain numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry = rl_corpus_de['edznachrichten180111']\n",
    "corpus_entry.summary()\n",
    "\n",
    "corpus_entry_nonnumeric = corpus_entry(numeric=False)\n",
    "corpus_entry_nonnumeric.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN architecture\n",
    "\n",
    "We train an RNN with the following architecture (inspired by [this repository](https://github.com/philipperemy/tensorflow-ctc-speech-recognition)):\n",
    "\n",
    "* number of features: 13\n",
    "* number of hidden layers: 1\n",
    "* RNN-cell type: LSTM\n",
    "\n",
    "### RNN cost\n",
    "We measure the cost in two ways:\n",
    "\n",
    "* CTC-cost\n",
    "* Label Error Rate (LER)\n",
    "\n",
    "#### CTC cost\n",
    "The CTC cost is calculated as follows:\n",
    "\n",
    "tbd...\n",
    "\n",
    "#### Label Error Rate (LER)\n",
    "The LER is defined as the [edit distance](https://www.tensorflow.org/api_docs/python/tf/edit_distance) between prediction (_hypothesis_) and actual labels (_truth_), also called the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). Observe for example the LER for the hypothesis `hello` and the truth `hallo`: The two strings differ in 1 of 5 characters, therefore the LER is `0.2`.\n",
    "\n",
    "The LER can also be computed for strings of different lengths. The following table gives an overview of a few samples.\n",
    "\n",
    "| Hypothesis | Truth | LER |\n",
    "|---|---|---|\n",
    "| 'hello' | 'hallo' | 0.2\n",
    "| 'hell' | 'hallo' | 0.4\n",
    "| 'hel' | 'hallo' | 0.6\n",
    "| 'helloo' | 'hallo' | 0.4\n",
    "| 'hellooo' | 'hallo' | 0.6\n",
    "| 'helo' | 'hallo' | 0.4\n",
    "| 'heloo' | 'hallo' | 0.4\n",
    "| 'helooo' | 'hallo' | 0.6\n",
    "| 'allo' | 'hallo' | 0.2\n",
    "| 'elo' | 'hallo' | 0.6\n",
    "\n",
    "You can also execute the cell below to see how the values are calculated with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    ('hello', 'hallo'),\n",
    "    ('hell', 'hallo'),\n",
    "    ('hel', 'hallo'),\n",
    "    ('helloo', 'hallo'),\n",
    "    ('hellooo', 'hallo'),\n",
    "    ('helo', 'hallo'),\n",
    "    ('heloo', 'hallo'),\n",
    "    ('helooo', 'hallo'),\n",
    "    ('allo', 'hallo'),\n",
    "    ('elo', 'hallo'),\n",
    "]\n",
    "\n",
    "import tensorflow as tf\n",
    "from rnn_utils import *\n",
    "\n",
    "print('hypothesis'.ljust(15) + 'truth'.ljust(15) + 'ler'.ljust(10))\n",
    "print('-'.join('' for _ in range(40)))\n",
    "for hypothesis, truth in samples:\n",
    "    h_values = encode(hypothesis)\n",
    "    h_indices = [[0, i] for i in range(len(h_values))]\n",
    "    h_shape = [1, len(h_values)]\n",
    "    h_tensor = tf.SparseTensor(indices=h_indices, values=h_values, dense_shape=h_shape)\n",
    "    \n",
    "    t_values = encode(truth)\n",
    "    t_indices = [[0, i] for i in range(len(t_values))]\n",
    "    t_shape = [1, len(h_values)]\n",
    "    t_tensor = tf.SparseTensor(indices=t_indices, values=t_values, dense_shape=t_shape)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        ler = tf.edit_distance(h_tensor, t_tensor)\n",
    "        edit_distance = sess.run(ler)\n",
    "        print(f'{hypothesis.ljust(15)}{truth.ljust(15)}{str(edit_distance[0]).ljust(10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Concept\n",
    "\n",
    "The RNN is supposed to learn the relationship between an audio signal and its transcription, i.e. if trained properly it should be able to generate a transcription for any unseen audio signal afterwards. To see whether the RNN learns something useful we train the RNN on only a single corpus entry. For simplicity, only speech segments that do not contain numbers are considered. \n",
    "\n",
    "### Learning to write\n",
    "\n",
    "Since the RNN constantly compares the audio signal with its transcript, there are many aspects influencing the learning process:\n",
    "\n",
    "* speaking rate: fast vs. slow speakers\n",
    "* pitch of the voice: female speakers usually have a higher voice than male speakers. Therefore the audio signal resp. spectrogram might differ considerably\n",
    "* language: different languages have different languages models and pronunciation patterns. This is especially important for languages like French or English where the pronunciation of a word can be quite different from its written form\n",
    "* Sequence length: it is expected that short sequences are easier to fit than long sequences\n",
    "\n",
    "To get an intuition for how much the RNN is sensitive to different properties of the input data we examine the last two aspects a bit closer. To do this we use two corpus entries in different languages, whereas the average sequence length in one corpus is considerably longer than in the other. To minimize the influence of the pitch only corpus entries with female speakers were considered. For the sake of simplicity it is assumed that the speaking rate between the samples is similar. In consequence the term _length_ refers to both the length of the audio segment (in seconds) as well as the transcript length (number of characters).\n",
    "\n",
    "This leaves us with the RNN being trained in three variations:\n",
    "\n",
    "* **PoC #1**: Train on a corpus entry in German\n",
    "* **PoC #2**: Train on a corpus entry in English with an average segment length similar to the sample used for PoC #1\n",
    "* **PoC #3**: Train on a corpus entry in English with an average segment length that is longer than in the sample used for PoC #2\n",
    "\n",
    "By comparing PoC #1 and #2 we get a feeling for how much the training depends on the language of the corpus entries. By comparing PoC #2 and #3 we get a feeling for how much the training depends on the length of the speech segments. \n",
    "\n",
    "### PoC #1: German sample (short segments)\n",
    "For the _ReadyLingua_ corpus the first corpus entry in German is the poem _\"An die Freude\"_ from F. Schiller. For the first PoC we will train the RNN exclusively on this sample.\n",
    "\n",
    "#### Partial sample\n",
    "\n",
    "To get a fast feedback on the learning progress we will train the RNN only on the first five speech segments of the training sample. These segments have the following transcript (normalized):\n",
    "\n",
    "    1. an die freude von friedrich schiller\n",
    "    2. freude schoner gotterfunken\n",
    "    3. tochter aus elysium\n",
    "    4. wir betreten feuertrunken himmlische dein heiligtum\n",
    "    5. deine zauber binden wieder\n",
    "    \n",
    "We can now train the RNN with just these segments using the CTC- and LER-cost explained above. We train on 300 variants of the segments. In each variant the audio signal of the segment is cropped by a random value between 1 and 2000 frames. With a sampling rate of 16kHz this corresponds to an audio segment of 125ms at max. By comparing the actual transcription (_ground truth_) with the decoded output of the RNN (_prediction_) we can see that the RNN is indeed learning how to recognize speech.\n",
    "\n",
    "We can see that while in the beginning the generated transcript does not make much sense. It consists mainly of the letter `e` which happens to be the most frequent character in a German text. With proceeding training the generated transcript become clearer until they match up almost perfectly with the actual transcripts. This is also reflected in the curves for the CTC- and LER-cost:\n",
    "\n",
    "    Iteration 4:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       e ee e e e e\n",
    "    \n",
    "    Iteration 26:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       ie enienuenienheieiu\n",
    "    \n",
    "    Iteration 112:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       i betreterteueruner himmlische en heitum\n",
    "    \n",
    "    Iteration 178:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       wi betreteneuertrunkeni himmlische dein heiligtum\n",
    "\n",
    "    Iteration 298:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       wir betreten feuertrunken himmlische dein heigtum\n",
    "\n",
    "<img src=\"../assets/cost_ctc_sample.png\" alt=\"CTC cost\" style=\"width: 450px; float: left;\"/>\n",
    "<img src=\"../assets/cost_ler_sample.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "\n",
    "Of course by re-using the same sample 300 times the RNN has hopelessly overfitted to that sample. The RNN will not generalize well, meaning it is not expected to perform well on unseen examples. The result is therefore not representative. However, the PoC shows that the RNN is generally able to learn something useful.\n",
    "\n",
    "#### Whole sample\n",
    "\n",
    "To get a better intuition on how fast the RNN learns the relationship we train it on the same corpus entry, but this time we use all speech segments. We can see that the RNN still learns the relationships between audio signal and transcription, but requires a considerably larger number of training epochs to get comparable results (i.e. similar costs like when only training on five speech segments):\n",
    "\n",
    "    tbd.    \n",
    "\n",
    "### PoC #2: English sample (short segments)\n",
    "\n",
    "For the second PoC we use a corpus entry with similar properties like in PoC #1. The first corpus entry in the _ReadyLingua_ corpus is ... . Its first five speech segments have the following transcripts:\n",
    "\n",
    "    tbd...\n",
    "    \n",
    "Training the RNN the same way like shown above gives us the following progress:\n",
    "\n",
    "    tbd...\n",
    "    \n",
    "\n",
    "### PoC #: English sample (long segments)\n",
    "\n",
    "For the third PoC the chapter _\"Tom, the Piper's Son\"_ from _\"Mother Goose in Prose\"_ from the _LibriSpeech_-corpus was used (`corpus_entry.id=121669`). The first five speech segments are:\n",
    "\n",
    "    1. tom the pipers son\n",
    "    2. the pig was eat and tom was beat and tom ran crying down the street\n",
    "    3. he never did any work except to play the pipes and he played so badly that few pennies ever found their way into his pouch it was whispered around that old barney was not very honest\n",
    "    4. but he was so sly and cautious that no one had ever caught him in the act of stealing although a good many things had been missed after they had fallen into the old mans way barney had one son named tom\n",
    "    5. and they lived all alone in a little hut away at the end of the village street for toms mother had died when he was a baby you may not suppose that tom was a very good boy since he had such a queer father but neither was he very bad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the performance\n",
    "To compare the transcription produced by the RNN with the actual transcription we need a way to measure the similarity between these two texts. One way of doing this is the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) which calculates the degree of similarity as the edit distance (number of single-character edits required to change text 1 into text 2), whereas a lower value means a higher degree of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
