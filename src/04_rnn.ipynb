{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNN prototype\n",
    "\n",
    "From the previous notebooks the following data was created:\n",
    "\n",
    "1. audio signal resampled to 16kHz (mono)\n",
    "1. segmentation of the audio signal into speech- and pause-segments derived from the raw data or through WebRTC\n",
    "1. spectrograms or MFCC of the audio signal to use as training data\n",
    "1. transcriptions for the raw speech segments in original and normalized form\n",
    "\n",
    "We can now train an RNN that will learn the relationship between an audio signal and its textual representation using the [CTC loss function](https://www.cs.toronto.edu/~graves/icml_2006.pdf) as described in the [introduction](00_introduction.ipynb).\n",
    "\n",
    "The spectrograms of speech segments (3.) and their transcriptions (4.) are called the _labelled data_. This data consists of the input data  `X` and the labels `Y`. To train the network we use only a part of it. This part is referred to as **training set**. To prevent overfitting on this set we use another part of the labelled data to form a **validation/dev set**.\n",
    "\n",
    "This notebook describes the training of a RNN prototype. The prototype does not represent a valid RNN that can be used for RNN. Instead, it is trained on various input. The results are then compared in order to gain knowledge about the influence of different properties of the input on the result.\n",
    "\n",
    "The goal of this process is to get valuable information to train a real RNN later. If the training is successful, the RNN will be able to output a transcription for unknown instances that is roughly equivalent to the actual transcription. We can evaluate the RNN's performance by comparing its output with the actual transcription on a **test-set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = r'E:/' # define the path to where the corpus files are located!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, some imports and some helper functions need to be defined before we start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from util.corpus_util import *\n",
    "from util.audio_util import *\n",
    "\n",
    "rl_corpus_root = os.path.join(corpus_root, 'readylingua-corpus')\n",
    "ls_corpus_root = os.path.join(corpus_root, 'librispeech-corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "Since the pronunciation of a given piece of text is highly dependent on the language, we train the RNN only on a specific language of the corpus at a time. Therefore different RNN need to be trained for different languages. \n",
    "\n",
    "To show the impact of various properties of the training set, the following Proof of Concepts (PoC) are created:\n",
    "\n",
    "* **PoC #1 (benchmark)**: We start with a very simple RNN that is only trained on five speech segments from a single corpus entry in German and observe the training progress.\n",
    "* **PoC #2 (convergence for German)**: We then compare the results by extending this example. The same RNN is trained on the same corpus entry, but this time with all speech segments. This should give us some hints as to how additional input from the same distribution will influence convergence during the learning progress.\n",
    "* **PoC #3 (language, sequence length)**: The same RNN is trained again with only five speech segments, but those are taken from a corpus entry in a different language (English instead of German). From the result we hope to infer some information about how robust the RNN architecture is to various languages.\n",
    "* **PoC #4 (convergence for English)**: The same RNN is trained again with all speech segments of the English corpus Entry. Comparison with PoC#2 should give us some information about the influence of language on convergence.\n",
    "\n",
    "The RNN is trained with both spectrograms and MFCC as features. Comparing the performance between both types of features should give some information about what features are better suited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_corpus_de = load_corpus(rl_corpus_root)(languages='de')\n",
    "rl_corpus_de.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_entry = rl_corpus_de['edznachrichten180111']\n",
    "corpus_entry.summary()\n",
    "\n",
    "corpus_entry_nonnumeric = corpus_entry(numeric=False)\n",
    "corpus_entry_nonnumeric.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN architecture\n",
    "\n",
    "We train a simple RNN with the following architecture (inspired by [this repository](https://github.com/philipperemy/tensorflow-ctc-speech-recognition)):\n",
    "\n",
    "* number of features: 13 for MFCC resp. 161 for spectrogram (see [notebook 3](03_feature_extraction.ipynb#From-raw-waves-to-spectrograms) on how to calculate those values)\n",
    "* number of hidden layers: 1\n",
    "* RNN-cell type: LSTM\n",
    "\n",
    "### RNN cost\n",
    "The RNN performance is measured with two metrics:\n",
    "\n",
    "* CTC-loss\n",
    "* Label Error Rate (LER)\n",
    "\n",
    "The calculation of the CTC-loss has been described as part of the description of CTC in [the introduction](00_Introduction.ipynb#Calculating-the-CTC-loss-by-creating-valid-alignments-with-dynamic-programming). The LER is calculated as follows.\n",
    "\n",
    "#### Label Error Rate (LER)\n",
    "The LER is defined as the [edit distance](https://www.tensorflow.org/api_docs/python/tf/edit_distance) between prediction (_hypothesis_) and actual labels (_ground truth_), also called the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). Observe for example the LER for the hypothesis `hello` and the truth `hallo`: The two strings differ in 1 of 5 characters, therefore the LER is `0.2`.\n",
    "\n",
    "The LER can also be computed for strings of different lengths. The following table gives an overview of a few samples.\n",
    "\n",
    "| Hypothesis | Truth | LER |\n",
    "|---|---|---|\n",
    "| 'hello' | 'hallo' | 0.2\n",
    "| 'hell' | 'hallo' | 0.4\n",
    "| 'hel' | 'hallo' | 0.6\n",
    "| 'helloo' | 'hallo' | 0.4\n",
    "| 'hellooo' | 'hallo' | 0.6\n",
    "| 'helo' | 'hallo' | 0.4\n",
    "| 'heloo' | 'hallo' | 0.4\n",
    "| 'helooo' | 'hallo' | 0.6\n",
    "| 'allo' | 'hallo' | 0.2\n",
    "| 'elo' | 'hallo' | 0.6\n",
    "\n",
    "You can also execute the cell below to see how the values are calculated with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    ('hello', 'hallo'),\n",
    "    ('hell', 'hallo'),\n",
    "    ('hel', 'hallo'),\n",
    "    ('helloo', 'hallo'),\n",
    "    ('hellooo', 'hallo'),\n",
    "    ('helo', 'hallo'),\n",
    "    ('heloo', 'hallo'),\n",
    "    ('helooo', 'hallo'),\n",
    "    ('allo', 'hallo'),\n",
    "    ('elo', 'hallo'),\n",
    "]\n",
    "\n",
    "import tensorflow as tf\n",
    "from util.rnn_util import *\n",
    "\n",
    "print('hypothesis'.ljust(15) + 'truth'.ljust(15) + 'LER'.ljust(10))\n",
    "print('-'.join('' for _ in range(40)))\n",
    "for hypothesis, truth in samples:\n",
    "    h_values = encode(hypothesis)\n",
    "    h_indices = [[0, i] for i in range(len(h_values))]\n",
    "    h_shape = [1, len(h_values)]\n",
    "    h_tensor = tf.SparseTensor(indices=h_indices, values=h_values, dense_shape=h_shape)\n",
    "    \n",
    "    t_values = encode(truth)\n",
    "    t_indices = [[0, i] for i in range(len(t_values))]\n",
    "    t_shape = [1, len(h_values)]\n",
    "    t_tensor = tf.SparseTensor(indices=t_indices, values=t_values, dense_shape=t_shape)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        ler = tf.edit_distance(h_tensor, t_tensor)\n",
    "        edit_distance = sess.run(ler)\n",
    "        print(f'{hypothesis.ljust(15)}{truth.ljust(15)}{str(edit_distance[0]).ljust(10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training profiles\n",
    "\n",
    "In order to efficiently use the project time, the RNN was trained several times with different profiles. A profile is a combination of values for of the following configuration items:\n",
    "\n",
    "| configuration item | possible values |\n",
    "|---|---|\n",
    "| language | German or English |\n",
    "| feature type | MFCC or spectrograms |\n",
    "| data type | original or synthetisized data (see below) |\n",
    "\n",
    "An iterative approach was taken to train the RNNs. For each step in the iteration the value of configuration item was changed. The resulting profiles were used to train a PoC. The following table shows the mapping of PoCs to their profiles:\n",
    "\n",
    "\n",
    "\n",
    "For each profile convergence behavior for the CTC- and LER-cost was inspected by inspecting their plots. This served as a basis for further decisions. \n",
    "\n",
    "This approach should ensure that no time is wasted to create a sophisticated setup that may or may not lead to better results. By changing only ever one single configuration item in each step, the impact of each change could be analyzed in isolation and conclusions for the following steps could be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| PoC id | language | feature type | data type |\n",
    "|---|---|---|---|\n",
    "| Poc#1 | German | MFCC | original |\n",
    "| Poc#2 | German | MFCC | synthesized|\n",
    "| Poc#3 | German | Mel-Spectrogram | original |\n",
    "| Poc#4 | German | Mel-Spectrogram | synthesized|\n",
    "| Poc#5 | German | Power-Spectrogram | original |\n",
    "| Poc#6 | German | Power-Spectrogram | synthesized|\n",
    "| Poc#7 | English | MFCC | original |\n",
    "| Poc#8 | English | MFCC | synthesized|\n",
    "| Poc#9 | English | Mel-Spectrogram | original |\n",
    "| Poc#10| English | Mel-Spectrogram | synthesized|\n",
    "| Poc#11 | English | Power-Spectrogram | original |\n",
    "| Poc#12 | English | Power-Spectrogram | synthesized|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetisized training data\n",
    "\n",
    "Audio data can be particularly well synthetisized by altering the original data through the addition of distortion (change of tempo/loudness/pitch, adding echo/reverb or background noise etc.). To artificially reduce overfitting, the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, rate = corpus_entry.audio, corpus_entry.rate\n",
    "display(Audio(data=audio, rate=rate))\n",
    "\n",
    "distorted = distort(audio, rate, tempo=2.0)\n",
    "display(Audio(data=distorted, rate=rate))\n",
    "\n",
    "distorted = distort(audio, rate, pitch=True)\n",
    "display(Audio(data=distorted, rate=rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of convergence\n",
    "\n",
    "Observing the trend of the cost is one way to gain information about the training process. Another type of information that can be gained implicitly from the cost is the time needed until the training process has converged. For this, _convergence_ needs to be defined first. For this process, the term _convergence_ was defined for the LER-cost to meet two criteria:\n",
    "\n",
    "1. the mean LER-cost over the last 10 epochs must be below 0.05. This ensures that the predicted transcriptions diverge from the actual transcriptions by 5% at max (in terms of edit distance). The value of 5% is somewhat arbitrarily chosen. However it lies within the range of the LERs found for the best STT systems found in research papers.\n",
    "1. the change rate of the average LER costs over the last 10 averages must be below 0.01, i.e. the last 10 average LER-costs must not change more than 1% on average. This ensures that no early stopping is performed because the average cost will have flattened and reached a plateau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Concept\n",
    "\n",
    "The RNN is supposed to learn the relationship between an audio signal and its transcription, i.e. if trained properly it should be able to generate a transcription for any unseen audio signal afterwards. To see whether the RNN is even able to learn something useful with the given architecture we train the RNN on only a single corpus entry. For simplicity, only speech segments that do not contain numbers are considered. \n",
    "\n",
    "To get an intuition for how much the RNN is sensitive to different properties of the input data we examine the last two aspects a bit closer. To do this we use two corpus entries in different languages (German and English), whereas the average speech sequence length in one entry (English) is considerably longer than in the other. To minimize the influence of acoustic properties like pitch of the voice, only corpus entries with female speakers were considered. For the sake of simplicity it is assumed that the speaking rate between the samples is similar. As a consequence the term _length_ refers to both the length of the audio segment (in seconds) as well as the transcript length (number of characters).\n",
    "\n",
    "This leaves us with the RNN being trained in three variations:\n",
    "\n",
    "* **PoC #1 (benchmark)**: Train the RNN on a German corpus entry. Only five speech segments are used. Observe the training progress. This should give us a benchmark to compare to.\n",
    "* **PoC #2 (convergence for German)**: Train the same RNN on the same corpus entry, but this time with all speech segments.\n",
    "* **PoC #3 (language, sequence length)**: Train the same RNN again, but with a corpus entry in English. Only five speech segments are considered. Compare the result with PoC#1.\n",
    "* **PoC #4 (convergence for English)**: Train the same RNN again with all speech segments of the English corpus entry.\n",
    "\n",
    "Comparing Poc #1 and #2 should give us some hints as to how additional input from the same distribution will impact convergence during the learning progress. Comparing PoC #1 and #3 should allow for inferring  how robust the RNN architecture is to various languages and average sequence lenghts. By comparing PoC #2 and #4 we should get a feeling about the influence of language on convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #1\n",
    "For the _ReadyLingua_ corpus the first corpus entry in German is the poem _\"An die Freude\"_ from F. Schiller. The first PoC is trained exclusively on the first five speech segments. These segments have the following transcript (normalized):\n",
    "\n",
    "    1. an die freude von friedrich schiller\n",
    "    2. freude schoner gotterfunken\n",
    "    3. tochter aus elysium\n",
    "    4. wir betreten feuertrunken himmlische dein heiligtum\n",
    "    5. deine zauber binden wieder\n",
    "    \n",
    "The RNN is trained by repeatedly feeding it these training samples. One iteration over the whole training set is called _epoch_. The RNN is trained until convergence as defined above. The LER- and CTC-cost is measured after each epoch. Additionally, an artificial validation sample is created by randomly choosing a sample from the training set and shifting its audio signal to the left. Shifting is done by cropping a random number (between 1 and 2000) of samples from the beginning. With a sampling rate of 16kHz this corresponds to an audio segment of 125ms at most.\n",
    "\n",
    "Of course by training on such a small training set the RNN will hopelessly overfit to those 5 samples.  Also, the validation set consist variants of the training set and is therefore not representative because the RNN will have already seen the samples in some other form. \n",
    "\n",
    "However, although the result are not representative, this PoC is a quick and cheap way to validate if the RNN is able to learn the relationship between audio signal and transcription at all. The results serve as a baseline to compare to.\n",
    "\n",
    "#### Results\n",
    "\n",
    "By comparing the actual transcription (_ground truth_) with the decoded output of the RNN (_prediction_) we can see that the RNN indeed learns how to recognize speech. The following log extract shows the learning progress during various stages of the learning progress.\n",
    "\n",
    "    Epoch 1:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       e    \n",
    "    \n",
    "    Epoch 5:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       irie e eiei e e e e ei \n",
    "    \n",
    "    Epoch 25:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       tirberete euernke uishe en heitum\n",
    "    \n",
    "    Epoch 38:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       wir betreten feuernkenfmimische ein heiligtum\n",
    "\n",
    "    Epoch 63:\n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       wir betreten feuertrunken himlische dein heiligum\n",
    "    \n",
    "It is evident that in the first few epochs the generated transcript does not make much sense. It consists mainly of the letter `e` which happens to be the most frequent character in a German text. With further iterations however, the generated transcripts become clearer until they match up almost perfectly with the actual transcripts. After just 63 epochs the RNN produces transcripts that are very similar to the originl ones. Note that in the last two epochs above the RNN has actually unlearned how to recognize the word _heiligtum_. This is possible in LSTM-networks, because LSTM-cells include a trainable parameter $\\Gamma_f$ that controls how much of the cell value of the previous time step is used to calculate the cell value in the current time step (forget-gate). For more information about LSTM-cells see [Christopher Olah's blog about understanding LSTM cells](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "The learning progress is also reflected in the plots for the CTC- and LER-cost:    \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc1_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc1_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n",
    "The PoC shows that the RNN is generally able to learn something useful. However, the results are not representative as the RNN will not generalize well, meaning it is not expected to perform well on unseen examples. The results can still be used as a reference value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #2\n",
    "\n",
    "To get a better intuition on how fast the RNN learns, it was trained on the same data again using MFCC as features. However, the original data was only included in the first epoch. For all following epochs, the audio was distorted by slightly changing the tempo by a random factor between 0.8 and 1.2. This resulted in synthesized data which should simulate slower and faster speakers.\n",
    "\n",
    "#### Results\n",
    "\n",
    "Because the RNN did not converge, training was aborted after more than 13.000 epochs. By that time the LER rate oscillated around a value between 0.07 and 0.08 with average change rates below 0.1%, which is close to the convergence criteria. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc2_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc2_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "This is also reflected in the results of the last epochs:\n",
    "\n",
    "    Ground truth (train-set):     an die freude von friedrich schiller\n",
    "    Prediction (train-set):       an die freude von f friedrich schiler            \n",
    "    Ground truth (train-set):     freude schoner gotterfunken\n",
    "    Prediction (train-set):       freude schoner goterfunken                       \n",
    "    Ground truth (train-set):     tochter aus elysium\n",
    "    Prediction (train-set):       tochter aus elysium                              \n",
    "    Ground truth (train-set):     wir betreten feuertrunken himmlische dein heiligtum\n",
    "    Prediction (train-set):       wr betren feuertrunken himlische dein h heiligtum\n",
    "    Ground truth (train-set):     deine zauber binden wieder\n",
    "    Prediction (train-set):       deine zauber binde wieder    \n",
    "  \n",
    "#### Interpretation\n",
    "\n",
    "We can see that the RNN still learns the relationships between audio signal and transcription when trained on synthesized data. In contrast to PoC#1 the training batches were similar but no two training samples were exactly the same. This reduced overfitting and the risk of the RNN learning the mapping by heard, but required a considerably larger number of training epochs to get comparable results (more than 13k epochs compared to only a few dozen before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #3\n",
    "\n",
    "For the third PoC we use a corpus entry with similar properties like in PoC #1. The first corpus entry in the _LibriSpeech_ corpus is ... . Five speech segments with similar lenghts as in PoC #1 have the following transcripts:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc3_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc3_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Results\n",
    "Training the RNN the same way like shown above gives us the following progress:\n",
    "\n",
    "    tbd...   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #4\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc4_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc4_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #5\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc5_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc5_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #6\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc6_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc6_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #7\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc7_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc7_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #8\n",
    "\n",
    "#### Results\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc8_ctc.png\" alt=\"CTC cost\" style=\"width: 450px; \"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../assets/poc8_ler.png\" alt=\"LER cost\" style=\"width: 450px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC #: English sample (long segments)\n",
    "\n",
    "For the third PoC the chapter _\"Tom, the Piper's Son\"_ from _\"Mother Goose in Prose\"_ from the _LibriSpeech_-corpus was used (`corpus_entry.id=121669`). The first five speech segments are:\n",
    "\n",
    "    1. tom the pipers son\n",
    "    2. the pig was eat and tom was beat and tom ran crying down the street\n",
    "    3. he never did any work except to play the pipes and he played so badly that few pennies ever found their way into his pouch it was whispered around that old barney was not very honest\n",
    "    4. but he was so sly and cautious that no one had ever caught him in the act of stealing although a good many things had been missed after they had fallen into the old mans way barney had one son named tom\n",
    "    5. and they lived all alone in a little hut away at the end of the village street for toms mother had died when he was a baby you may not suppose that tom was a very good boy since he had such a queer father but neither was he very bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
